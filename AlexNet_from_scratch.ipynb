{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMQEe_fr1tov"
      },
      "source": [
        "# Writing code from scratch for implementing CNN using python and numpy for 2D input . But I have to modify all my code to do bathch gradient descent ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xs4k8wm0mHp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaKGEPqiTxn4"
      },
      "outputs": [],
      "source": [
        "def padding_1d(x,layer):\n",
        "  z = np.array([0])\n",
        "  z = np.repeat(z, layer)\n",
        "  return np.concatenate([z, x, z])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nrLIMbfrhqw"
      },
      "outputs": [],
      "source": [
        "def padding(x,layer):\n",
        "  outs = [padding_1d(row,layer) for row in x]\n",
        "  return np.array(outs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWBT6J6Or8kP",
        "outputId": "a398717e-219c-4d01-cbad-d3934700ae04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 7)\n",
            "2\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "inp = np.array([[1,2,3,4,5], [6,7,8,9,10]])\n",
        "out = padding(inp, 1)\n",
        "print(out.shape)\n",
        "print(out.shape[0])\n",
        "print(out.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18HpHmwHdU0M",
        "outputId": "6040188d-8858-487a-8800-b6a7965b061e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5  0]\n",
            " [ 0  6  7  8  9 10  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBdYrYRrztS7"
      },
      "source": [
        "### Convolution layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04hTV-Ak019n"
      },
      "outputs": [],
      "source": [
        "# def convulation(input,filter,padding_layer=0,stride=1):\n",
        "#   pad_input = padding(input,padding_layer)\n",
        "#   output_height = int(((input.shape[0] - filter.shape[0] + (2*padding_layer))/stride) + 1)\n",
        "#   output_weidth= int(((input.shape[1] - filter.shape[1] + (2*padding_layer))/stride) + 1)\n",
        "\n",
        "#   output = np.zeros((output_height, output_weidth))\n",
        "\n",
        "#   out_h = 0\n",
        "#   for i in range(0,pad_input.shape[0]-filter.shape[0] + 1, stride):\n",
        "#     out_w = 0\n",
        "#     for j in range(0,pad_input.shape[1]-filter.shape[1] + 1, stride):\n",
        "#       for f_h in range(filter.shape[0]):\n",
        "#         for f_w in range(filter.shape[1]):\n",
        "#           output[out_h][out_w] += filter[f_h][f_w] * pad_input[i+f_h][j+f_w]\n",
        "#       out_w += 1\n",
        "#     out_h += 1\n",
        "\n",
        "#   return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8XNu92ZXy4s"
      },
      "outputs": [],
      "source": [
        "# input = np.array([[0,0,0,0,0], [0,1,2,3,0], [0,4,5,6,0], [0,7,8,9,0], [0,0,0,0,0]])\n",
        "# filter = np.array([[0,1], [2,3]])\n",
        "# filter_2 = np.array([[1,3,-1], [0,2,2], [0,-1,-2]])\n",
        "\n",
        "# output = convulation(input,filter,0,1)\n",
        "# output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def pad_2d_array(x, pad_size):\n",
        "#     \"\"\"\n",
        "#     Pad a 2D numpy array 'x' with zeros.\n",
        "\n",
        "#     Parameters:\n",
        "#     x (numpy.ndarray): Input 2D array.\n",
        "#     pad_size (int): Number of rows/columns to pad on each side.\n",
        "\n",
        "#     Returns:\n",
        "#     numpy.ndarray: Padded 2D array.\n",
        "#     \"\"\"\n",
        "#     return np.pad(x, pad_size, mode='constant')\n",
        "\n",
        "# # Example usage:\n",
        "# x = np.array([[1, 2, 3],\n",
        "#               [4, 5, 6],\n",
        "#               [7, 8, 9]])\n",
        "\n",
        "# padded_x = pad_2d_array(x, 2)\n",
        "# print(padded_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoayFHoIjZmn",
        "outputId": "4ba98964-1dec-4fb4-cbb7-e11061d6f06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 1 2 3 0 0]\n",
            " [0 0 4 5 6 0 0]\n",
            " [0 0 7 8 9 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HAtj2Hw_GF8"
      },
      "outputs": [],
      "source": [
        "class Convoluation_0:\n",
        "\n",
        "  def __init__(self,filter_size:tuple=(2,2),padding_layer:int=0,stride:int=1):\n",
        "    self.filter = np.random.randn(filter_size[0],filter_size[1])\n",
        "    self.bias = np.random.randn()\n",
        "    self.padding_layer = padding_layer\n",
        "    self.stride = stride\n",
        "\n",
        "\n",
        "  def padding(self,x, pad_size):\n",
        "    \"\"\"\n",
        "    Pad a 2D numpy array 'x' with zeros.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): Input 2D array.\n",
        "    pad_size (int): Number of rows/columns to pad on each side.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Padded 2D array.\n",
        "    \"\"\"\n",
        "    return np.pad(x, pad_size, mode='constant')\n",
        "\n",
        "\n",
        "  def forward(self, input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    this function will do convoluation operation on input array with filter\n",
        "    and will also add bias after it .\n",
        "\n",
        "    Args : input array (image)\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    # if we want to do padding around input\n",
        "    self.pad_input = padding(self.input,self.padding_layer)\n",
        "    # height of the output array . calculate by formula\n",
        "    output_height = int(((self.input.shape[0] - self.filter.shape[0] + (2*self.padding_layer))/self.stride) + 1)\n",
        "    # width of the output array\n",
        "    output_weidth= int(((self.input.shape[1] - self.filter.shape[1] + (2*self.padding_layer))/self.stride) + 1)\n",
        "\n",
        "    # take a array of shape (output_height, output_width) with all element zero\n",
        "    self.output = np.zeros((output_height, output_weidth))\n",
        "\n",
        "    # for putting element in each row of output array\n",
        "    out_h = 0\n",
        "    # looping through the pad_input array in row .\n",
        "    for i in range(0,self.pad_input.shape[0]-self.filter.shape[0] + 1, self.stride):\n",
        "      # for putting element in column in output array\n",
        "      out_w = 0\n",
        "      # looping through the pad_input column .\n",
        "      for j in range(0,self.pad_input.shape[1]-self.filter.shape[1] + 1, self.stride):\n",
        "\n",
        "        # looping through the all row and column of the filter . row wise\n",
        "        for f_h in range(self.filter.shape[0]):\n",
        "          for f_w in range(self.filter.shape[1]):\n",
        "            # putting element in output array after convoluation(X*w)\n",
        "            self.output[out_h][out_w] += self.filter[f_h][f_w] * self.pad_input[i+f_h][j+f_w]\n",
        "        out_w += 1\n",
        "      out_h += 1\n",
        "\n",
        "    self.output += self.bias\n",
        "\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray):\n",
        "    \"\"\"\n",
        "    this backward function will calculate the gradient of loss w.r.t weight and bias.\n",
        "\n",
        "    Args :\n",
        "          prev_grad : it is the gradient of loss w.r.t next layer . It will use in\n",
        "                       calculating gradient of loss wr.t weight and bias.\n",
        "\n",
        "    \"\"\"\n",
        "    # first try to find out the gradient of loss w.r.t bias\n",
        "\n",
        "    # gradient of loss w.r.t bias will be simply summation of\n",
        "    # all element of prev_grad (derive from calculation)\n",
        "    self.grad_bias = np.sum(prev_grad, dtype=np.float32)\n",
        "\n",
        "    # Now try to find out gradient of loss w.r.t weight\n",
        "    # it will be convolutation operation of padded input array and prev_grad array\n",
        "\n",
        "    self.grad_weight = np.zeros(self.filter.shape)\n",
        "\n",
        "    row_grad = 0         # for looping through all the row of prev_grad\n",
        "    # loop through all the row of input array\n",
        "    for i in range(0, self.pad_input.shape[0]-prev_grad.shape[0] + 1, self.stride):\n",
        "      col_grad = 0       # for looping through all the column of prev_grad\n",
        "      # loop through all the column of padded input array\n",
        "      for j in range(0, self.pad_input.shape[1]-prev_grad.shape[1] + 1, self.stride):\n",
        "        # looping through the all row and column of the prev_grad . row wise\n",
        "        for f_h in range(prev_grad.shape[0]):\n",
        "          for f_w in range(prev_grad.shape[1]):\n",
        "            # putting element in grad_weight array after convoluation(input,prev_grad))\n",
        "            self.grad_weight[row_grad][col_grad] += prev_grad[f_h][f_w] * self.pad_input[i+f_h][j+f_w]\n",
        "        col_grad += 1\n",
        "      row_grad += 1\n",
        "\n",
        "\n",
        "    # finding gradient of loss w.r.t input . It is simply full convolution operation of\n",
        "    # prev_grad and 180 deg rotted filter . To do full convolution we will do padding of size (filter-1) around prev_grad\n",
        "\n",
        "    self.grad_input = np.zeros(self.pad_input.shape)\n",
        "\n",
        "    # padding around the prev_grad so that we can do full convolution .\n",
        "    pad_prev_grad = padding(prev_grad,self.filter.shape[0]-1)\n",
        "\n",
        "    # Rotate the filter by 180 degrees for convolution\n",
        "    rotated_filter = np.rot90(self.filter, 2)\n",
        "\n",
        "\n",
        "    # Now we will do convolution operation of pad_prev_grad and filter .\n",
        "    row_grad = 0         # for looping through all the row of prev_grad\n",
        "    # loop through all the row of pad_prev_grad array\n",
        "    for i in range(0, pad_prev_grad.shape[0]-self.filter.shape[0] + 1, 1):\n",
        "      col_grad = 0       # for looping through all the column of prev_grad\n",
        "      # loop through all the column of padded input array\n",
        "      for j in range(0, pad_prev_grad.shape[1]-self.filter.shape[1] + 1, 1):\n",
        "        # looping through the all row and column of the prev_grad . row wise\n",
        "        for f_h in range(self.filter.shape[0]):\n",
        "          for f_w in range(self.filter.shape[1]):\n",
        "            # putting element in grad_weight array after convoluation(input,prev_grad))\n",
        "            self.grad_input[row_grad][col_grad] += rotated_filter[f_h][f_w] * pad_prev_grad[i+f_h][j+f_w]\n",
        "        col_grad += 1\n",
        "      row_grad += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def update(self, learning_rate):\n",
        "    \"\"\"\n",
        "    this will take learning rate as parameter and will update the weight and bias of the filter and layer\n",
        "    \"\"\"\n",
        "    self.filter -= learning_rate * self.grad_weight\n",
        "    self.bias -= learning_rate * self.grad_bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skrmx-hkzlDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2ba6bb-69bb-476c-8bd1-97c2a492d1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 16)\n"
          ]
        }
      ],
      "source": [
        "input = np.random.randn(28,28)\n",
        "\n",
        "# filter = np.random.randn(4,4)\n",
        "# filter_2 = np.array([[1,3,-1], [0,2,2], [0,-1,-2]])\n",
        "\n",
        "\n",
        "conv1 = Convoluation_0((4,4),3,2)\n",
        "conv1.forward(input)\n",
        "print(conv1.output.shape)\n",
        "\n",
        "# pre_grad = np.array([[0.5,0.4], [1.11,-1.03]])\n",
        "\n",
        "# grad = conv1.backward(pre_grad)\n",
        "# print(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKwUSHw0HBC"
      },
      "source": [
        "### Relu activation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRHr2xVUYT87"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2IXoQSx-lcy",
        "outputId": "e6f1d191-70ab-44f8-e24c-493b000512a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 2],\n",
              "       [0, 0, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x = np.array([[1,-1,2], [0, -3 , 4]])\n",
        "x = relu(x)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj3hsQDc_sNM"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return (1/(1+np.exp(-x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laLCgosrABPm",
        "outputId": "fd47cdd4-f956-4716-d381-11fc70f14ee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.73105858, 0.5       , 0.88079708],\n",
              "       [0.5       , 0.5       , 0.98201379]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x = sigmoid(x)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P3pLLse4eSS"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def forward(self, inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    after multiplying weight with previous layer output and adding bias , we will pass the\n",
        "    result through relu activation layer . If value is greater than 0 it will be same\n",
        "    otherwise it will be zero .\n",
        "\n",
        "    Args :\n",
        "          output numpy array after dot product and bias addition\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the gradient of loss w.r.t the ReLU activation layer.\n",
        "\n",
        "    Args:\n",
        "        prev_grad: Numpy array of the gradient of loss w.r.t the outputs from the previous layer.\n",
        "\n",
        "    Return:\n",
        "        Numpy array containing the gradient of loss w.r.t the inputs to the ReLU activation layer.\n",
        "    \"\"\"\n",
        "    self.grad = np.where(self.output > 0, prev_grad, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJJa9ZGG0NtS"
      },
      "source": [
        "### Maxpool layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm0rQYkGA_tm"
      },
      "outputs": [],
      "source": [
        "def maxpool(x,size=1):\n",
        "  height_x = x.shape[0]\n",
        "  width_x = x.shape[1]\n",
        "\n",
        "  if(height_x%size != 0 or width_x%size != 0):\n",
        "    return (\"size of pooling layer is not compatable with size of x\")\n",
        "\n",
        "  output = np.zeros(shape=(height_x//size, width_x//size))\n",
        "\n",
        "  out_h = 0\n",
        "  for i in range(0, height_x, size):\n",
        "    out_w = 0\n",
        "    for j in range(0, width_x, size):\n",
        "      # getting maximum number from the window of size*size\n",
        "      max_num = float('-inf')\n",
        "      for k in range(size):\n",
        "        for l in range(size):\n",
        "            if x[i+k][j+l] > max_num:\n",
        "                max_num = x[i+k][j+l]\n",
        "\n",
        "      # putting the maximum number that we get from window into output matrix\n",
        "      output[out_h][out_w] = max_num\n",
        "      out_w+=1\n",
        "    out_h+=1\n",
        "\n",
        "  return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaDna8uTp3YE",
        "outputId": "bf92959d-5531-42f8-fede-2a57d3e88b58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8., 6.],\n",
              "       [9., 9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x = np.array([[7,3,5,2], [8,7,1,6], [4,9,3,9], [0,8,4,5]])\n",
        "max_poop = maxpool(x,2)\n",
        "max_poop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW8pXbGIMT3Z"
      },
      "outputs": [],
      "source": [
        "class Maxpool:\n",
        "\n",
        "  def __init__(self,size_of_window:int=1):\n",
        "    self.size_of_window = size_of_window\n",
        "\n",
        "\n",
        "  def forward(self,input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This forward layer will take maximum from the feature in window of size = size_of_window .\n",
        "\n",
        "    Args:\n",
        "         input : numpy input array that we have to pass to maxpool layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "\n",
        "    # calculating the height and width of the input matrix\n",
        "    height_input = self.input.shape[0]\n",
        "    width_input = self.input.shape[1]\n",
        "\n",
        "    # if the size_of_window is not compatiable with the height and width of input matrix\n",
        "    # then return .\n",
        "    if(height_input%self.size_of_window != 0 or width_input%self.size_of_window != 0):\n",
        "      return (\"size of pooling layer is not compatable with size of x\")\n",
        "\n",
        "    # defining a output zeros matrix with size height_input//size_of_window , width_input//size_of_window\n",
        "    self.output = np.zeros(shape=(height_input//self.size_of_window, width_input//self.size_of_window))\n",
        "\n",
        "    # take a index at 0 for row of output matrix\n",
        "    out_h = 0\n",
        "    # looping through the row of input matrix by step = size of window\n",
        "    for i in range(0, height_input, self.size_of_window):\n",
        "      # take a index of width = 0 for column of output matrix\n",
        "      out_w = 0\n",
        "      # looping through the every column for each row of input matrix with step = size of window\n",
        "      for j in range(0, width_input, self.size_of_window):\n",
        "        # getting maximum number from the window of size*size from the input matrix\n",
        "        max_num = float('-inf')\n",
        "        for k in range(self.size_of_window):\n",
        "          for l in range(self.size_of_window):\n",
        "              if self.input[i+k][j+l] > max_num:\n",
        "                  max_num = self.input[i+k][j+l]\n",
        "\n",
        "        # putting the maximum number that we get from window into output matrix\n",
        "        self.output[out_h][out_w] = max_num\n",
        "        # incrementing the column of output matrix by 1 so that we can put next element there\n",
        "        out_w+=1\n",
        "      # incrementing the row of output matrix after going through every column for previous row\n",
        "      out_h+=1\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "      this function will calculate backward propogation for maxpool layer.\n",
        "      this function will give output such that it will put prev_grad at maximum\n",
        "      in window and zero else where for every window.\n",
        "\n",
        "      Args :\n",
        "            prev_grad : gradient of the prev layer\n",
        "\n",
        "    \"\"\"\n",
        "    # take a numpy array of size input.shape with all zeros .\n",
        "    self.grad = np.zeros(self.input.shape)\n",
        "\n",
        "      # calculating the height and width of the input matrix\n",
        "    height_input = self.input.shape[0]\n",
        "    width_input = self.input.shape[1]\n",
        "\n",
        "    # take a index at 0 for row of prev_grad matrix\n",
        "    out_h = 0\n",
        "    # looping through the row of input matrix by step = size of window\n",
        "    for i in range(0, height_input, self.size_of_window):\n",
        "      # take a index of width = 0 for column of output matrix\n",
        "      out_w = 0\n",
        "      # looping through the every column for each row of input matrix with step = size of window\n",
        "      for j in range(0, width_input, self.size_of_window):\n",
        "        # getting maximum number from the window of size*size from the input matrix\n",
        "        max_num = float('-inf')\n",
        "        max_x = 0\n",
        "        max_y = 0\n",
        "        for k in range(self.size_of_window):\n",
        "          for l in range(self.size_of_window):\n",
        "              if self.input[i+k][j+l] > max_num:\n",
        "                  max_num = self.input[i+k][j+l]\n",
        "                  max_x = i+k\n",
        "                  max_y = j+l\n",
        "\n",
        "        # putting the value from prev_grad in grad matrix on that index\n",
        "        # that will give max value\n",
        "        self.grad[max_x][max_y] = prev_grad[out_h][out_w]\n",
        "        # incrementing the column of prev_grad matrix by 1 so that we can put next element there\n",
        "        out_w+=1\n",
        "      # incrementing the row of prev_grad matrix after going through every column for previous row\n",
        "      out_h+=1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HQkFg5rvk1T",
        "outputId": "6bc65ea6-65d2-4120-bcc2-f502bf9a428e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7  3  5 21]\n",
            " [ 8  7  1  6]\n",
            " [ 4  9  3  9]\n",
            " [ 0  8  4  5]]\n",
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[7,3,5,21], [8,7,1,6], [4,9,3,9], [0,8,4,5]])\n",
        "print(x)\n",
        "pool_layer = Maxpool(2)\n",
        "ans1 = pool_layer.forward(x)\n",
        "print(ans1)\n",
        "\n",
        "prev_grad = np.array([[0.1, 0.8], [0.25, 0.83]])\n",
        "ans2 = pool_layer.backward(prev_grad)\n",
        "print(ans2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sugDgTck0TyQ"
      },
      "source": [
        "### Flatten layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGMGUgLfqrq-"
      },
      "outputs": [],
      "source": [
        "def flatten(x):\n",
        "  output = []\n",
        "\n",
        "  for row in range(x.shape[0]):\n",
        "    for col in range(x.shape[1]):\n",
        "      output.append(x[row][col])\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8KlOk6_fWZe"
      },
      "outputs": [],
      "source": [
        "class Flatten:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "\n",
        "  def forward(self,input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    After passing input image through all the convoulation layer , now we have to give it to fully\n",
        "    connected layer but before that first we will change it into 1D so that we can learn all the\n",
        "    kernel parameter\n",
        "\n",
        "    Args :\n",
        "          input : numpy array of previous layer that we have to pass through the\n",
        "                  flatten layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "\n",
        "    # changing the 2D input numpy array into 1D input array\n",
        "    self.output =  self.input.reshape(1,-1)\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    From fully connected layer we will get gradient of loss . Now we will just reshape it into\n",
        "    shape of input and pass it .\n",
        "\n",
        "    Args :\n",
        "          numpy array of gradient of loss w.r.t to previous layer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # resizing the prev_grad into the shape of input\n",
        "    self.grad =  prev_grad.reshape(self.input.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlz-DFMnXirg",
        "outputId": "2fb5bbfd-b92e-452a-fbaf-e46d8b38b7c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7, 3, 5, 2, 8, 7, 1, 6, 4, 9, 3, 9, 0, 8, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x2 = np.array([[7,3,5,2], [8,7,1,6], [4,9,3,9], [0,8,4,5]])\n",
        "x2.reshape(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CixYkZvxekM",
        "outputId": "592961ec-5f08-4238-8bd0-dc91dc8cae06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "x2 = np.array([[7,3,5,2], [8,7,1,6], [4,9,3,9], [0,8,4,5]])\n",
        "fl = Flatten()\n",
        "res = fl.forward(x2)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcjCA4IHEwM",
        "outputId": "cb13b28e-d631-4178-b6ac-e686fa9af8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -5.47018296  43.19784916 -34.13356532  -3.5211673  -17.02363622]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([7, 3, 5, 2, 8, 7, 1, 6, 4, 9, 3, 9, 0, 8, 4, 5])\n",
        "w = np.random.randn(5,16)\n",
        "\n",
        "y = x@(w.T)\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AoDtjUUxmIk"
      },
      "outputs": [],
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8nC_I064vr_"
      },
      "outputs": [],
      "source": [
        "def create_data(points, classes):\n",
        "  X = np.zeros((points*classes, 2))\n",
        "  y = np.zeros(points*classes, dtype=\"int8\")\n",
        "  for class_number in range(classes):\n",
        "    ix = range(points*class_number, points*(class_number+1))\n",
        "    r = np.linspace(0.0, 1, points)\n",
        "    t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
        "    X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "    y[ix] = class_number\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "vs8LJDdb8ALe",
        "outputId": "320bb8ec-136b-4de3-9205-a0a571d92c33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "X,y = create_data(100,3)\n",
        "plt.scatter(X[:,0], X[:,1], c = y, cmap=\"brg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzAgGcDa0bGJ"
      },
      "source": [
        "### Softmax layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkbuJI4zVVA7"
      },
      "outputs": [],
      "source": [
        "def softmax(inputs):\n",
        "  # substracting the maximum value of each row from every element of input row to avoid\n",
        "  # explosion of value\n",
        "  exp_values = np.exp(x - np.max(x , axis=1, keepdims=True))\n",
        "  proba = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "  return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veehJQHSdL83",
        "outputId": "7a00ec52-a59d-41cb-e010-c7e07c2a624f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.70538451, 0.25949646, 0.03511903],\n",
              "       [0.01794253, 0.00242826, 0.97962921],\n",
              "       [0.48785555, 0.48785555, 0.0242889 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "x = np.array([[5, 4, 2], [4, 2, 8], [4, 4, 1]])\n",
        "x = softmax(x)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okYfcamKd6Vo"
      },
      "outputs": [],
      "source": [
        "class Activation_Softmax:\n",
        "  def forward(self, inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    to calculate the probablity we pass the output of the last layer to the softmax\n",
        "\n",
        "    Args:\n",
        "         output numpy array of the last layer\n",
        "\n",
        "    Return:\n",
        "           numpy array after transformation\n",
        "    \"\"\"\n",
        "    # substracting maximum value from each value to avoid exploding exponential value\n",
        "    exp_values = np.exp(inputs - np.max(inputs))\n",
        "    self.output = exp_values / np.sum(exp_values)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, doutput):\n",
        "    # Create empty array to hold gradient of inputs\n",
        "    self.dinputs = np.empty_like(doutput)\n",
        "\n",
        "    # Number of samples\n",
        "    samples = len(doutput)\n",
        "\n",
        "    # Iterate over each sample's output and gradient\n",
        "    for idx, (output, grad_output) in enumerate(zip(self.output, doutput)):\n",
        "      # Reshape output and gradient to 2D arrays\n",
        "      output = output.reshape(-1, 1)\n",
        "      grad_output = grad_output.reshape(-1, 1)\n",
        "\n",
        "      # Create the Jacobian matrix of the softmax function\n",
        "      jacobian_matrix = np.diagflat(output) - np.dot(output, output.T)\n",
        "\n",
        "      # Calculate the gradient of inputs using the chain rule\n",
        "      self.dinputs[idx] = np.dot(jacobian_matrix, grad_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lrHJ7NY0ieB"
      },
      "source": [
        "### cross entropy loss layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSlmhgW6zp5N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Loss_Crossentropy():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.y_pred = None\n",
        "    self.y_true = None\n",
        "\n",
        "  def forward(self, y_pred:np.ndarray, y_true:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Function to calculate the cross entropy loss between y_pred and y_true\n",
        "\n",
        "    Args:\n",
        "        y_pred : numpy array of output from previous layer .\n",
        "        y_true : numpy array of correct label . Should be 1D or 2D\n",
        "\n",
        "    Return :\n",
        "            It will return numpy array containing loss of the network .\n",
        "    \"\"\"\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "\n",
        "    # calculating the number of samples\n",
        "    samples = len(y_pred)\n",
        "    # if it is 0 in the y_pred then there will be problem while calculating log.\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "    # if the true value in 1D like [1,2,0,1]\n",
        "    if len(y_true.shape) == 1:\n",
        "      # only taking the proba of index for which there is true value\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "    # if the true value in 2D i.e one hot encoding like [[1,0,0],[0,0,1],[0,1,0]]\n",
        "    elif len(y_true.shape) == 2:\n",
        "      # multiplying true value with predicted value . It will result in for each list in list\n",
        "      # all value will be zero but one value so doing sum will not affect result but decrease dim\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    self.output =  np.mean(negative_log_likelihoods)\n",
        "\n",
        "  def backward(self) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Function to calculate the gradients of the loss with respect to the inputs (y_pred).\n",
        "\n",
        "    Args:\n",
        "        y_pred : numpy array of output from previous layer.\n",
        "        y_true : numpy array of correct label. Should be 1D or 2D.\n",
        "\n",
        "    \"\"\"\n",
        "    # calculating the number of samples\n",
        "    samples = len(self.y_pred)\n",
        "\n",
        "    # creating an array of zeros with the same shape as y_pred\n",
        "    self.grad = np.zeros_like(self.y_pred)\n",
        "\n",
        "    # if the true value is in 1D like [1,2,0,1]\n",
        "    if len(self.y_true.shape) == 1:\n",
        "      # for each sample, set the gradient of the correct class to -1 / correct_confidence\n",
        "      self.grad[range(samples), self.y_true] = -1 / self.y_pred[range(samples), self.y_true]\n",
        "\n",
        "    # if the true value is in 2D i.e., one-hot encoding like [[1,0,0],[0,0,1],[0,1,0]]\n",
        "    elif len(self.y_true.shape) == 2:\n",
        "      # for each sample, divide the predicted probabilities by the true values\n",
        "      self.grad = -self.y_true / self.y_pred\n",
        "\n",
        "    # normalize the gradients by the number of samples\n",
        "    self.grad /= samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqPr9oiAqlC0"
      },
      "outputs": [],
      "source": [
        "# X, y = create_data(100, 3)\n",
        "# print(f\"shape of X: {X.shape}\")\n",
        "# dense1 = Layer_Dense(2,3)\n",
        "# activation1 = Activation_ReLU()\n",
        "\n",
        "# dense2 = Layer_Dense(3,3)\n",
        "# activation2 = Activation_Softmax()\n",
        "\n",
        "# dense1.forward(X)\n",
        "# print(f\"shape after dense1: {dense1.output.shape}\")\n",
        "# activation1.forward(dense1.output)\n",
        "# print(f\"shape after activation1: {activation1.output.shape}\")\n",
        "\n",
        "# dense2.forward(activation1.output)\n",
        "# print(f\"shape after dense2: {dense2.output.shape}\")\n",
        "# activation2.forward(dense2.output)\n",
        "# print(f\"shape after activation2: {activation2.output.shape}\")\n",
        "\n",
        "# print(activation2.output[:5])\n",
        "\n",
        "# loss_function = Loss_CategoricalCrossentropy()\n",
        "# loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "# print(f\"Loss : {loss}\")\n",
        "\n",
        "# print(f\"len(y_pred): {len(activation2.output)}\")\n",
        "# print(f\"len of y: {len(y.shape)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq2hxbcSud48",
        "outputId": "5e0eeca4-8f1d-4326-896a-07d99de70af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_pred = np.array([[0.0, 0.29754023, 0.23773866],\n",
        "          [0.21735772, 0.0,  0.56863068],\n",
        "          [0.38036754, 0.38036754, 0.0]])\n",
        "\n",
        "y_true = np.array([1,2,3])\n",
        "\n",
        "# y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "# print(y_pred_clipped)\n",
        "# samples = len(y_pred)\n",
        "# print(f\"len(y_pred): {samples}\")\n",
        "# y_true = [1,2,1]\n",
        "\n",
        "# correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "print(y_true.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh8L00s9FUk6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335Znvuj0oQS"
      },
      "source": [
        "### Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNjw0K-INK-v"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, in_feature:int, out_feature:int):\n",
        "    self.in_feature = in_feature\n",
        "    self.out_feature = out_feature\n",
        "    self.weight = np.random.randn(self.out_feature,self.in_feature)\n",
        "    self.bias = np.random.randn(self.out_feature)\n",
        "\n",
        "  def forward(self, input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    this function will calculate y i.e x@w^T + bias\n",
        "\n",
        "    Args :\n",
        "          input : input numpy array (x)\n",
        "\n",
        "    Return : numpy array after linear transformation\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    self.output = (self.input@(self.weight.T)) + self.bias\n",
        "\n",
        "\n",
        "  def backward(self,prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    it will calculate gradient of loss w.r.t weight and bias\n",
        "\n",
        "    Args :\n",
        "          prev_grad : gradient of loss w.r.t previous layer\n",
        "\n",
        "    Return : tuple with gradient of loss w.r.t weight and bias\n",
        "    \"\"\"\n",
        "\n",
        "    self.grad_weight = (prev_grad.T)@self.input\n",
        "\n",
        "    self.grad_bias = np.sum(prev_grad)\n",
        "\n",
        "    self.grad_input = prev_grad@self.weight\n",
        "\n",
        "\n",
        "  def update(self, learning_rate):\n",
        "    self.weight -= learning_rate * self.grad_weight\n",
        "    self.bias -= learning_rate * self.grad_bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GNPMAJ-FW2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3ea675-da9d-4397-c6e2-6967d5bf58b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n",
            "(5, 3)\n",
            "[[5.19529993 6.54922293 5.42652159]]\n",
            "(1, 3)\n",
            "[[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]]\n",
            "(5, 1)\n",
            "[[-1.44743302 -2.89486604 -4.34229906 -5.78973208 -7.23716509]\n",
            " [-0.97472774 -1.94945548 -2.92418321 -3.89891095 -4.87363869]\n",
            " [-0.09364146 -0.18728291 -0.28092437 -0.37456583 -0.46820729]]\n",
            "(3, 5)\n",
            "[[ 1.61583411  0.79678868  0.8562693  -3.94133708 -0.88491527]]\n",
            "(1, 5)\n",
            "-2.5158022136131417\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[1,2,3,4,5]])\n",
        "w = np.random.randn(3,5)\n",
        "out = x@w.T\n",
        "print(x.shape)\n",
        "print(w.T.shape)\n",
        "print(out)\n",
        "print(out.shape)\n",
        "print(x.T)\n",
        "print(x.T.shape)\n",
        "prev_grad = np.random.randn(out.shape[0],out.shape[1])\n",
        "grad = prev_grad.T@x\n",
        "print(grad)\n",
        "print(grad.shape)\n",
        "grad_inp = prev_grad@w\n",
        "print(grad_inp)\n",
        "print(grad_inp.shape)\n",
        "grad_bias = np.sum(prev_grad)\n",
        "print(grad_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psDAOHt229YN"
      },
      "source": [
        "### loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO3-uDyJ4IPF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6doYgvL_3jze"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_data = datasets.MNIST(root = './data', train = True,\n",
        "#                         transform = ToTensor(), download = True)\n",
        "\n",
        "# test_data = datasets.MNIST(root = './data', train = False,\n",
        "#                        transform = ToTensor(), download = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFiRl1zqCCf5"
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n",
        "data_test = pd.read_csv('/content/sample_data/mnist_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It_DE23xCfLZ",
        "outputId": "d2b5ba2d-d954-4b9f-f167-4fd0cb35ffb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        5\n",
              "1        7\n",
              "2        9\n",
              "3        5\n",
              "4        2\n",
              "        ..\n",
              "19994    0\n",
              "19995    1\n",
              "19996    2\n",
              "19997    9\n",
              "19998    5\n",
              "Name: 6, Length: 19999, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y = data_train.iloc[:,0]\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbyNkeY6wvf3",
        "outputId": "51c58502-9237-46c9-bfd8-0fc1307aa836"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 7, 9, ..., 2, 9, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y = y.to_numpy()\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgbmz3i1vwpB",
        "outputId": "ca69f716-093a-423f-c434-97526ba2035a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# changing label into one hot encoding\n",
        "num_classes = 10\n",
        "\n",
        "# Convert y to a one-hot encoded 2D array\n",
        "y_one_hot = np.zeros((len(y), num_classes))\n",
        "y_one_hot[np.arange(len(y)), y] = 1\n",
        "y_one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdcZa8PviQw",
        "outputId": "d8994555-2970-4137-b3e4-7cc4c610ae8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  ...  0.581  0.582  \\\n",
              "0      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "2      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "4      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "...   ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
              "19994  0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "19995  0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "19996  0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "19997  0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "19998  0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "\n",
              "       0.583  0.584  0.585  0.586  0.587  0.588  0.589  0.590  \n",
              "0          0      0      0      0      0      0      0      0  \n",
              "1          0      0      0      0      0      0      0      0  \n",
              "2          0      0      0      0      0      0      0      0  \n",
              "3          0      0      0      0      0      0      0      0  \n",
              "4          0      0      0      0      0      0      0      0  \n",
              "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
              "19994      0      0      0      0      0      0      0      0  \n",
              "19995      0      0      0      0      0      0      0      0  \n",
              "19996      0      0      0      0      0      0      0      0  \n",
              "19997      0      0      0      0      0      0      0      0  \n",
              "19998      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[19999 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0531cdee-fddc-479f-8002-601ba4a31a41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>...</th>\n",
              "      <th>0.581</th>\n",
              "      <th>0.582</th>\n",
              "      <th>0.583</th>\n",
              "      <th>0.584</th>\n",
              "      <th>0.585</th>\n",
              "      <th>0.586</th>\n",
              "      <th>0.587</th>\n",
              "      <th>0.588</th>\n",
              "      <th>0.589</th>\n",
              "      <th>0.590</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19999 rows  784 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0531cdee-fddc-479f-8002-601ba4a31a41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0531cdee-fddc-479f-8002-601ba4a31a41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0531cdee-fddc-479f-8002-601ba4a31a41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-29a3b116-17d3-428e-bab3-aab2b5b66262\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29a3b116-17d3-428e-bab3-aab2b5b66262')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-29a3b116-17d3-428e-bab3-aab2b5b66262 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X = data_train.iloc[:,1:]\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l9X93MRwM8A"
      },
      "outputs": [],
      "source": [
        "X = X.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NulKND6_v2D5",
        "outputId": "005269b4-6fc7-4ce2-cd0c-e87ec370e7c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X = X.reshape(-1,28,28)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNwTJewqwm0b",
        "outputId": "3c9be244-4c77-4687-df98-8740383c010e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19999, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odW_Bv4xqjf0",
        "outputId": "26a43a54-7fe3-422a-c6ff-29fdb48f1727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19999, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since the number of data is incompatible with good batch size , we will last row from data\n",
        "# to get 19998 data then will will divide it into batch size of 66\n",
        "# Remove the last row from X\n",
        "X_new = X[:-1, :, :]\n",
        "y_one_hot_new = y_one_hot[:-1, :]\n",
        "\n",
        "print(X_new.shape)\n",
        "print(y_one_hot_new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEwM7LBWsGQC",
        "outputId": "329235cf-922b-4a58-be17-d9ac7a50b11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19998, 28, 28)\n",
            "(19998, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing the data into batches\n",
        "X_batch = X_new.reshape(-1,33,28,28)\n",
        "y_batch = y_one_hot_new.reshape(-1,33,10)\n",
        "\n",
        "print(X_batch.shape)\n",
        "print(y_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdn4_HsOpFN7",
        "outputId": "84b0653e-04fe-400a-e852-85ed4f57e1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(606, 33, 28, 28)\n",
            "(606, 33, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3dqbZUbxhgr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUR3lbX-x3FW"
      },
      "outputs": [],
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X_batch,y_batch,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViLGzL7nyQ_s",
        "outputId": "5bc8111c-e90e-4daf-d6b4-8f10439502b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(424, 33, 28, 28)\n",
            "(424, 33, 10)\n",
            "(182, 33, 28, 28)\n",
            "(182, 33, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwutzqml0tQP"
      },
      "source": [
        "### fully intergrated architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXf_anssuXrj"
      },
      "outputs": [],
      "source": [
        "class FirstNetwork:\n",
        "  def __init__(self):\n",
        "    self.conv1 = Convoluation((2,2),0,1)\n",
        "    self.act1 = Activation_ReLU()\n",
        "\n",
        "    self.conv2 = Convoluation((2,2),0,1)\n",
        "    self.act2 = Activation_ReLU()\n",
        "\n",
        "    self.pool = Maxpool(2)\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.linear1 = Linear(169,80)\n",
        "    self.act3 = Activation_ReLU()\n",
        "\n",
        "    self.linear2 = Linear(80,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.conv1.forward(x)\n",
        "    self.act1.forward(self.conv1.output)\n",
        "\n",
        "    self.conv2.forward(self.act1.output)\n",
        "    self.act2.forward(self.conv2.output)\n",
        "\n",
        "    self.pool.forward(self.act2.output)\n",
        "    self.flatten.forward(self.pool.output)\n",
        "\n",
        "    self.linear1.forward(self.flatten.output)\n",
        "    self.act3.forward(self.linear1.output)\n",
        "\n",
        "    self.linear2.forward(self.act3.output)\n",
        "\n",
        "    return self.linear2.output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMv_x4Zyz7MM"
      },
      "outputs": [],
      "source": [
        "def backpropogation(network, loss_fn, learning_rate=0.1):\n",
        "  \"\"\"\n",
        "  this function will do backpropogation and then update the weight and bias of the layers\n",
        "\n",
        "  Args :\n",
        "        network : instance of the model\n",
        "        loss_fn : instance of the loss function\n",
        "        learning_rate : it will use for weight updation\n",
        "  \"\"\"\n",
        "  # backward pass\n",
        "  loss_fn.backward()\n",
        "  network.linear2.backward(loss_fn.grad)\n",
        "  network.act3.backward(network.linear2.grad_input)\n",
        "  network.linear1.backward(network.act3.grad)\n",
        "\n",
        "  network.flatten.backward(network.linear1.grad_input)\n",
        "  network.pool.backward(network.flatten.grad)\n",
        "  network.act2.backward(network.pool.grad)\n",
        "  network.conv2.backward(network.act2.grad)\n",
        "  network.act1.backward(network.conv2.grad_input)\n",
        "  network.conv1.backward(network.act1.grad)\n",
        "\n",
        "  # update weight and bias of convolutation and linear layer\n",
        "  # print(f\"bias of the linear2 before updating: {network.linear2.bias}\")\n",
        "  # print(f\"weight of the linear2 layer before updating {network.linear2.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.linear2.update(learning_rate)\n",
        "  # print(f\"bias of the linear2 after updating: {network.linear2.bias}\")\n",
        "  # print(f\"weight of the linear2 layer after updating {network.linear2.weight}\")\n",
        "\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "\n",
        "  # print(f\"bias of the linear1 before updating: {network.linear1.bias}\")\n",
        "  # print(f\"weight of the linear1 layer before updating {network.linear1.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.linear1.update(learning_rate)\n",
        "  # print(f\"bias of the linear1 after updating: {network.linear1.bias}\")\n",
        "  # print(f\"weight of the linear1 layer after updating {network.linear1.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.conv2.update(learning_rate)\n",
        "  network.conv1.update(learning_rate)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yh08l9K2OCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d75f22-e2d9-4f8a-92ac-1e0acfa5e6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(424, 33, 28, 28)\n",
            "(424, 33, 10)\n"
          ]
        }
      ],
      "source": [
        "# we will train the model on X_train and y_train\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will take each batch and then run the forward method of model , then calculate the loss then\n",
        "# we will run backpropogation , again next batch of data will be use in same sequence\n",
        "# in that way our model will be train\n",
        "\n"
      ],
      "metadata": {
        "id": "c0TFgoxmvGxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_new.shape)\n",
        "print(y_one_hot_new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St8JrDHn3rxa",
        "outputId": "3cf802f4-ca89-4d72-8292-8727c98b3187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19998, 28, 28)\n",
            "(19998, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_temp = X_new[:10000, : , :]\n",
        "y_temp = y_one_hot_new[:10000, :]\n",
        "print(X_temp.shape)\n",
        "print(y_temp.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq7uUpCm4ERB",
        "outputId": "65d74b9e-c3b9-4656-f487-965c30311b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 28, 28)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [[0,0,1,0,0]]\n",
        "y_pred = [[0.5,0.2,0.1,0.1,0.2]]\n",
        "\n",
        "y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "print(correct_confidences)\n",
        "negative_log_likelihoods = -np.log(correct_confidences)\n",
        "print(negative_log_likelihoods)\n",
        "output =  np.mean(negative_log_likelihoods)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ui4Zg7jxp4",
        "outputId": "50c11844-0a83-4eb0-80b3-ba2a43b2141b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1]\n",
            "[2.30258509]\n",
            "2.3025850929940455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNWZTUKjvcRl"
      },
      "outputs": [],
      "source": [
        "# # Now we will use SGD method i.e we will take every data and then train the model\n",
        "\n",
        "# # get a instance of FirstNetwork and loss function\n",
        "# model1 = FirstNetwork()\n",
        "# loss_fn = Loss_Crossentropy()\n",
        "\n",
        "\n",
        "\n",
        "# # Iterate through each data point in X\n",
        "# for i in range(10000):\n",
        "\n",
        "#   data_X = X_temp[i]\n",
        "#   label_y = y_temp[i]\n",
        "\n",
        "#   # Get the output for the current data point using the network's forward method\n",
        "#   output = model1.forward(data_X)\n",
        "\n",
        "#   # Label y will be 1D , change it into 2D then calculate the loss\n",
        "#   loss_fn.forward(output,label_y.reshape(1,-1))\n",
        "\n",
        "#   if i%100 == 0:\n",
        "\n",
        "#     print(f\"loss: {loss_fn.output}\")\n",
        "\n",
        "#   # do backpropogation to update the weight and bias of the network\n",
        "#   backpropogation(network=model1,loss_fn=loss_fn,learning_rate=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying above code for batch training"
      ],
      "metadata": {
        "id": "PNolUz29qXAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv layer"
      ],
      "metadata": {
        "id": "p--BqhTbT3tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convoluation:\n",
        "\n",
        "  def __init__(self,filter_size:tuple=(2,2),padding_layer:int=0,stride:int=1):\n",
        "    # initialize the filter using He initialization method .\n",
        "    #Normal distribution (0 , sd = sqrt(filter_height* filter_width))\n",
        "    fan_in = filter_size[0]*filter_size[1]\n",
        "    stddev = np.sqrt(2.0/fan_in)\n",
        "    self.filter = np.random.normal(loc=0, scale=stddev ,size=(filter_size[0],filter_size[1])).astype(np.float64)\n",
        "    self.bias = 0.0\n",
        "    self.padding_layer = padding_layer\n",
        "    self.stride = stride\n",
        "\n",
        "\n",
        "  def padding(self,x, pad_size):\n",
        "    \"\"\"\n",
        "    Pad a 2D numpy array 'x' with zeros.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): Input 2D array.\n",
        "    pad_size (int): Number of rows/columns to pad on each side.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Padded 2D array.\n",
        "    \"\"\"\n",
        "    return np.pad(x, pad_size, mode='constant')\n",
        "\n",
        "\n",
        "  def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This function will do convolution operation on input array with filter\n",
        "    and will also add bias after it.\n",
        "\n",
        "    Args:\n",
        "        input: input array (image) with shape (batch_size, height, width)\n",
        "\n",
        "    Returns:\n",
        "        Output array after convolution with shape (batch_size, output_height, output_width)\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    batch_size, input_height, input_width = input.shape\n",
        "\n",
        "    # Calculate the output height and width based on filter size and padding\n",
        "    output_height = int(((input_height - self.filter.shape[0] + (2 * self.padding_layer)) / self.stride) + 1)\n",
        "    output_width = int(((input_width - self.filter.shape[1] + (2 * self.padding_layer)) / self.stride) + 1)\n",
        "\n",
        "    # Initialize the output array with zeros\n",
        "    self.output = np.zeros((batch_size, output_height, output_width), dtype=np.float64)\n",
        "\n",
        "    # Loop over each input in the batch\n",
        "    for batch_idx in range(batch_size):\n",
        "        # Get the current input from the batch\n",
        "        current_input = input[batch_idx]\n",
        "\n",
        "        # If we want to do padding around the input\n",
        "        pad_input = self.padding(current_input, self.padding_layer)\n",
        "\n",
        "        # For putting element in each row of output array\n",
        "        out_h = 0\n",
        "        # Loop through the pad_input array in rows\n",
        "        for i in range(0, pad_input.shape[0] - self.filter.shape[0] + 1, self.stride):\n",
        "            # For putting element in column in output array\n",
        "            out_w = 0\n",
        "            # Loop through the pad_input column\n",
        "            for j in range(0, pad_input.shape[1] - self.filter.shape[1] + 1, self.stride):\n",
        "                # Loop through all rows and columns of the filter row-wise\n",
        "                for f_h in range(self.filter.shape[0]):\n",
        "                    for f_w in range(self.filter.shape[1]):\n",
        "                        # Putting element in output array after convolution (X * w)\n",
        "                        self.output[batch_idx][out_h][out_w] += self.filter[f_h][f_w] * pad_input[i + f_h][j + f_w]\n",
        "                out_w += 1\n",
        "            out_h += 1\n",
        "\n",
        "        # Add bias to the output\n",
        "        self.output[batch_idx] += self.bias\n",
        "\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad: np.ndarray):\n",
        "    \"\"\"\n",
        "    This backward function will calculate the gradient of loss w.r.t weight and bias.\n",
        "\n",
        "    Args:\n",
        "        prev_grad: It is the gradient of loss w.r.t next layer. It will be used in\n",
        "                  calculating the gradient of loss wr.t weight and bias.\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size, output_height, output_width = prev_grad.shape\n",
        "\n",
        "    # First try to find out the gradient of loss w.r.t bias\n",
        "    self.grad_bias = np.sum(prev_grad, axis=(1, 2), dtype=np.float64)\n",
        "\n",
        "    # Now try to find out the gradient of loss w.r.t weight\n",
        "    # It will be convolution operation of padded input array and prev_grad array\n",
        "    self.grad_weight = np.zeros((batch_size, self.filter.shape[0], self.filter.shape[1]), dtype=np.float64)\n",
        "\n",
        "    # doing just to get shape of input after padding\n",
        "    # If we want to do padding around the input\n",
        "    pad_input = self.padding(self.input[0], self.padding_layer)\n",
        "\n",
        "    # Lists to store gradients for each input in the batch\n",
        "    self.grad_input = np.zeros((batch_size , pad_input.shape[0], pad_input.shape[1]), dtype=np.float64)\n",
        "\n",
        "    # Loop over each input in the batch\n",
        "    for batch_idx in range(batch_size):\n",
        "      # Get the current input from the batch\n",
        "      current_input_without_pad = self.input[batch_idx]\n",
        "\n",
        "      # If we want to do padding around the input , it will become current input\n",
        "      current_input = self.padding(current_input_without_pad, self.padding_layer)\n",
        "\n",
        "      # Get the padded gradient from the batch\n",
        "      current_prev_grad = prev_grad[batch_idx]\n",
        "\n",
        "      for row in range(current_prev_grad.shape[0]):\n",
        "        for col in range(current_prev_grad.shape[1]):\n",
        "          vertical_start = row*self.stride\n",
        "          vertical_end = vertical_start + self.filter.shape[0]\n",
        "          horizontal_start = col*self.stride\n",
        "          horizontal_end = horizontal_start + self.filter.shape[1]\n",
        "\n",
        "          slice_input = current_input[vertical_start:vertical_end, horizontal_start:horizontal_end]\n",
        "\n",
        "          self.grad_weight[batch_idx] += slice_input*current_prev_grad[row,col]\n",
        "\n",
        "      # Finding the gradient of loss w.r.t input.\n",
        "      # Padding around the prev_grad so that we can do full convolution.\n",
        "      pad_prev_grad = self.padding(current_prev_grad, self.filter.shape[0] - 1)\n",
        "\n",
        "      # Rotate the filter by 180 degrees for convolution\n",
        "      rotated_filter = np.rot90(self.filter, 2)\n",
        "\n",
        "      row_grad = 0  # For looping through all the row of prev_grad\n",
        "      # Loop through all the row of pad_prev_grad array\n",
        "      for i in range(0, pad_prev_grad.shape[0] - self.filter.shape[0] + 1, 1):\n",
        "          col_grad = 0  # For looping through all the column of prev_grad\n",
        "          # Loop through all the column of padded input array\n",
        "          for j in range(0, pad_prev_grad.shape[1] - self.filter.shape[1] + 1, 1):\n",
        "              # Loop through all rows and columns of the prev_grad row-wise\n",
        "              for f_h in range(self.filter.shape[0]):\n",
        "                  for f_w in range(self.filter.shape[1]):\n",
        "                      # Putting element in grad_input array after convolution (input, prev_grad)\n",
        "                      self.grad_input[batch_idx][row_grad][col_grad] += rotated_filter[f_h][f_w] * pad_prev_grad[i + f_h][j + f_w]\n",
        "              col_grad += 1\n",
        "          row_grad += 1\n",
        "\n",
        "\n",
        "   ### Above function backward will produce three things \\\n",
        "      #  1. self.grad_input\n",
        "      #  2. self.grad_weight\n",
        "      #  3. self.grad_bias\n",
        "\n",
        "  def update(self, learning_rate):\n",
        "    \"\"\"\n",
        "    This will take the learning rate as a parameter and will update the weight and bias of the filter and layer.\n",
        "    \"\"\"\n",
        "    # Update the weight and bias for the current input in the batch\n",
        "    self.filter -= learning_rate * np.mean(self.grad_weight, axis = 0, dtype=np.float64)\n",
        "    self.bias -= learning_rate * np.mean(self.grad_bias, axis = 0, dtype=np.float64)\n",
        "\n"
      ],
      "metadata": {
        "id": "MvTN6prIqhQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def padding_1d(x,layer):\n",
        "#     z = np.array([0])\n",
        "#     z = np.repeat(z, layer)\n",
        "#     return np.concatenate([z, x, z])\n",
        "\n",
        "# def padding2( x,layer):\n",
        "#   outs = np.array([padding_1d(row,layer) for row in x])\n",
        "#   zero_layer = np.array([0])\n",
        "#   zero_layer = np.array(np.repeat(zero_layer, len(outs[0]))).reshape(1,-1)\n",
        "#   zero_layer = np.repeat(zero_layer,layer,axis=0)\n",
        "#   print(zero_layer)\n",
        "#   print(outs)\n",
        "#   full_padding = np.concatenate((zero_layer, outs, zero_layer), axis=0)\n",
        "#   return np.array(full_padding)"
      ],
      "metadata": {
        "id": "ohYIoayydM1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.array([[1,2,3,4,5], [2,3,45,6,7], [2,4,5,6,7]])\n",
        "# y = padding2(x,2)\n",
        "# print(y.shape)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "MytQPgvT4kfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(1,7,7)\n",
        "conv = Convoluation((3,3),2,2)\n",
        "conv.forward(x)\n",
        "print(f\"output: {conv.output}\")\n",
        "print(f\"conv.output.shape : {conv.output.shape}\")\n",
        "grad = np.random.randn(1,5,5)\n",
        "conv.backward(grad)\n",
        "print(f\"grad_weight: {conv.grad_weight}\")\n",
        "print(f\"grad_input: {conv.grad_input}\")\n",
        "print(f\"grad_bias: {conv.grad_bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfiF7bWfeYqu",
        "outputId": "2be70863-021b-4f14-ceb0-bdb35e3ac417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: [[[ 0.09438967 -0.56162881  1.78719433 -0.46616543 -0.22455125]\n",
            "  [ 0.0250496  -3.09121191  1.21967862  0.78536421 -1.14523854]\n",
            "  [ 0.03445551  0.65590137 -0.89512482  4.0815872  -0.42722188]\n",
            "  [ 0.11226974  1.44756196  0.28676854  1.69445247  1.17991356]\n",
            "  [-0.03801549  1.5519333  -0.26520878 -0.20433244  0.53825932]]]\n",
            "conv.output.shape : (1, 5, 5)\n",
            "grad_weight: [[[ 3.38036284 -0.55305411 -1.89369793]\n",
            "  [ 0.91265649  6.19757272  4.69235408]\n",
            "  [ 0.87837607  3.84904019  1.81495958]]]\n",
            "grad_input: [[[-0.16840694 -0.44037576  0.26744359  1.04750683  0.57689279\n",
            "    0.36651085 -0.05626907  0.          0.          0.\n",
            "    0.        ]\n",
            "  [-0.34622329 -0.14157     0.35230931  0.58790552  0.16744065\n",
            "   -0.50146814  0.12805101  0.          0.          0.\n",
            "    0.        ]\n",
            "  [-0.15491802 -1.66124003 -0.92280554  0.58789883  2.73427629\n",
            "   -0.55017995 -0.08663834  0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.41844191 -0.42836475  0.93559806 -1.58264217  2.10457687\n",
            "    0.61861151 -0.09594784  0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.68540292  0.894777   -2.53720718 -2.49771121  0.88342842\n",
            "    0.73626435  0.11188418  0.          0.          0.\n",
            "    0.        ]\n",
            "  [-0.39960694  1.61958638 -0.29899363 -0.29443474 -0.85172634\n",
            "    1.78504127 -0.22608929  0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.3082891   1.36233204 -1.1689671  -1.36971456 -0.52707556\n",
            "    0.45097141 -0.05121301  0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.          0.          0.\n",
            "    0.          0.          0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.          0.          0.\n",
            "    0.          0.          0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.          0.          0.\n",
            "    0.          0.          0.          0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.          0.          0.\n",
            "    0.          0.          0.          0.          0.\n",
            "    0.        ]]]\n",
            "grad_bias: [0.84209562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maxpool layer"
      ],
      "metadata": {
        "id": "MTyLe84sxGWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Maxpool:\n",
        "\n",
        "  def __init__(self,size_of_window:int=1):\n",
        "    self.size_of_window = size_of_window\n",
        "\n",
        "\n",
        "  def forward(self,input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This forward layer will take maximum from the feature in window of size = size_of_window .\n",
        "\n",
        "    Args:\n",
        "         input : numpy input array that we have to pass to maxpool layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    batch_size = self.input.shape[0]\n",
        "\n",
        "    # calculating the height and width of the input matrix\n",
        "    height_input = self.input.shape[1]\n",
        "    width_input = self.input.shape[2]\n",
        "\n",
        "    # if the size_of_window is not compatiable with the height and width of input matrix\n",
        "    # then return .\n",
        "    if(height_input%self.size_of_window != 0 or width_input%self.size_of_window != 0):\n",
        "      return (\"size of pooling layer is not compatable with size of x\")\n",
        "\n",
        "    # defining a output zeros matrix with size height_input//size_of_window , width_input//size_of_window\n",
        "    self.output = np.zeros(shape=(batch_size, height_input//self.size_of_window, width_input//self.size_of_window), dtype=np.float64)\n",
        "\n",
        "    for r in range(batch_size):\n",
        "\n",
        "      # take a index at 0 for row of output matrix\n",
        "      out_h = 0\n",
        "      # looping through the row of input matrix by step = size of window\n",
        "      for i in range(0, height_input, self.size_of_window):\n",
        "        # take a index of width = 0 for column of output matrix\n",
        "        out_w = 0\n",
        "        # looping through the every column for each row of input matrix with step = size of window\n",
        "        for j in range(0, width_input, self.size_of_window):\n",
        "          # getting maximum number from the window of size*size from the input matrix\n",
        "          max_num = float('-inf')\n",
        "          for k in range(self.size_of_window):\n",
        "            for l in range(self.size_of_window):\n",
        "                if self.input[r][i+k][j+l] > max_num:\n",
        "                    max_num = self.input[r][i+k][j+l]\n",
        "\n",
        "          # putting the maximum number that we get from window into output matrix\n",
        "          self.output[r][out_h][out_w] = max_num\n",
        "          # incrementing the column of output matrix by 1 so that we can put next element there\n",
        "          out_w+=1\n",
        "        # incrementing the row of output matrix after going through every column for previous row\n",
        "        out_h+=1\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "      this function will calculate backward propogation for maxpool layer.\n",
        "      this function will give output such that it will put prev_grad at maximum\n",
        "      in window and zero else where for every window.\n",
        "\n",
        "      Args :\n",
        "            prev_grad : gradient of the prev layer\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size = self.input.shape[0]\n",
        "    # take a numpy array of size input.shape with all zeros .\n",
        "    self.grad = np.zeros((batch_size, self.input.shape[1], self.input.shape[2]), dtype=np.float64)\n",
        "\n",
        "      # calculating the height and width of the input matrix\n",
        "    height_input = self.input.shape[1]\n",
        "    width_input = self.input.shape[2]\n",
        "\n",
        "    for r in range(batch_size):\n",
        "\n",
        "      # take a index at 0 for row of prev_grad matrix\n",
        "      out_h = 0\n",
        "      # looping through the row of input matrix by step = size of window\n",
        "      for i in range(0, height_input, self.size_of_window):\n",
        "        # take a index of width = 0 for column of output matrix\n",
        "        out_w = 0\n",
        "        # looping through the every column for each row of input matrix with step = size of window\n",
        "        for j in range(0, width_input, self.size_of_window):\n",
        "          # getting maximum number from the window of size*size from the input matrix\n",
        "          max_num = float('-inf')\n",
        "          max_x = 0\n",
        "          max_y = 0\n",
        "          for k in range(self.size_of_window):\n",
        "            for l in range(self.size_of_window):\n",
        "                if self.input[r][i+k][j+l] > max_num:\n",
        "                    max_num = self.input[r][i+k][j+l]\n",
        "                    max_x = i+k\n",
        "                    max_y = j+l\n",
        "\n",
        "          # putting the value from prev_grad in grad matrix on that index\n",
        "          # that will give max value\n",
        "          self.grad[r][max_x][max_y] = prev_grad[r][out_h][out_w]\n",
        "          # incrementing the column of prev_grad matrix by 1 so that we can put next element there\n",
        "          out_w+=1\n",
        "        # incrementing the row of prev_grad matrix after going through every column for previous row\n",
        "        out_h+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "nejweroBxKFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[7,3,5,21], [8,7,1,6], [4,9,3,9], [0,8,4,5]], [[7,13,5,21], [8,7,1,16], [4,9,3,9], [10,8,4,15]]])\n",
        "pool = Maxpool(2)\n",
        "pool.forward(x)\n",
        "print(pool.output)\n",
        "prev_grad = [[[1,2], [5,3]], [[5,3], [8,34]]]\n",
        "pool.backward(prev_grad)\n",
        "print(pool.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHHyO7Vjcbpj",
        "outputId": "df4e9477-c59c-4068-e576-9fdbdf6aa537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 8. 21.]\n",
            "  [ 9.  9.]]\n",
            "\n",
            " [[13. 21.]\n",
            "  [10. 15.]]]\n",
            "[[[ 0.  0.  0.  2.]\n",
            "  [ 1.  0.  0.  0.]\n",
            "  [ 0.  5.  0.  3.]\n",
            "  [ 0.  0.  0.  0.]]\n",
            "\n",
            " [[ 0.  5.  0.  3.]\n",
            "  [ 0.  0.  0.  0.]\n",
            "  [ 0.  0.  0.  0.]\n",
            "  [ 8.  0.  0. 34.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten layer"
      ],
      "metadata": {
        "id": "b_civ4YDtIV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "\n",
        "  def forward(self,input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    After passing input image through all the convoulation layer , now we have to give it to fully\n",
        "    connected layer but before that first we will change it into 1D so that we can learn all the\n",
        "    kernel parameter\n",
        "\n",
        "    Args :\n",
        "          input : numpy array of previous layer that we have to pass through the\n",
        "                  flatten layer\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    batch_size = self.input.shape[0]\n",
        "\n",
        "    # I don't know what is the flatten dim of input so I will use temp variable to\n",
        "    # calcualte the dim of single input after flatting and the use info to create output\n",
        "    temp = input[0].reshape(1,-1)              # just for knowing dim . To use in output\n",
        "    self.output = np.zeros((batch_size, 1, temp.shape[1]))\n",
        "\n",
        "    # going through the each input of batches one by one\n",
        "    for i in range(batch_size):\n",
        "      # changing the 2D input numpy array into 1D input array\n",
        "      self.output[i] =  self.input[i].reshape(1,-1)\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    From fully connected layer we will get gradient of loss . Now we will just reshape it into\n",
        "    shape of input and pass it .\n",
        "\n",
        "    Args :\n",
        "          numpy array of gradient of loss w.r.t to previous layer\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size = self.input.shape[0]\n",
        "\n",
        "    self.grad = np.zeros((batch_size, self.input.shape[1], self.input.shape[2]), dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "\n",
        "      # resizing the prev_grad into the shape of input\n",
        "      self.grad[i] =  prev_grad[i].reshape(self.input.shape[1], self.input.shape[2])\n",
        "\n"
      ],
      "metadata": {
        "id": "gATrF-fWtNfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[1,2,3],[2,34,4],[3,2,4]], [[2,3,4],[2,4,5],[5,4,5]]])\n",
        "flatten = Flatten()\n",
        "flatten.forward(x)\n",
        "print(flatten.output)\n",
        "print(\"-------------\")\n",
        "prev_grad = np.array([[[1,2,3,4,5,6,7,8,9]], [[9,8,7,6,5,4,3,2,1]]])\n",
        "flatten.backward(prev_grad)\n",
        "print(flatten.grad)"
      ],
      "metadata": {
        "id": "604JEzAgvdHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e05734d-df8c-4d66-a33e-387c402f3b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 1.  2.  3.  2. 34.  4.  3.  2.  4.]]\n",
            "\n",
            " [[ 2.  3.  4.  2.  4.  5.  5.  4.  5.]]]\n",
            "-------------\n",
            "[[[1. 2. 3.]\n",
            "  [4. 5. 6.]\n",
            "  [7. 8. 9.]]\n",
            "\n",
            " [[9. 8. 7.]\n",
            "  [6. 5. 4.]\n",
            "  [3. 2. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### linear layer"
      ],
      "metadata": {
        "id": "IqWAb_A7T-Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, in_feature:int, out_feature:int):\n",
        "    self.in_feature = in_feature\n",
        "    self.out_feature = out_feature\n",
        "    # initialize the weight using He initialization method .\n",
        "    # Taking value from the normal distribution (0, std = sqrt(2/f_in))\n",
        "    stddev = np.sqrt(2.0/in_feature)\n",
        "    self.weight = np.random.normal(loc=0, scale=stddev, size=(self.out_feature,self.in_feature)).astype(np.float64)\n",
        "    self.bias = np.zeros(shape=self.out_feature, dtype=np.float64)\n",
        "\n",
        "  def forward(self, input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    this function will calculate y i.e x@w^T + bias\n",
        "\n",
        "    Args :\n",
        "          input : input numpy array (x)\n",
        "\n",
        "    Return : numpy array after linear transformation\n",
        "    \"\"\"\n",
        "    self.input = input\n",
        "    batch_size = self.input.shape[0]\n",
        "\n",
        "    self.output = np.zeros((batch_size, 1, self.out_feature), dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.output[i] = (self.input[i]@(self.weight.T)) + self.bias\n",
        "\n",
        "\n",
        "  def backward(self,prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    it will calculate gradient of loss w.r.t weight and bias\n",
        "\n",
        "    Args :\n",
        "          prev_grad : gradient of loss w.r.t previous layer\n",
        "\n",
        "    Return : tuple with gradient of loss w.r.t weight and bias\n",
        "    \"\"\"\n",
        "    batch_size = self.input.shape[0]\n",
        "    self.grad_weight = np.zeros((batch_size, self.out_feature, self.in_feature), dtype=np.float64)     # grad_weight for each input will be (3*1)*(1*5) = (3*5) , shape of weight\n",
        "    self.grad_bias = np.zeros((batch_size, 1), dtype=np.float64)\n",
        "    self.grad_input = np.zeros((batch_size, 1, self.in_feature), dtype=np.float64)                     # grad_input for each input (1*3)*(3*5) = (1*5) , shape of the input\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.grad_weight[i] = (prev_grad[i].T)@self.input[i]\n",
        "\n",
        "      self.grad_bias[i] = np.sum(prev_grad[i])\n",
        "\n",
        "      self.grad_input[i] = prev_grad[i]@self.weight\n",
        "\n",
        "\n",
        "  def update(self, learning_rate):\n",
        "    self.weight -= learning_rate * np.mean(self.grad_weight, axis=0, dtype=np.float64)\n",
        "    self.bias -= learning_rate * np.mean(self.grad_bias, axis=0, dtype=np.float64)\n"
      ],
      "metadata": {
        "id": "ibRPFtyEUCA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_batch = np.zeros((10, 1, 3))\n",
        "# output_batch[1] = [[1,2,3]]\n",
        "# output_batch"
      ],
      "metadata": {
        "id": "wgqXNFK7URoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.array([[[1,2,3,4,5]], [[2,3,5,65,3]], [[2,4,5,6,7]],[[3,4,56,6,76]]])\n",
        "print(input.shape)\n",
        "\n",
        "lin1 = Linear(5,3)\n",
        "lin1.forward(input)\n",
        "print(lin1.output)\n",
        "print(lin1.output.shape)\n",
        "\n",
        "prev_grad = np.random.randn(4,1,3)\n",
        "lin1.backward(prev_grad)\n",
        "\n",
        "print(lin1.grad_weight)\n",
        "print(\"---------------\")\n",
        "print(lin1.grad_bias)\n",
        "print(\"--------------\")\n",
        "print(lin1.grad_input)\n"
      ],
      "metadata": {
        "id": "o1-iwmdwY1-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34771c64-6554-4129-e80a-2b7eed53fc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 1, 5)\n",
            "[[[  6.95257977   6.12325      8.31691988]]\n",
            "\n",
            " [[ 25.56594886  53.79770943   7.7429751 ]]\n",
            "\n",
            " [[ 10.56280986   9.61040132  12.78048058]]\n",
            "\n",
            " [[ 77.03032096  42.58557011 133.38945707]]]\n",
            "(4, 1, 3)\n",
            "[[[-1.76169299e+00 -3.52338598e+00 -5.28507897e+00 -7.04677196e+00\n",
            "   -8.80846496e+00]\n",
            "  [ 1.04605663e+00  2.09211325e+00  3.13816988e+00  4.18422650e+00\n",
            "    5.23028313e+00]\n",
            "  [-2.25301359e+00 -4.50602717e+00 -6.75904076e+00 -9.01205435e+00\n",
            "   -1.12650679e+01]]\n",
            "\n",
            " [[ 9.31737638e-01  1.39760646e+00  2.32934409e+00  3.02814732e+01\n",
            "    1.39760646e+00]\n",
            "  [-1.82535751e+00 -2.73803627e+00 -4.56339378e+00 -5.93241192e+01\n",
            "   -2.73803627e+00]\n",
            "  [-1.82685429e+00 -2.74028144e+00 -4.56713573e+00 -5.93727645e+01\n",
            "   -2.74028144e+00]]\n",
            "\n",
            " [[-2.91246236e+00 -5.82492472e+00 -7.28115589e+00 -8.73738707e+00\n",
            "   -1.01936183e+01]\n",
            "  [-5.74529235e-03 -1.14905847e-02 -1.43632309e-02 -1.72358770e-02\n",
            "   -2.01085232e-02]\n",
            "  [ 9.49798470e-01  1.89959694e+00  2.37449617e+00  2.84939541e+00\n",
            "    3.32429464e+00]]\n",
            "\n",
            " [[-5.30943763e+00 -7.07925017e+00 -9.91095023e+01 -1.06188753e+01\n",
            "   -1.34505753e+02]\n",
            "  [ 2.56244865e+00  3.41659821e+00  4.78323749e+01  5.12489731e+00\n",
            "    6.49153659e+01]\n",
            "  [ 2.67438943e+00  3.56585257e+00  4.99219360e+01  5.34877886e+00\n",
            "    6.77511989e+01]]]\n",
            "---------------\n",
            "[[-2.96864995]\n",
            " [-1.36023708]\n",
            " [-0.98420459]\n",
            " [-0.02419985]]\n",
            "--------------\n",
            "[[[ 0.72449901 -1.22971266 -2.51121553  0.28394329 -3.28967571]]\n",
            "\n",
            " [[-0.39840286 -0.16508318 -1.18392646 -0.53755481 -0.70318052]]\n",
            "\n",
            " [[-0.23278892 -0.49269954  0.31323245 -0.47574198 -0.80219918]]\n",
            "\n",
            " [[ 0.21107486 -0.40452213  0.95064318  0.07657707 -0.4441265 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.array([[[1,2,3],[2,1,4]], [[5,10,3], [4,5,1]], [[2,5,8],[6,7,3]]])\n",
        "# x = np.array([[2],[1],[6]])\n",
        "# y = np.mean(x, axis=0)\n",
        "# print(y.shape)\n",
        "# print(y)\n",
        "# z = np.random.randn(1)\n",
        "# print(z-y)"
      ],
      "metadata": {
        "id": "6T9JG7Whi5wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relu Activation layer"
      ],
      "metadata": {
        "id": "Wag871i3sIty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_ReLU:\n",
        "  def forward(self, inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    after multiplying weight with previous layer output and adding bias , we will pass the\n",
        "    result through relu activation layer . If value is greater than 0 it will be same\n",
        "    otherwise it will be zero .\n",
        "\n",
        "    Args :\n",
        "          output numpy array after dot product and bias addition\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    self.output = np.zeros((batch_size, inputs.shape[1], inputs.shape[2]), dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.output[i] = np.maximum(0, inputs[i])\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the gradient of loss w.r.t the ReLU activation layer.\n",
        "\n",
        "    Args:\n",
        "        prev_grad: Numpy array of the gradient of loss w.r.t the outputs from the previous layer.\n",
        "\n",
        "    Return:\n",
        "        Numpy array containing the gradient of loss w.r.t the inputs to the ReLU activation layer.\n",
        "    \"\"\"\n",
        "    batch_size = prev_grad.shape[0]\n",
        "\n",
        "    self.grad = np.zeros_like(prev_grad, dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.grad[i] = np.where(self.output[i] > 0, prev_grad[i], 0)"
      ],
      "metadata": {
        "id": "AMJ63fcfbHPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.array([[[1,-2,3]], [[2,3,-4]], [[-4,-2,-1]], [[-1.3, -3, 3.24]]])\n",
        "# print(x.shape)\n",
        "# act1 = Activation_ReLU()\n",
        "# act1.forward(x)\n",
        "# print(act1.output)\n",
        "# print(act1.output.shape)\n",
        "# print(\"-----------------\")\n",
        "# prev_grad = np.array([[[1.07,0.2,3.123]], [[0.42,0.3433,0.944]], [[0.4344,0.3432,-1]], [[-1.3, 0.433, 3.24]]])\n",
        "# act1.backward(prev_grad)\n",
        "# print(act1.grad)"
      ],
      "metadata": {
        "id": "zMBolnMbCD0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_LeakyReLU:\n",
        "  def forward(self, inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    after multiplying weight with previous layer output and adding bias , we will pass the\n",
        "    result through relu activation layer . If value is greater than 0 it will be same\n",
        "    otherwise it will be 0.01*input .\n",
        "\n",
        "    Args :\n",
        "          output numpy array after dot product and bias addition\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    self.input = inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    self.output = np.zeros((batch_size, inputs.shape[1], inputs.shape[2]), dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.output[i] = np.maximum(0.01*inputs[i], inputs[i])\n",
        "\n",
        "\n",
        "  def backward(self, prev_grad:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the gradient of loss w.r.t the LeakyReLU activation layer.\n",
        "    If the output is positive then put prev_grad as it is . And if output is negative\n",
        "    then put grad as 0.01 .\n",
        "\n",
        "    Args:\n",
        "        prev_grad: Numpy array of the gradient of loss w.r.t the outputs from the previous layer.\n",
        "\n",
        "    Return:\n",
        "        Numpy array containing the gradient of loss w.r.t the inputs to the ReLU activation layer.\n",
        "    \"\"\"\n",
        "    batch_size = prev_grad.shape[0]\n",
        "\n",
        "    self.grad = np.zeros_like(prev_grad, dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      self.grad[i] = np.where(self.output[i] > 0, prev_grad[i], 0.01)"
      ],
      "metadata": {
        "id": "ddUvGdPBVdt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy loss"
      ],
      "metadata": {
        "id": "Q5OTjBCwsNHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    to calculate the probablity we pass the output of the last layer to the softmax\n",
        "\n",
        "    Args:\n",
        "         output numpy array of the last layer\n",
        "\n",
        "    Return:\n",
        "           numpy array after transformation\n",
        "    \"\"\"\n",
        "    # substracting maximum value from each value to avoid exploding exponential value\n",
        "    exp_values = np.exp(inputs - np.max(inputs))\n",
        "    output = exp_values / np.sum(exp_values)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "-9rsARwvX--2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array([[ 9.24509915e+01, -5.57731481e+02 ,-1.74263620e+03 , 1.35156736e+03,1.13105210e+03 , 2.26596646e+03, -3.41934527e+01 , 1.99968764e+03,4.63964018e+03 ,-2.15820369e+02]])\n",
        "print(np.max(pred))\n",
        "print(pred-np.max(pred))\n",
        "print(np.sum(pred))\n",
        "y = softmax(pred)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0zGPsa6nKK6",
        "outputId": "da94c3e8-4a59-4db2-8f48-becfd902b07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4639.64018\n",
            "[[-4547.1891885 -5197.371661  -6382.27638   -3288.07282   -3508.58808\n",
            "  -2373.67372   -4673.8336327 -2639.95254       0.        -4855.460549 ]]\n",
            "8929.983228800002\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax_Crossentropy_Loss:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.y_pred = None\n",
        "    self.y_true = None\n",
        "\n",
        "\n",
        "  def softmax(self, inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    to calculate the probablity we pass the output of the last layer to the softmax\n",
        "\n",
        "    Args:\n",
        "         output numpy array of the last layer\n",
        "\n",
        "    Return:\n",
        "           numpy array after transformation\n",
        "    \"\"\"\n",
        "    # substracting maximum value from each value to avoid exploding exponential value\n",
        "    exp_values = np.exp(inputs - np.max(inputs), dtype=np.float64)\n",
        "    output = exp_values / np.sum(exp_values, dtype=np.float64)\n",
        "\n",
        "    output = np.float64(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def forward(self, y_pred:np.ndarray, y_true:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For input first it will calculate softmax and then it will calculate softmaxcross\n",
        "    entropy loss .\n",
        "\n",
        "    Args:\n",
        "        y_pred : numpy array of output from previous layer .\n",
        "        y_true : numpy array of correct label . Should be 1D or 2D\n",
        "\n",
        "    Return :\n",
        "            It will return numpy array containing loss of the network .\n",
        "    \"\"\"\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "\n",
        "    batch_size = y_pred.shape[0]\n",
        "\n",
        "    self.output = np.zeros(batch_size, dtype=np.float64)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      # taking out the ele one by one from batches .\n",
        "      current_y_pred = y_pred[i]         # It will like [[1,2,3,4,5]]\n",
        "      y_true_current = y_true[i]         # It will like [0,1,0,0,0]\n",
        "      # changing current_y_true to 2D for convienence in calculation\n",
        "      current_y_true = y_true_current.reshape(1,-1)       # It will become like [[0,1,0,0,0]]\n",
        "\n",
        "      # calculating the number of samples\n",
        "      samples = current_y_pred.shape[1]\n",
        "\n",
        "      # put the currenty_pred into softmax function to get softmax value\n",
        "      softmax_y_pred = self.softmax(current_y_pred)\n",
        "\n",
        "      # if it is 0 in the y_pred then there will be problem while calculating log.\n",
        "      y_pred_clipped = np.clip(softmax_y_pred, 1e-18, 1-1e-18)    # Do not use this line without softmax\n",
        "\n",
        "      # if the true value in 1D like [1,2,0,1]\n",
        "      if len(current_y_true.shape) == 1:\n",
        "        # only taking the proba of index for which there is true value\n",
        "        correct_confidences = y_pred_clipped[range(samples), current_y_true]\n",
        "\n",
        "      # if the true value in 2D i.e one hot encoding like [[1,0,0],[0,0,1],[0,1,0]]\n",
        "      elif len(current_y_true.shape) == 2:\n",
        "        # multiplying true value with predicted value . It will result in for each list in list\n",
        "        # all value will be zero but one value so doing sum will not affect result but decrease dim\n",
        "        correct_confidences = np.sum(y_pred_clipped*current_y_true, axis=1, dtype = np.float64)\n",
        "\n",
        "      negative_log_likelihoods = -np.log(correct_confidences, dtype = np.float64)\n",
        "      self.output[i] =  np.mean(negative_log_likelihoods, dtype = np.float64)\n",
        "\n",
        "  def backward(self) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the gradient of the loss with respect to y_pred.\n",
        "\n",
        "    Return:\n",
        "        Gradient of loss with respect to y_pred (numpy array).\n",
        "    \"\"\"\n",
        "    batch_size = self.y_pred.shape[0]\n",
        "    self.grad = np.zeros_like(self.y_pred)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        current_y_pred = self.y_pred[i]\n",
        "        y_true_current = self.y_true[i]\n",
        "        current_y_true = y_true_current.reshape(1,-1)\n",
        "\n",
        "        softmax_y_pred = self.softmax(current_y_pred)\n",
        "        y_pred_clipped = np.clip(softmax_y_pred, 1e-18, 1-1e-18)\n",
        "\n",
        "        # Calculate the gradient of loss w.r.t. softmax output\n",
        "        grad_softmax = (y_pred_clipped - current_y_true)\n",
        "\n",
        "        self.grad[i] = np.float64(grad_softmax)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6z7iac_8tu_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_Crossentropy:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.y_pred = None\n",
        "    self.y_true = None\n",
        "\n",
        "  def forward(self, y_pred:np.ndarray, y_true:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Function to calculate the cross entropy loss between y_pred and y_true\n",
        "\n",
        "    Args:\n",
        "        y_pred : numpy array of output from previous layer .\n",
        "        y_true : numpy array of correct label . Should be 1D or 2D\n",
        "\n",
        "    Return :\n",
        "            It will return numpy array containing loss of the network .\n",
        "    \"\"\"\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "\n",
        "    batch_size = y_pred.shape[0]\n",
        "\n",
        "    self.output = np.zeros(batch_size)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      # taking out the ele one by one from batches .\n",
        "      current_y_pred = y_pred[i]         # It will like [[1,2,3,4,5]]\n",
        "      current_y_true = y_true[i]         # It will like [0,1,0,0,0]\n",
        "      # changing current_y_true to 2D for convienence in calculation\n",
        "      current_y_true = current_y_true.reshape(1,-1)       # It will become like [[0,1,0,0,0]]\n",
        "\n",
        "      # calculating the number of samples\n",
        "      samples = current_y_pred.shape[1]\n",
        "      # if it is 0 in the y_pred then there will be problem while calculating log.\n",
        "      # y_pred_clipped = np.clip(current_y_pred, 1e-7, 1-1e-7)    # Do not use this line without softmax\n",
        "\n",
        "      # if the true value in 1D like [1,2,0,1]\n",
        "      if len(current_y_true.shape) == 1:\n",
        "        # only taking the proba of index for which there is true value\n",
        "        correct_confidences = current_y_pred[range(samples), current_y_true]\n",
        "\n",
        "      # if the true value in 2D i.e one hot encoding like [[1,0,0],[0,0,1],[0,1,0]]\n",
        "      elif len(current_y_true.shape) == 2:\n",
        "        # multiplying true value with predicted value . It will result in for each list in list\n",
        "        # all value will be zero but one value so doing sum will not affect result but decrease dim\n",
        "        correct_confidences = np.sum(current_y_pred*current_y_true, axis=1)\n",
        "\n",
        "      negative_log_likelihoods = -np.log(correct_confidences)\n",
        "      self.output[i] =  np.mean(negative_log_likelihoods)\n",
        "\n",
        "  def backward(self) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Function to calculate the gradients of the loss with respect to the inputs (y_pred).\n",
        "\n",
        "    Args:\n",
        "        y_pred : numpy array of output from previous layer.                     # [[[1,2,3,4,5]], [[1,2,3,44,5]], [[2,3,4,5,4]]]\n",
        "        y_true : numpy array of correct label. Should be 1D or 2D.              # [[0,0,1,0,0], [1,0,0,0,0], [0,1,0,0,1]]\n",
        "\n",
        "    \"\"\"\n",
        "    # calculating the number of batch_size\n",
        "    batch_size = y_pred.shape[0]\n",
        "\n",
        "    # creating an array of zeros with the same shape as y_pred\n",
        "    self.grad = np.zeros_like(self.y_pred)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      # taking out the ele one by one from batches .\n",
        "      current_y_pred = self.y_pred[i]         # It will like [[1,2,3,4,5]]\n",
        "      current_y_true = self.y_true[i]         # It will like [0,1,0,0,0]\n",
        "      # changing current_y_true to 2D for convienence in calculation\n",
        "      current_y_true = current_y_true.reshape(1,-1)       # It will become like [[0,1,0,0,0]]\n",
        "\n",
        "      # calculating the number of samples\n",
        "      samples = current_y_pred.shape[1]\n",
        "\n",
        "      # if the true value is in 1D like [1,2,0,1]\n",
        "      if len(current_y_true.shape) == 1:\n",
        "        # for each sample, set the gradient of the correct class to -1 / correct_confidence\n",
        "        self.grad[i][range(samples), current_y_true] = -1 / current_y_pred[range(samples), current_y_true]\n",
        "\n",
        "      # if the true value is in 2D i.e., one-hot encoding like [[1,0,0],[0,0,1],[0,1,0]]\n",
        "      elif len(current_y_true.shape) == 2:\n",
        "        # for each sample, divide the predicted probabilities by the true values\n",
        "        self.grad[i] = -current_y_true / current_y_pred\n",
        "\n",
        "      # normalize the gradients by the number of samples\n",
        "      self.grad[i] /= batch_size\n",
        "\n"
      ],
      "metadata": {
        "id": "lkn698eHCTpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax( inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    to calculate the probablity we pass the output of the last layer to the softmax\n",
        "\n",
        "    Args:\n",
        "         output numpy array of the last layer\n",
        "\n",
        "    Return:\n",
        "           numpy array after transformation\n",
        "    \"\"\"\n",
        "    # substracting maximum value from each value to avoid exploding exponential value\n",
        "    exp_values = np.exp(inputs - np.max(inputs), dtype=np.float64)\n",
        "    output = exp_values / np.sum(exp_values, dtype=np.float64)\n",
        "\n",
        "    output = np.float64(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "XjlAY8dhdDpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array([[[  13.5829348  ,  36.48532427  , 77.40344015  ,-24.71464484  ,  0.35529315, 161.14677311 ,  42.50280807 , -54.77230824 , -23.1689407,   -17.51465871]]])"
      ],
      "metadata": {
        "id": "VHRH-y_jdHM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array([[[  13.5829348  ,  36.48532427  , 77.40344015  ,-24.71464484  ,  0.35529315, 161.14677311 ,  42.50280807 , -54.77230824 , -23.1689407,   -17.51465871]]])\n",
        "y_true = np.array([[0., 0., 0., 0. ,0., 0., 0. ,0. ,0., 1.]])\n",
        "\n",
        "loss = Softmax_Crossentropy_Loss()\n",
        "loss.forward(y_pred,y_true)\n",
        "print(loss.output)\n",
        "print(\"---------------\")\n",
        "loss.backward()\n",
        "print(loss.grad)"
      ],
      "metadata": {
        "id": "1nr5vpM_oF0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac97d25-c161-45d3-fbd6-33c587e31947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41.44653167]\n",
            "---------------\n",
            "[[[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18\n",
            "    1.e-18 -1.e+00]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoqBWtyTqZvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full integrated architecture"
      ],
      "metadata": {
        "id": "IlexZu4af9Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstNetwork_2:\n",
        "  def __init__(self):\n",
        "    self.conv1 = Convoluation((2,2),0,1)\n",
        "    self.act1 = Activation_LeakyReLU()\n",
        "\n",
        "    self.conv2 = Convoluation((2,2),0,1)\n",
        "    self.act2 = Activation_LeakyReLU()\n",
        "\n",
        "    self.pool = Maxpool(2)\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.linear1 = Linear(169,80)\n",
        "    self.act3 = Activation_LeakyReLU()\n",
        "\n",
        "    self.linear2 = Linear(80,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.conv1.forward(x)\n",
        "    self.act1.forward(self.conv1.output)\n",
        "\n",
        "    self.conv2.forward(self.act1.output)\n",
        "    self.act2.forward(self.conv2.output)\n",
        "\n",
        "    self.pool.forward(self.act2.output)\n",
        "    self.flatten.forward(self.pool.output)\n",
        "\n",
        "    self.linear1.forward(self.flatten.output)\n",
        "    self.act3.forward(self.linear1.output)\n",
        "\n",
        "    self.linear2.forward(self.act3.output)\n",
        "\n",
        "    return self.linear2.output\n",
        "\n"
      ],
      "metadata": {
        "id": "V-5XCEnSgGrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropogation(network, loss_fn, learning_rate=0.01):\n",
        "  \"\"\"\n",
        "  this function will do backpropogation and then update the weight and bias of the layers\n",
        "\n",
        "  Args :\n",
        "        network : instance of the model\n",
        "        loss_fn : instance of the loss function\n",
        "        learning_rate : it will use for weight updation\n",
        "  \"\"\"\n",
        "  # backward pass\n",
        "  loss_fn.backward()\n",
        "  network.linear2.backward(loss_fn.grad)\n",
        "  network.act3.backward(network.linear2.grad_input)\n",
        "  network.linear1.backward(network.act3.grad)\n",
        "\n",
        "  network.flatten.backward(network.linear1.grad_input)\n",
        "  network.pool.backward(network.flatten.grad)\n",
        "  network.act2.backward(network.pool.grad)\n",
        "  network.conv2.backward(network.act2.grad)\n",
        "  network.act1.backward(network.conv2.grad_input)\n",
        "  network.conv1.backward(network.act1.grad)\n",
        "\n",
        "  # update weight and bias of convolutation and linear layer\n",
        "  # print(f\"bias of the linear2 before updating: {network.linear2.bias}\")\n",
        "  # print(f\"weight of the linear2 layer before updating {network.linear2.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.linear2.update(learning_rate)\n",
        "  # print(f\"bias of the linear2 after updating: {network.linear2.bias}\")\n",
        "  # print(f\"weight of the linear2 layer after updating {network.linear2.weight}\")\n",
        "\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "\n",
        "  # print(f\"bias of the linear1 before updating: {network.linear1.bias}\")\n",
        "  # print(f\"weight of the linear1 layer before updating {network.linear1.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.linear1.update(learning_rate)\n",
        "  # print(f\"bias of the linear1 after updating: {network.linear1.bias}\")\n",
        "  # print(f\"weight of the linear1 layer after updating {network.linear1.weight}\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  # print(\"-------------------------------------------------------------\")\n",
        "  network.conv2.update(learning_rate)\n",
        "  network.conv1.update(learning_rate)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OrCeXbXOhnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstNetwork_3:\n",
        "  def __init__(self):\n",
        "    self.conv1 = Convoluation((4,4),3,2)\n",
        "    self.act1 = Activation_LeakyReLU()\n",
        "    self.pool = Maxpool(2)\n",
        "\n",
        "    self.conv2 = Convoluation((2,2),0,1)\n",
        "    self.act2 = Activation_LeakyReLU()\n",
        "    self.conv3 = Convoluation((2,2),0,1)\n",
        "    self.act3 = Activation_LeakyReLU()\n",
        "\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.linear1 = Linear(36,20)\n",
        "    self.act4 = Activation_LeakyReLU()\n",
        "\n",
        "    self.linear2 = Linear(20,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.conv1.forward(x)\n",
        "    self.act1.forward(self.conv1.output)\n",
        "    self.pool.forward(self.act1.output)\n",
        "\n",
        "    self.conv2.forward(self.pool.output)\n",
        "    self.act2.forward(self.conv2.output)\n",
        "    self.conv3.forward(self.act2.output)\n",
        "    self.act3.forward(self.conv3.output)\n",
        "\n",
        "    self.flatten.forward(self.act3.output)\n",
        "\n",
        "    self.linear1.forward(self.flatten.output)\n",
        "    self.act4.forward(self.linear1.output)\n",
        "\n",
        "    self.linear2.forward(self.act4.output)\n",
        "\n",
        "    return self.linear2.output\n",
        "\n"
      ],
      "metadata": {
        "id": "DaE7eFaswUAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropogation_3(network, loss_fn, learning_rate=0.01):\n",
        "  \"\"\"\n",
        "  this function will do backpropogation and then update the weight and bias of the layers\n",
        "\n",
        "  Args :\n",
        "        network : instance of the model\n",
        "        loss_fn : instance of the loss function\n",
        "        learning_rate : it will use for weight updation\n",
        "  \"\"\"\n",
        "  # backward pass\n",
        "  loss_fn.backward()\n",
        "  network.linear2.backward(loss_fn.grad)\n",
        "  network.linear2.update(learning_rate)           # updating linear2 weight and bias\n",
        "  network.act4.backward(network.linear2.grad_input)\n",
        "  network.linear1.backward(network.act4.grad)\n",
        "  network.linear1.update(learning_rate)           # updating linear1 weight and bias\n",
        "\n",
        "  network.flatten.backward(network.linear1.grad_input)\n",
        "\n",
        "  network.act3.backward(network.flatten.grad)\n",
        "  network.conv3.backward(network.act3.grad)\n",
        "  network.conv3.update(learning_rate)            # updating conv3 filter and bias\n",
        "  network.act2.backward(network.conv3.grad_input)\n",
        "  network.conv2.backward(network.act2.grad)\n",
        "  network.conv2.update(learning_rate)            # updating conv2 filter and bias\n",
        "\n",
        "  network.pool.backward(network.conv2.grad_input)\n",
        "  network.act1.backward(network.pool.grad)\n",
        "  network.conv1.backward(network.act1.grad)\n",
        "  network.conv1.update(learning_rate)            # updating conv1 filter and bias\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pC9NtRwSzkmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of the model"
      ],
      "metadata": {
        "id": "-VeM9Sp_gyqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XlzbkS4g4ik",
        "outputId": "26353d97-3626-490a-fb95-281fe635d16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(424, 33, 28, 28)\n",
            "(424, 33, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will use SGD method i.e we will take every data and then train the model\n",
        "\n",
        "# get a instance of FirstNetwork and loss function\n",
        "model1 = FirstNetwork_3()\n",
        "loss_fn = Softmax_Crossentropy_Loss()\n",
        "\n",
        "# finding batch size\n",
        "batch_size = X_train.shape[0]\n",
        "\n",
        "# iterate through every batch\n",
        "for batch in range(batch_size):\n",
        "\n",
        "  # taking out current batch input and label\n",
        "  current_input_batch = X_train[batch]\n",
        "  current_label_batch = y_train[batch]\n",
        "\n",
        "\n",
        "  # Get the output for the current batch using the network's forward method\n",
        "  output = model1.forward(current_input_batch)\n",
        "\n",
        "  # Label y will be 1D , change it into 2D then calculate the loss\n",
        "  loss_fn.forward(output,current_label_batch)\n",
        "\n",
        "  # do backpropogation to update the weight and bias of the network\n",
        "  backpropogation_3(network=model1,loss_fn=loss_fn,learning_rate=0.01)\n",
        "\n",
        "  if batch%30 == 0:\n",
        "    print(f\"Batch: {batch}\")\n",
        "    print(f\"y_true: {loss_fn.y_true}\")\n",
        "    print(f\"y_pred: {loss_fn.y_pred}\")\n",
        "    print(f\"loss: {loss_fn.output}\")\n",
        "    print(f\"loss grad: {loss_fn.grad}\")\n",
        "    # print(f\"current_label: {current_label_batch}\")\n",
        "    # print(f\"Output: {output}\")\n",
        "    # print(f\"Conv1 filter: {model1.linear2.weight}\")\n",
        "    # print(f\"Conv1 bias: {model1.linear2.bias}\")\n",
        "    print(f\"linear2 grad input: {model1.linear2.grad_input}\")\n",
        "    print(f\"linear2 input: {model1.linear2.input}\")\n",
        "    # print(f\"linear2 grad weight: {model1.linear2.grad_weight}\")\n",
        "    # print(f\"linear2 grad bias: {model1.linear2.grad_bias}\")\n",
        "    # print(f\"Act3 input: {model1.act3.input}\")\n",
        "    # print(f\"Act3 output: {model1.act3.output}\")\n",
        "    # print(f\"linear2 input: {model1.linear2.input}\")\n",
        "    print(\"---------------------------------\")\n",
        "    print(f\"True value : {np.argmax(current_label_batch, axis=1)}\")\n",
        "    print(f\"Prediction : {np.argmax(output, axis=2).reshape(-1)}\")\n",
        "    # print(f\"loss: {loss_fn.output}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kuNn0FeOg6EL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a636141-77f9-4ac9-e66c-4d1fa55ed649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0\n",
            "y_true: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "y_pred: [[[  13.5829348    36.48532427   77.40344015  -24.71464484    0.35529315\n",
            "    161.14677311   42.50280807  -54.77230824  -23.1689407   -17.51465871]]\n",
            "\n",
            " [[  41.29650527   38.39326904   39.87992009    9.45028164  -17.05291194\n",
            "     93.72100252   62.44212423  -47.72773963  -48.83889588   -0.72485417]]\n",
            "\n",
            " [[  32.3971754    87.37613558   68.63783288    6.33761632   -6.31176413\n",
            "    173.53252624   83.39971166  -49.64421328  -65.03194578   39.25875511]]\n",
            "\n",
            " [[  -1.28046917   20.57942334  109.59072665  -36.91986965   20.11489942\n",
            "    173.33191423   38.61808192  -46.74724778   -5.19796165   31.08429664]]\n",
            "\n",
            " [[ -20.30322483   84.15579706  126.09253567  -38.67052335  -10.42923674\n",
            "    210.46923494   57.1717364   -56.6535793   -10.54499374   15.48283241]]\n",
            "\n",
            " [[  70.27449826   91.32909302   78.17430609  -52.13052305  -18.6139945\n",
            "    149.66186823   59.22829955  -95.87577349  -64.3451473    23.06829235]]\n",
            "\n",
            " [[  28.4285693   -15.62783997   65.0114401    10.571653      7.07983134\n",
            "     94.22445043   43.10654256  -14.70055632  -14.72467468    2.68342934]]\n",
            "\n",
            " [[  35.14404272   69.56246586   30.93653571   19.9811366    -5.14595953\n",
            "    128.32252742   53.13279835  -53.46353217  -13.72949403   28.22733392]]\n",
            "\n",
            " [[  -0.92221917  -33.92808264   76.24110552    7.56885394   30.27735292\n",
            "    105.1457335    45.23316372  -27.18862665   -2.1250026     6.58809344]]\n",
            "\n",
            " [[  56.37957055   66.99129069   86.91346347  -39.88409777  -21.70937249\n",
            "    163.45009488   56.69317077  -61.74959643  -69.533508      8.73826612]]\n",
            "\n",
            " [[  32.43349873    3.7555078    67.46331053  -19.41455071   22.25390059\n",
            "    127.3888496    57.62589228  -50.70278032  -24.76104606   22.42139106]]\n",
            "\n",
            " [[  40.82432559   -0.32316352   22.72482485   26.79848831  -11.32857989\n",
            "    132.70182483   42.67372651 -105.53893645  -41.36893167  -29.83005566]]\n",
            "\n",
            " [[ -15.92488563   30.05540317   77.20598787   15.30067435   24.55355715\n",
            "    124.59317556    7.75689781  -38.61099856  -19.51227844    4.27206543]]\n",
            "\n",
            " [[  39.5543336    69.89037956  107.32372972  -15.37587311    3.33594225\n",
            "    165.96252049   64.62112161  -22.42643111  -17.77264539   28.05724114]]\n",
            "\n",
            " [[  61.49901832  121.63781478  111.9197504   -25.34155979  -58.02168602\n",
            "    218.40761774   44.21748878  -67.12278311  -68.39142824   -2.40348243]]\n",
            "\n",
            " [[ -21.56952988   41.46432877  146.31215949  -42.7034888    -8.68718853\n",
            "    190.33464344   77.76794573  -62.17270828  -13.2533988    18.94607333]]\n",
            "\n",
            " [[  45.8632223    74.78061121   31.14106279  -11.62923154   17.69410243\n",
            "    164.37802653   19.78072095 -102.4054227   -26.7462026     9.17853428]]\n",
            "\n",
            " [[  25.43953207   47.32917633   64.11087039  -22.47497949  -14.31596918\n",
            "    119.77626232   28.14218921  -36.08456778  -49.86756771  -16.93665316]]\n",
            "\n",
            " [[   9.37244491   88.02730668   86.37278731  -14.23831032  -11.19183579\n",
            "    216.62101192   80.31340222  -53.48626994  -51.09621806   47.24887367]]\n",
            "\n",
            " [[  43.85226766   57.56022136   61.75740159  -28.64551383  -22.30405189\n",
            "    105.59244769   38.59677609  -39.33903007  -49.81472938   12.78704746]]\n",
            "\n",
            " [[  52.70281187  108.80729132  110.53495686  -69.7084599    -8.18500891\n",
            "    182.32933679   31.29106949  -28.98330113  -44.06542652  -17.97518276]]\n",
            "\n",
            " [[  -3.25620151  -31.54619239   60.18464977   21.43022301   20.95999138\n",
            "    107.54196863   26.62351621  -39.27279532   38.80639915  -14.41047473]]\n",
            "\n",
            " [[  43.12753774  123.83286191   73.79880656   -1.51956783  -17.8043197\n",
            "    185.48036642   80.77144691  -47.24002107  -80.47723503   30.38374641]]\n",
            "\n",
            " [[ -33.32331868  -16.79365388  110.96915624    3.77296078   31.58454394\n",
            "    180.24173072   79.34921381  -63.3495347    16.62519135   36.66587082]]\n",
            "\n",
            " [[  26.83123746   62.56819307   80.5902214   -54.56291839  -13.3262543\n",
            "    120.50446602   15.73675548  -67.90987177  -53.35040865   -1.39881798]]\n",
            "\n",
            " [[  38.36230541   62.27152846   90.93464585  -26.34249265  -11.89365161\n",
            "    137.14664221   40.14422088  -27.71480188  -29.41896677  -14.57637364]]\n",
            "\n",
            " [[  70.2716883    91.87585506   15.92405552  -56.9512663   -42.7908449\n",
            "     87.46756331   -4.4250943   -74.69585862  -75.64547544  -14.99635533]]\n",
            "\n",
            " [[  77.86505978   98.5550295    77.34976888  -41.77335427  -74.25259114\n",
            "    167.4965587    38.7795415  -110.071502    -52.77869893  -18.40165985]]\n",
            "\n",
            " [[  28.9897738    62.48796522  112.07411348  -72.99238204  -34.44960236\n",
            "    155.97748121   -3.89731653  -65.78445128  -64.42570308    6.44711861]]\n",
            "\n",
            " [[  17.76623267  103.02785222   81.9179575   -56.19649946  -35.52580479\n",
            "    165.46324186   35.80613807  -54.30180294  -77.7622942     8.34550575]]\n",
            "\n",
            " [[  63.61238127   37.62405241   -4.47578925   -4.45398823  -41.35600473\n",
            "     64.38782137   16.38008406  -52.77978229  -68.54895575   15.69609847]]\n",
            "\n",
            " [[  39.15572871   33.16108024   63.06268354  -21.0604221   -16.62518715\n",
            "    109.21735473   54.90754763  -44.09181242  -84.79293806    0.50770957]]\n",
            "\n",
            " [[  51.28927406    3.41269248  102.44206699  -26.45052994  -10.4176116\n",
            "    166.23589185   47.11623594  -61.83083834  -14.84376877  -39.41789096]]]\n",
            "loss: [41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167  0.         41.44653167  0.\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167  0.         41.44653167]\n",
            "loss grad: [[[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18 -1.00000000e+00]]\n",
            "\n",
            " [[ 1.00000000e-18 -1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  2.60468827e-14  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "   -1.00000000e+00  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18 -1.00000000e+00  2.05565776e-13  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18 -1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  2.79820524e-13 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  0.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[-1.00000000e+00  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  0.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[-1.00000000e+00  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18 -1.00000000e+00]]\n",
            "\n",
            " [[-1.00000000e+00  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "   -1.00000000e+00  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18 -1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  4.62875092e-18  1.00000000e-18\n",
            "   -1.00000000e+00  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 4.09424258e-10  9.87970510e-01  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.20294895e-02 -1.00000000e+00  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "   -1.00000000e+00  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18 -1.00000000e+00\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18 -1.00000000e+00]]\n",
            "\n",
            " [[ 3.15303480e-01  1.62982489e-12  1.00000000e-18  1.00000000e-18\n",
            "   -1.00000000e+00  6.84696520e-01  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  0.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e-18]]\n",
            "\n",
            " [[ 1.00000000e-18  1.00000000e-18  1.00000000e-18  1.00000000e-18\n",
            "    1.00000000e-18  1.00000000e+00  1.00000000e-18  1.00000000e-18\n",
            "   -1.00000000e+00  1.00000000e-18]]]\n",
            "linear2 grad input: [[[-2.72252972e-01 -2.98576517e-01  3.19662543e-01  7.17002816e-02\n",
            "    9.10976737e-02  3.86456308e-01  2.99843567e-01 -8.48587861e-02\n",
            "   -8.66594156e-01 -1.31414316e-01  1.96845800e-01  1.71042188e-01\n",
            "    1.00795545e+00  4.79911016e-02 -6.04819899e-01  2.56483465e-01\n",
            "   -7.06247041e-02  1.93427293e-01  3.27268695e-01  2.64051624e-01]]\n",
            "\n",
            " [[-4.20326083e-01  3.94175375e-01 -2.08779167e-01 -2.66088761e-01\n",
            "   -3.58496902e-01  2.30108548e-01  2.08573902e-01 -1.32876673e-01\n",
            "   -9.45716527e-01  4.28903968e-01  6.66432049e-01  4.16808626e-01\n",
            "   -3.06728887e-02 -3.51778809e-01 -4.85618019e-01  2.61440052e-01\n",
            "   -3.91781926e-01 -1.35876827e-01  3.57172840e-01  6.14478238e-01]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[-1.27849765e-01 -9.10782471e-03  4.96354725e-01  3.01756375e-01\n",
            "    4.37206273e-01  8.40401643e-01  6.44905337e-01 -1.60557853e-01\n",
            "   -7.43551090e-01 -7.01708478e-01  2.96172314e-01  4.39567052e-02\n",
            "    2.11003798e-01 -1.75237580e-01 -4.37603956e-01  5.73412017e-01\n",
            "   -4.70231378e-01  2.31133966e-01 -2.21116908e-01  4.20077525e-01]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[-2.19404227e-01  5.70098316e-01  5.65028944e-01  3.72090822e-01\n",
            "    4.97424790e-01  7.00629327e-02 -2.55742188e-01  5.20171826e-01\n",
            "   -7.30427391e-01 -2.34910656e-01  6.67175757e-01 -3.67253119e-01\n",
            "    4.95158397e-01 -6.32893098e-02 -4.88401372e-01  3.59716183e-01\n",
            "    2.98961443e-01  4.33457522e-01  6.58234159e-01 -8.43120559e-03]]\n",
            "\n",
            " [[-4.20326083e-01  3.94175375e-01 -2.08779167e-01 -2.66088761e-01\n",
            "   -3.58496902e-01  2.30108548e-01  2.08573902e-01 -1.32876673e-01\n",
            "   -9.45716527e-01  4.28903968e-01  6.66432049e-01  4.16808626e-01\n",
            "   -3.06728887e-02 -3.51778809e-01 -4.85618019e-01  2.61440052e-01\n",
            "   -3.91781926e-01 -1.35876827e-01  3.57172840e-01  6.14478238e-01]]\n",
            "\n",
            " [[-4.20326083e-01  3.94175375e-01 -2.08779167e-01 -2.66088761e-01\n",
            "   -3.58496902e-01  2.30108548e-01  2.08573902e-01 -1.32876673e-01\n",
            "   -9.45716527e-01  4.28903968e-01  6.66432049e-01  4.16808626e-01\n",
            "   -3.06728887e-02 -3.51778809e-01 -4.85618019e-01  2.61440052e-01\n",
            "   -3.91781926e-01 -1.35876827e-01  3.57172840e-01  6.14478238e-01]]\n",
            "\n",
            " [[-1.27849765e-01 -9.10782471e-03  4.96354725e-01  3.01756375e-01\n",
            "    4.37206273e-01  8.40401643e-01  6.44905337e-01 -1.60557853e-01\n",
            "   -7.43551090e-01 -7.01708478e-01  2.96172314e-01  4.39567052e-02\n",
            "    2.11003798e-01 -1.75237580e-01 -4.37603956e-01  5.73412017e-01\n",
            "   -4.70231378e-01  2.31133966e-01 -2.21116908e-01  4.20077525e-01]]\n",
            "\n",
            " [[ 2.59354142e-19  8.35197400e-19  1.72978039e-19  9.12419174e-19\n",
            "   -2.10880188e-19  2.44776965e-18 -8.71847953e-19 -1.36701963e-18\n",
            "    1.46930023e-19  1.08113074e-19 -5.50481934e-19 -9.44204960e-19\n",
            "   -1.08864736e-18  3.34475126e-19 -2.61115536e-19 -1.43441983e-18\n",
            "    4.80064703e-19  3.40239439e-19  3.88880326e-19 -5.19934647e-19]]\n",
            "\n",
            " [[-2.42692493e-01  3.11997545e-01 -1.68996123e-01  2.46056382e-01\n",
            "    4.61403598e-01  1.05824894e+00 -1.95380208e-01  6.93452329e-01\n",
            "   -5.67798821e-01 -3.02414505e-01 -7.47030419e-02  5.38298193e-02\n",
            "    6.77031399e-02 -2.96251657e-02  1.10164060e-01  9.21931012e-01\n",
            "    6.26928550e-01  1.06461185e-01  6.01919040e-01  1.84621055e-01]]\n",
            "\n",
            " [[ 2.59354142e-19  8.35197400e-19  1.72978039e-19  9.12419174e-19\n",
            "   -2.10880188e-19  2.44776965e-18 -8.71847953e-19 -1.36701963e-18\n",
            "    1.46930023e-19  1.08113074e-19 -5.50481934e-19 -9.44204960e-19\n",
            "   -1.08864736e-18  3.34475126e-19 -2.61115536e-19 -1.43441983e-18\n",
            "    4.80064703e-19  3.40239439e-19  3.88880326e-19 -5.19934647e-19]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[-2.42692493e-01  3.11997545e-01 -1.68996123e-01  2.46056382e-01\n",
            "    4.61403598e-01  1.05824894e+00 -1.95380208e-01  6.93452329e-01\n",
            "   -5.67798821e-01 -3.02414505e-01 -7.47030419e-02  5.38298193e-02\n",
            "    6.77031399e-02 -2.96251657e-02  1.10164060e-01  9.21931012e-01\n",
            "    6.26928550e-01  1.06461185e-01  6.01919040e-01  1.84621055e-01]]\n",
            "\n",
            " [[-2.72252972e-01 -2.98576517e-01  3.19662543e-01  7.17002816e-02\n",
            "    9.10976737e-02  3.86456308e-01  2.99843567e-01 -8.48587861e-02\n",
            "   -8.66594156e-01 -1.31414316e-01  1.96845800e-01  1.71042188e-01\n",
            "    1.00795545e+00  4.79911016e-02 -6.04819899e-01  2.56483465e-01\n",
            "   -7.06247041e-02  1.93427293e-01  3.27268695e-01  2.64051624e-01]]\n",
            "\n",
            " [[-2.42692493e-01  3.11997545e-01 -1.68996123e-01  2.46056382e-01\n",
            "    4.61403598e-01  1.05824894e+00 -1.95380208e-01  6.93452329e-01\n",
            "   -5.67798821e-01 -3.02414505e-01 -7.47030419e-02  5.38298193e-02\n",
            "    6.77031399e-02 -2.96251657e-02  1.10164060e-01  9.21931012e-01\n",
            "    6.26928550e-01  1.06461185e-01  6.01919040e-01  1.84621055e-01]]\n",
            "\n",
            " [[-2.19404227e-01  5.70098316e-01  5.65028944e-01  3.72090822e-01\n",
            "    4.97424790e-01  7.00629327e-02 -2.55742188e-01  5.20171826e-01\n",
            "   -7.30427391e-01 -2.34910656e-01  6.67175757e-01 -3.67253119e-01\n",
            "    4.95158397e-01 -6.32893098e-02 -4.88401372e-01  3.59716183e-01\n",
            "    2.98961443e-01  4.33457522e-01  6.58234159e-01 -8.43120559e-03]]\n",
            "\n",
            " [[ 3.44870878e-01 -4.90277370e-02  1.41715169e-02 -1.16238507e-01\n",
            "    9.29051110e-01  5.82096745e-01  5.44313811e-01 -2.01169316e-01\n",
            "   -3.29225942e-01 -3.79724840e-01  3.38489772e-01  6.82348820e-01\n",
            "    1.71731002e-01 -2.23591770e-01 -4.25380521e-01  4.06862578e-01\n",
            "   -4.29613037e-01  2.75549198e-01  4.56301416e-01  6.46549924e-01]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[ 3.44870878e-01 -4.90277370e-02  1.41715169e-02 -1.16238507e-01\n",
            "    9.29051110e-01  5.82096745e-01  5.44313811e-01 -2.01169316e-01\n",
            "   -3.29225942e-01 -3.79724840e-01  3.38489772e-01  6.82348820e-01\n",
            "    1.71731002e-01 -2.23591770e-01 -4.25380521e-01  4.06862578e-01\n",
            "   -4.29613037e-01  2.75549198e-01  4.56301416e-01  6.46549924e-01]]\n",
            "\n",
            " [[-4.20326083e-01  3.94175375e-01 -2.08779167e-01 -2.66088761e-01\n",
            "   -3.58496902e-01  2.30108548e-01  2.08573902e-01 -1.32876673e-01\n",
            "   -9.45716527e-01  4.28903968e-01  6.66432049e-01  4.16808626e-01\n",
            "   -3.06728887e-02 -3.51778809e-01 -4.85618019e-01  2.61440052e-01\n",
            "   -3.91781926e-01 -1.35876827e-01  3.57172840e-01  6.14478238e-01]]\n",
            "\n",
            " [[ 2.78668677e-01  1.48137768e-01  1.91462991e-01  6.80846458e-01\n",
            "   -1.62251419e-01  7.85654170e-02  2.97683043e-01 -1.31046751e-01\n",
            "    7.86484943e-02 -7.96788772e-02  6.25976940e-01  4.43402802e-01\n",
            "    5.62438545e-01 -3.33278006e-03  1.36168133e-01  1.48894317e-01\n",
            "    1.22692476e-02  1.07161342e-01  7.72273824e-01 -3.90768134e-01]]\n",
            "\n",
            " [[-1.27849765e-01 -9.10782471e-03  4.96354725e-01  3.01756375e-01\n",
            "    4.37206273e-01  8.40401643e-01  6.44905337e-01 -1.60557853e-01\n",
            "   -7.43551090e-01 -7.01708478e-01  2.96172314e-01  4.39567052e-02\n",
            "    2.11003798e-01 -1.75237580e-01 -4.37603956e-01  5.73412017e-01\n",
            "   -4.70231378e-01  2.31133966e-01 -2.21116908e-01  4.20077525e-01]]\n",
            "\n",
            " [[ 3.74802284e-01 -1.20706112e-01  1.24903821e-01  2.54228865e-01\n",
            "    7.06294802e-01  1.77668283e-01  2.91813477e-01 -1.61522991e-01\n",
            "   -7.06203771e-01  7.66048149e-02  4.80705201e-01 -3.79584196e-01\n",
            "    4.95172732e-01 -2.40143124e-01 -3.19092455e-01  6.58417388e-01\n",
            "    2.02483285e-01  6.37500691e-01 -1.23406812e-01  3.23947545e-01]]\n",
            "\n",
            " [[-1.27849765e-01 -9.10782471e-03  4.96354725e-01  3.01756375e-01\n",
            "    4.37206273e-01  8.40401643e-01  6.44905337e-01 -1.60557853e-01\n",
            "   -7.43551090e-01 -7.01708478e-01  2.96172314e-01  4.39567052e-02\n",
            "    2.11003798e-01 -1.75237580e-01 -4.37603956e-01  5.73412017e-01\n",
            "   -4.70231378e-01  2.31133966e-01 -2.21116908e-01  4.20077525e-01]]\n",
            "\n",
            " [[-1.19702092e-01 -3.89494925e-01  4.19064759e-01  4.61471800e-01\n",
            "    4.54965476e-01  2.34445315e-01 -2.46310866e-01  2.19923757e-01\n",
            "    1.72088571e-01 -7.61561029e-01 -7.83374604e-03  1.34163596e-01\n",
            "    8.67953723e-01  2.83665854e-01 -2.03687629e-02  7.82302893e-02\n",
            "    9.88492955e-01  6.70587247e-02 -4.39489104e-01 -4.57278032e-01]]\n",
            "\n",
            " [[-2.19404227e-01  5.70098316e-01  5.65028944e-01  3.72090822e-01\n",
            "    4.97424790e-01  7.00629327e-02 -2.55742188e-01  5.20171826e-01\n",
            "   -7.30427391e-01 -2.34910656e-01  6.67175757e-01 -3.67253119e-01\n",
            "    4.95158397e-01 -6.32893098e-02 -4.88401372e-01  3.59716183e-01\n",
            "    2.98961443e-01  4.33457522e-01  6.58234159e-01 -8.43120559e-03]]\n",
            "\n",
            " [[ 3.44870878e-01 -4.90277370e-02  1.41715169e-02 -1.16238507e-01\n",
            "    9.29051110e-01  5.82096745e-01  5.44313811e-01 -2.01169316e-01\n",
            "   -3.29225942e-01 -3.79724840e-01  3.38489772e-01  6.82348820e-01\n",
            "    1.71731002e-01 -2.23591770e-01 -4.25380521e-01  4.06862578e-01\n",
            "   -4.29613037e-01  2.75549198e-01  4.56301416e-01  6.46549924e-01]]\n",
            "\n",
            " [[-2.72252972e-01 -2.98576517e-01  3.19662543e-01  7.17002816e-02\n",
            "    9.10976737e-02  3.86456308e-01  2.99843567e-01 -8.48587861e-02\n",
            "   -8.66594156e-01 -1.31414316e-01  1.96845800e-01  1.71042188e-01\n",
            "    1.00795545e+00  4.79911016e-02 -6.04819899e-01  2.56483465e-01\n",
            "   -7.06247041e-02  1.93427293e-01  3.27268695e-01  2.64051624e-01]]\n",
            "\n",
            " [[ 4.51324072e-01 -2.19080023e-01  1.78188886e-01  1.76646431e-01\n",
            "    5.60812642e-01 -1.56001289e-01  3.53417537e-01 -3.80170923e-01\n",
            "   -5.27174827e-01  1.71957161e-01  5.04259330e-01 -3.96556925e-01\n",
            "    4.73825696e-01 -2.30802206e-01 -3.53827566e-01  3.67729332e-01\n",
            "    4.81053193e-03  6.03933109e-01 -3.13193980e-01  2.65735884e-01]]\n",
            "\n",
            " [[ 2.59354142e-19  8.35197400e-19  1.72978039e-19  9.12419174e-19\n",
            "   -2.10880188e-19  2.44776965e-18 -8.71847953e-19 -1.36701963e-18\n",
            "    1.46930023e-19  1.08113074e-19 -5.50481934e-19 -9.44204960e-19\n",
            "   -1.08864736e-18  3.34475126e-19 -2.61115536e-19 -1.43441983e-18\n",
            "    4.80064703e-19  3.40239439e-19  3.88880326e-19 -5.19934647e-19]]\n",
            "\n",
            " [[-2.19404227e-01  5.70098316e-01  5.65028944e-01  3.72090822e-01\n",
            "    4.97424790e-01  7.00629327e-02 -2.55742188e-01  5.20171826e-01\n",
            "   -7.30427391e-01 -2.34910656e-01  6.67175757e-01 -3.67253119e-01\n",
            "    4.95158397e-01 -6.32893098e-02 -4.88401372e-01  3.59716183e-01\n",
            "    2.98961443e-01  4.33457522e-01  6.58234159e-01 -8.43120559e-03]]]\n",
            "linear2 input: [[[ 1.17662851e+01 -1.64630415e-01  1.11027088e+02 -2.36410133e-01\n",
            "    1.43772802e+01  6.53473074e+01 -4.12429523e-01 -5.42464829e-01\n",
            "   -2.91321153e-01  2.30823333e+01  3.33794892e+01  6.11745690e+00\n",
            "    3.83592520e+01 -7.96203471e-01 -3.10482052e-01  2.91965151e+01\n",
            "   -3.59079287e-01  5.07455018e+01  7.21884769e+01  1.33500468e+02]]\n",
            "\n",
            " [[-4.04111052e-02 -1.53108446e-01  1.38842401e+02 -2.76221770e-01\n",
            "    5.50255045e+01  3.67127050e+01 -9.97295446e-01 -1.07024185e+00\n",
            "    3.35637531e+01  8.44840414e+01  2.44552256e+01 -5.09183404e-02\n",
            "    1.39836263e+01 -5.37387949e-01 -8.77365808e-02 -2.22330917e-01\n",
            "   -7.19174854e-01  3.75406091e+01  6.15191554e+01  9.43250040e+01]]\n",
            "\n",
            " [[-1.07030218e-01  5.13528344e+00  1.51600076e+02 -1.27988051e+00\n",
            "    9.40219841e+01  8.41680222e+01 -5.46025455e-01 -7.15657745e-01\n",
            "    1.47715484e+01  7.87350533e+01  7.25137440e+01 -2.43979235e-01\n",
            "   -7.65978892e-02 -8.35862187e-01 -6.29920869e-01  6.33661734e+00\n",
            "   -7.40235485e-01  5.56458654e+01  8.02688218e+01  6.19607024e+01]]\n",
            "\n",
            " [[-2.46422657e-01  2.27026366e+01  1.01041105e+02 -6.67962386e-01\n",
            "    2.21110235e+01  8.79627986e+01  2.43128450e+01 -5.26005526e-01\n",
            "   -6.70235543e-01  1.67668760e+01  6.21619334e+01 -1.68361390e-01\n",
            "   -2.77196606e-01 -9.76607539e-01 -4.30025279e-01  2.25985061e+01\n",
            "    2.34933931e+01  2.42759652e+01  5.36122022e+01  1.38110121e+02]]\n",
            "\n",
            " [[-4.19325593e-01  1.50691595e+00  1.17411136e+02 -7.28728744e-02\n",
            "    6.28677476e+01  1.19557147e+02 -5.02053195e-01 -1.15602065e+00\n",
            "   -3.88129033e-01  4.65266557e+01  5.03999541e+01 -2.97914991e-01\n",
            "    3.51163188e+01 -6.86281851e-01  1.10421884e+01  5.16541814e+01\n",
            "   -5.45424109e-01  5.47079456e+01  5.81566444e+01  1.14519235e+02]]\n",
            "\n",
            " [[ 2.18223181e+01 -1.01270137e-01  2.22212853e+02 -1.08557677e+00\n",
            "    1.01195196e+02  6.33574580e+01 -2.78717176e-01 -9.88652980e-01\n",
            "    4.47717910e+01  5.63347448e+01  7.41406340e+01 -2.31528305e-01\n",
            "    6.19204926e+00 -9.53145956e-01  1.09233916e+00 -2.08180722e-01\n",
            "   -4.34368548e-01 -4.15100023e-02  5.25015285e+01  1.39391737e+02]]\n",
            "\n",
            " [[-9.03367267e-02  5.40490508e+01  1.03068615e+02 -8.64571536e-01\n",
            "   -2.06103147e-01  5.14723150e+01 -5.47601062e-01  5.94616333e+00\n",
            "    1.63009275e+01  8.58175307e+01 -3.02871992e-01  2.20734966e+01\n",
            "    2.80357063e+01 -3.73897628e-01 -6.38203541e-01 -2.20462128e-01\n",
            "   -1.64065997e-01  6.08419478e+01  2.17675815e+01  1.12775871e+02]]\n",
            "\n",
            " [[-2.00449117e-01 -2.70560629e-01  9.68075311e+01 -1.20861769e+00\n",
            "    6.28348283e+01  9.01986170e+01 -7.06423649e-01 -3.73115458e-01\n",
            "    5.74428502e+01  8.24490556e+01  6.89239195e+01  1.72079487e+01\n",
            "    2.80304264e+01 -1.00677782e+00 -7.48706244e-01 -6.04304518e-01\n",
            "   -8.87747765e-01  6.41979365e+01  4.69209821e+01  6.23510321e+01]]\n",
            "\n",
            " [[-7.44120476e-01  3.87310441e+01  9.63767896e+01 -3.72317405e-01\n",
            "   -2.64025147e-01  4.63505898e+01  3.43835616e+01 -8.02994093e-01\n",
            "   -3.89793330e-01  5.99281098e+01 -1.16023170e-01  3.22834231e+00\n",
            "    1.24453125e+00 -5.17419345e-01 -3.23926097e-01 -5.56567694e-02\n",
            "    3.37076546e+01 -1.22667614e-01  6.41997235e+01  1.43390194e+02]]\n",
            "\n",
            " [[ 3.75904616e+00  4.52482635e+00  1.53098509e+02 -3.65237924e-01\n",
            "    5.29161822e+01  4.79383167e+01 -4.37712115e-01 -6.43723508e-01\n",
            "   -2.28214118e-01  1.78074436e+01  6.43966321e+01 -2.86186733e-01\n",
            "    7.72634722e+00 -6.76551801e-01  1.82318972e+00 -1.12826264e-01\n",
            "   -6.40252937e-01  8.04587345e+01  6.04084605e+01  1.22764177e+02]]\n",
            "\n",
            " [[ 2.70042566e+01  2.97367639e+01  1.19373589e+02 -6.04129212e-01\n",
            "    2.15953857e+01  5.38583438e+01  7.50262534e+00 -3.37002156e-01\n",
            "   -3.70022715e-01  2.96934811e+01  4.39641020e+01 -1.12661925e-01\n",
            "   -2.15242305e-01 -5.96112623e-01 -2.77851337e-01 -4.93537279e-01\n",
            "   -4.97790151e-02 -7.72996008e-02  6.08131295e+01  1.26046225e+02]]\n",
            "\n",
            " [[-8.92541715e-01 -8.98297459e-01  9.69447591e+01  4.68971115e+00\n",
            "    3.93175083e+01  1.06479468e+01 -4.97591899e-01 -7.56703749e-01\n",
            "    5.69491592e-01  6.06323228e+01  2.15295802e+01  7.28151855e+01\n",
            "    2.82588435e+01 -1.01368895e+00 -7.84516281e-02 -1.72796448e-01\n",
            "   -5.39330078e-01  1.12237010e+02  9.63208041e+01  1.25518574e+02]]\n",
            "\n",
            " [[-2.14864338e-01  3.70185071e+01  8.39031614e+01 -3.29754050e-01\n",
            "    4.28956636e+01  5.26150284e+01 -4.30398467e-01 -1.85310296e-01\n",
            "    7.81749012e-01  3.45194074e+01  7.42970756e+00  2.49675316e+01\n",
            "    3.48468085e+01 -1.09926070e+00 -2.41520295e-01 -3.01461728e-01\n",
            "    3.78173103e+01  5.12409942e+00  5.10634497e+01  1.12476596e+02]]\n",
            "\n",
            " [[-2.99602906e-01 -4.07961097e-01  1.68905558e+02 -9.44542350e-01\n",
            "    7.47197067e+01  1.12893862e+02 -1.58291902e-01 -7.24186642e-01\n",
            "   -2.20587175e-01  1.05241674e+02  5.07561293e+01  6.18896267e+00\n",
            "   -1.63526329e-01 -3.24826733e-01 -5.14536046e-01 -8.83511767e-02\n",
            "   -2.69135761e-01  3.30376118e+01  3.52732478e+01  6.76215532e+01]]\n",
            "\n",
            " [[ 1.23624089e+01  2.40977616e+01  1.43485414e+02 -6.59866062e-01\n",
            "    1.08249126e+02  9.10835051e+01 -1.09096824e+00 -4.68594129e-01\n",
            "    3.73511497e+00  6.18888920e+01  7.59399749e+01 -9.21764416e-02\n",
            "    7.79290145e+01 -4.78792698e-01 -3.10916481e-01 -2.21417197e-02\n",
            "   -1.05067111e+00  8.91084902e+01  3.81250861e+01  1.08766725e+02]]\n",
            "\n",
            " [[-4.42988523e-01  3.24554900e+00  1.63520525e+02 -4.66777182e-01\n",
            "    6.15965049e+01  1.04831453e+02 -2.26599182e-01 -1.04354523e+00\n",
            "   -3.45776611e-01  8.58747350e+01  2.58658136e+01 -3.45389182e-01\n",
            "   -5.34682873e-02 -6.09883915e-01  1.66573575e+00  6.43753966e+01\n",
            "   -2.05855963e-01  2.59061095e+01  6.40506151e+01  1.45424546e+02]]\n",
            "\n",
            " [[-1.34191681e-01 -3.50435593e-01  1.27192443e+02 -7.12853759e-01\n",
            "    7.34960502e+01  7.66545750e+01 -4.82655240e-01 -4.81848217e-01\n",
            "    4.65797571e+01  3.30354939e+01  8.25788140e+01  5.30669431e+01\n",
            "    3.94688911e+01 -1.63306386e+00 -4.46724181e-01 -8.22233824e-01\n",
            "   -3.07323500e-01  5.10763231e+01  6.57745832e+01  1.00055130e+02]]\n",
            "\n",
            " [[-4.15175036e-01  1.56977866e+01  1.18146549e+02  5.60970446e+00\n",
            "    4.56275286e+01  3.54448770e+01 -3.11695980e-01 -5.36060429e-01\n",
            "   -2.23304774e-01  3.64003839e+01  1.77502280e+01 -2.96515804e-01\n",
            "    3.82298390e+01 -3.16159927e-01 -3.69660741e-02  1.44323333e+01\n",
            "   -8.03919768e-01  2.58556802e+01  4.38766089e+01  9.39119095e+01]]\n",
            "\n",
            " [[ 1.95128227e+01 -3.95054760e-01  1.58593350e+02 -7.43962849e-01\n",
            "    8.74729476e+01  1.11702514e+02 -3.24995619e-01 -1.05042393e+00\n",
            "   -5.65815129e-02  7.58960373e+01  9.46702801e+01 -6.38886287e-01\n",
            "   -4.12102998e-01 -1.02370216e+00 -4.69732155e-01  5.01532008e+01\n",
            "   -6.26871617e-01  3.53288166e+01  8.16163634e+01  6.78747395e+01]]\n",
            "\n",
            " [[-1.68492079e-01 -2.87179545e-01  1.38899469e+02 -7.91823723e-01\n",
            "    6.36852369e+01  4.24791310e+01 -4.16643122e-01 -5.91876373e-01\n",
            "    1.18827422e+01  4.99100539e+01  4.63044456e+01 -3.30959344e-01\n",
            "   -1.42864167e-03 -5.57024639e-01 -4.71379523e-01  5.46637834e+00\n",
            "   -4.88884871e-01  2.89460197e+01  2.76699174e+01  6.67923950e+01]]\n",
            "\n",
            " [[-4.39367025e-02  2.57916592e+00  1.84384329e+02 -2.04832177e-01\n",
            "    3.79302242e+01  8.70149158e+01 -6.37867283e-01 -5.08844463e-01\n",
            "   -1.95559330e-01  1.79798543e+01  3.75740799e+01 -4.51123588e-01\n",
            "    4.40737085e+01 -8.06686116e-01 -1.54827493e-01  1.14061224e+01\n",
            "   -1.07976660e+00  6.96590458e+01  3.00849373e+01  1.10869228e+02]]\n",
            "\n",
            " [[-9.71438177e-02 -1.71453941e-02  3.97159477e+01 -3.69432104e-01\n",
            "   -3.57163420e-01  7.19107247e+01 -1.60897125e-01 -2.66079538e-01\n",
            "   -6.48041513e-02  7.37617466e+01  1.07821012e+01  3.24309975e+01\n",
            "    3.15095121e+01 -6.70804148e-01 -3.54194194e-01  1.12454582e+01\n",
            "   -1.63643790e-01  3.13455921e+01  4.72768898e+01  1.02683848e+02]]\n",
            "\n",
            " [[-4.53464846e-01 -2.54715967e-01  1.75061834e+02 -8.77414185e-01\n",
            "    1.23338853e+02  8.56823487e+01  3.33072166e+00 -8.92603978e-01\n",
            "    1.12792265e+00  8.46523506e+01  6.78938856e+01 -1.64634400e-01\n",
            "    7.91195867e+00 -9.03442127e-01 -2.96638853e-01 -5.06584504e-01\n",
            "   -9.80605467e-01  5.41856676e+01  7.57846808e+01  3.41175539e+01]]\n",
            "\n",
            " [[-9.29922832e-01  3.13240665e+01  1.01148466e+02 -3.88745250e-01\n",
            "    3.00508364e+01  1.10748043e+02 -9.17388230e-02 -2.25422886e-01\n",
            "   -7.96964931e-02  8.08994611e+01  4.23359837e+01  1.59477573e+01\n",
            "    1.24758520e+01 -8.96089711e-01  4.20028560e+01  2.72347694e+01\n",
            "   -1.58768565e-01  3.23468353e+01  8.27932932e+01  1.77485762e+02]]\n",
            "\n",
            " [[-5.61960029e-01  1.56331696e+01  1.28651451e+02 -1.68244213e-01\n",
            "    7.56702834e+01  3.20159018e+01 -2.04735619e-01 -6.66444091e-01\n",
            "   -2.49703449e-02 -1.71917617e-02  3.88088413e+01 -2.95053999e-01\n",
            "    2.73408980e+01 -6.63305523e-01  1.55944881e+01  1.73831012e+00\n",
            "   -6.57082786e-02 -1.92214460e-01  3.19532315e+01  1.11590271e+02]]\n",
            "\n",
            " [[-2.38251656e-01  1.47062033e+01  1.30416481e+02 -6.23915169e-01\n",
            "    3.63919731e+01  6.61224615e+01 -3.37669199e-01 -9.84073793e-02\n",
            "   -1.57308904e-01  4.98185305e+01  1.27827759e+01  5.10506306e+00\n",
            "    3.96551189e+01 -4.84386826e-01 -4.14610658e-01 -1.91560786e-01\n",
            "   -8.04639708e-01  6.51663104e+01  3.01712227e+01  9.83332770e+01]]\n",
            "\n",
            " [[-5.38816387e-01 -7.54678540e-01  1.38011222e+02 -9.83123778e-02\n",
            "    8.37193356e+01 -3.75891003e-01  1.50867579e+01 -4.21937485e-01\n",
            "    2.32747402e+01 -3.69096585e-02  5.30466639e+01  1.84222024e+01\n",
            "    2.88419355e+01 -6.30811785e-01 -2.27908499e-01 -7.27318023e-01\n",
            "   -4.74465718e-01  3.07060824e+01  1.42500860e+01  4.87937810e+01]]\n",
            "\n",
            " [[ 4.01397650e+01  9.76024631e+00  1.40210251e+02 -5.90242212e-01\n",
            "    7.44108989e+01  5.24970281e+01 -1.00474270e+00 -6.32429456e-02\n",
            "    3.49945888e+01  3.25052035e+01  6.45976469e+01  9.03388230e+00\n",
            "    7.67257363e+01 -1.04924102e+00 -1.08295494e-01 -2.07863841e-01\n",
            "   -7.66860375e-01  8.91885803e+01  3.33055444e+01  1.62944190e+02]]\n",
            "\n",
            " [[-5.70257942e-01  4.22903680e+01  1.47782522e+02  3.36188032e+01\n",
            "    7.67098482e+01  2.89641134e+01 -2.36083149e-01 -1.16722309e+00\n",
            "   -4.53234738e-01 -2.13923173e-01  5.20545910e+01 -4.99400834e-01\n",
            "    4.26386871e+01 -3.80459512e-01  2.31837882e+01  1.01970473e+01\n",
            "    1.94158825e+01 -2.15680878e-01  1.15691394e+01  1.87966443e+02]]\n",
            "\n",
            " [[-3.67636408e-01 -6.21866551e-01  1.69697035e+02 -4.05199728e-01\n",
            "    8.93600141e+01  5.51839317e+01 -9.49504193e-01 -9.80074004e-01\n",
            "   -1.72492686e-02  3.20609450e+01  5.68400364e+01 -5.18900751e-01\n",
            "    1.44072666e+01 -4.13988137e-01 -5.27800430e-01  5.22916605e+01\n",
            "   -8.24365919e-01  3.15513679e+01  4.55954297e+01  6.48819992e+01]]\n",
            "\n",
            " [[-9.27382909e-02 -5.18647290e-01  1.00791214e+02 -5.07838375e-01\n",
            "    6.40180963e+01 -5.92277001e-03 -2.82659888e-01 -8.66557088e-01\n",
            "    3.14408371e+01  4.21109032e+01  7.54226437e+01  1.53880744e+01\n",
            "   -3.20535311e-02 -4.72086801e-01 -8.23780768e-01 -2.17306631e-01\n",
            "   -2.69391046e-01  4.25214936e+01  2.24832893e+01  3.72002446e+01]]\n",
            "\n",
            " [[-6.22707435e-01  2.35140115e+01  1.69682524e+02 -7.35108562e-01\n",
            "    5.81620710e+01  1.26626267e+01 -3.89898880e-01 -4.99594682e-01\n",
            "    8.31717036e-02  5.46242915e+01  1.95694783e+01 -5.81679301e-01\n",
            "   -1.24625678e-01 -4.29359279e-01 -2.89574006e-01  1.34222302e+01\n",
            "   -4.09060191e-01  2.63501802e+01  5.99237308e+01  1.00527142e+02]]\n",
            "\n",
            " [[ 7.21010168e+00 -7.70556701e-02  1.56603093e+02 -3.78681657e-01\n",
            "    1.91748813e+01  6.84844442e+01  1.54644512e+01 -1.40406545e+00\n",
            "   -4.31697719e-01  7.81587966e+01  2.59191797e+01  7.41827178e+00\n",
            "    4.65858803e+01 -8.66512859e-01 -3.29536103e-01  7.89308492e+00\n",
            "    2.07215841e+00  4.32535442e+01  6.72301255e+01  1.84648629e+02]]]\n",
            "---------------------------------\n",
            "True value : [9 1 2 3 2 8 1 1 3 5 0 5 2 0 9 0 8 7 2 2 7 1 2 3 4 3 6 8 7 9 4 5 8]\n",
            "Prediction : [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5]\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "Batch: 30\n",
            "y_true: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "y_pred: [[[ 5.81837429e+29  7.86264219e+29  2.20109570e+29  6.65075764e+29\n",
            "    6.04582982e+29  5.15580963e+29  3.98998322e+29  1.12531221e+28\n",
            "   -5.04261263e+30  1.25891026e+30]]\n",
            "\n",
            " [[ 7.60569807e+29  1.02779367e+30  2.87724173e+29  8.69377803e+29\n",
            "    7.90302478e+29  6.73960275e+29  5.21565065e+29  1.47099249e+28\n",
            "   -6.59163321e+30  1.64563001e+30]]\n",
            "\n",
            " [[ 6.49826007e+29  8.78140375e+29  2.45829704e+29  7.42790867e+29\n",
            "    6.75229412e+29  5.75827374e+29  4.45621875e+29  1.25680663e+28\n",
            "   -5.63184950e+30  1.40601582e+30]]\n",
            "\n",
            " [[ 1.06178728e+30  1.43484297e+30  4.01674987e+29  1.21368780e+30\n",
            "    1.10329533e+30  9.40876749e+29  7.28126658e+29  2.05356707e+28\n",
            "   -9.20219581e+30  2.29736837e+30]]\n",
            "\n",
            " [[ 8.58741627e+29  1.16045786e+30  3.24862652e+29  9.81594197e+29\n",
            "    8.92312092e+29  7.60952825e+29  5.88886948e+29  1.66086330e+28\n",
            "   -7.44245929e+30  1.85804246e+30]]\n",
            "\n",
            " [[ 5.10598564e+29  6.89995798e+29  1.93159850e+29  5.83645385e+29\n",
            "    5.30559202e+29  4.52454391e+29  3.50145865e+29  9.87531512e+27\n",
            "   -4.42520649e+30  1.10477212e+30]]\n",
            "\n",
            " [[ 9.15867571e+29  1.23765482e+30  3.46473443e+29  1.04689264e+30\n",
            "    9.51671238e+29  8.11573578e+29  6.28061388e+29  1.77134867e+28\n",
            "   -7.93755292e+30  1.98164474e+30]]\n",
            "\n",
            " [[ 1.26048307e+30  1.70334991e+30  4.76841763e+29  1.44080923e+30\n",
            "    1.30975866e+30  1.11694614e+30  8.64383421e+29  2.43785791e+28\n",
            "   -1.09242333e+31  2.72728255e+30]]\n",
            "\n",
            " [[ 1.75445290e+30  2.37087453e+30  6.63710953e+29  2.00544696e+30\n",
            "    1.82303907e+30  1.55466540e+30  1.20312604e+30  3.39322836e+28\n",
            "   -1.52053236e+31  3.79607542e+30]]\n",
            "\n",
            " [[ 5.80175262e+29  7.84018055e+29  2.19480771e+29  6.63175806e+29\n",
            "    6.02855836e+29  5.14108075e+29  3.97858481e+29  1.12209746e+28\n",
            "   -5.02820713e+30  1.25531386e+30]]\n",
            "\n",
            " [[ 4.09838369e+29  5.53833819e+29  1.55042187e+29  4.68470320e+29\n",
            "    4.25860027e+29  3.63168216e+29  2.81048990e+29  7.92654608e+27\n",
            "   -3.55194774e+30  8.86759262e+29]]\n",
            "\n",
            " [[ 8.93125481e+29  1.20692237e+30  3.37870092e+29  1.02089704e+30\n",
            "    9.28040101e+29  7.91421233e+29  6.12465871e+29  1.72736396e+28\n",
            "   -7.74045396e+30  1.93243813e+30]]\n",
            "\n",
            " [[ 4.24966641e+29  5.74277363e+29  1.60765224e+29  4.85762860e+29\n",
            "    4.41579703e+29  3.76573763e+29  2.91423288e+29  8.21913690e+27\n",
            "   -3.68305999e+30  9.19492008e+29]]\n",
            "\n",
            " [[ 1.12752586e+30  1.52367860e+30  4.26543945e+29  1.28883101e+30\n",
            "    1.17160380e+30  9.99129377e+29  7.73207262e+29  2.18070985e+28\n",
            "   -9.77193260e+30  2.43960564e+30]]\n",
            "\n",
            " [[ 9.28066583e+29  1.25413992e+30  3.51088339e+29  1.06083686e+30\n",
            "    9.64347142e+29  8.22383434e+29  6.36426931e+29  1.79494237e+28\n",
            "   -8.04327814e+30  2.00803951e+30]]\n",
            "\n",
            " [[ 4.98292284e+29  6.73365744e+29  1.88504374e+29  5.69578553e+29\n",
            "    5.17771837e+29  4.41549482e+29  3.41706764e+29  9.63730349e+27\n",
            "   -4.31855161e+30  1.07814527e+30]]\n",
            "\n",
            " [[ 9.45305566e+29  1.27743577e+30  3.57609860e+29  1.08054207e+30\n",
            "    9.82260042e+29  8.37659335e+29  6.48248661e+29  1.82828371e+28\n",
            "   -8.19268330e+30  2.04533916e+30]]\n",
            "\n",
            " [[ 2.68232552e+29  3.62475234e+29  1.01472591e+29  3.06606211e+29\n",
            "    2.78718467e+29  2.37687695e+29  1.83941996e+29  5.18779559e+27\n",
            "   -2.32469207e+30  5.80369525e+29]]\n",
            "\n",
            " [[ 6.18694067e+29  8.36070324e+29  2.34052466e+29  7.07205156e+29\n",
            "    6.42880443e+29  5.48240569e+29  4.24272971e+29  1.19659539e+28\n",
            "   -5.36203820e+30  1.33865625e+30]]\n",
            "\n",
            " [[ 2.39178699e+29  3.23213398e+29  9.04814952e+28  2.73395880e+29\n",
            "    2.48528822e+29  2.11942336e+29  1.64018151e+29  4.62587478e+27\n",
            "   -2.07289093e+30  5.17506271e+29]]\n",
            "\n",
            " [[ 4.73318794e+29  6.39617895e+29  1.79056882e+29  5.41032327e+29\n",
            "    4.91822068e+29  4.19419837e+29  3.24581051e+29  9.15429962e+27\n",
            "   -4.10211377e+30  1.02411061e+30]]\n",
            "\n",
            " [[ 8.18496671e+29  1.10607296e+30  3.09637953e+29  9.35591750e+29\n",
            "    8.50493854e+29  7.25290744e+29  5.61288741e+29  1.58302689e+28\n",
            "   -7.09366817e+30  1.77096523e+30]]\n",
            "\n",
            " [[ 6.16054362e+29  8.32503166e+29  2.33053863e+29  7.04187811e+29\n",
            "    6.40137544e+29  5.45901459e+29  4.22462778e+29  1.19149002e+28\n",
            "   -5.33916065e+30  1.33294477e+30]]\n",
            "\n",
            " [[ 7.65103065e+29  1.03391967e+30  2.89439109e+29  8.74559594e+29\n",
            "    7.95012953e+29  6.77977310e+29  5.24673773e+29  1.47976011e+28\n",
            "   -6.63092161e+30  1.65543853e+30]]\n",
            "\n",
            " [[ 6.53142124e+29  8.82621599e+29  2.47084194e+29  7.46581390e+29\n",
            "    6.78675164e+29  5.78765869e+29  4.47895921e+29  1.26322022e+28\n",
            "   -5.66058930e+30  1.41319083e+30]]\n",
            "\n",
            " [[ 5.88766221e+29  7.95627421e+29  2.22730738e+29  6.72995797e+29\n",
            "    6.11782639e+29  5.21720742e+29  4.03749780e+29  1.13871295e+28\n",
            "   -5.10266242e+30  1.27390195e+30]]\n",
            "\n",
            " [[ 1.22459158e+30  1.65484806e+30  4.63263984e+29  1.39978306e+30\n",
            "    1.27246408e+30  1.08514178e+30  8.39770630e+29  2.36844139e+28\n",
            "   -1.06131724e+31  2.64962484e+30]]\n",
            "\n",
            " [[ 4.47606776e+29  6.04872039e+29  1.69330005e+29  5.11641918e+29\n",
            "    4.65104899e+29  3.96635763e+29  3.06948889e+29  8.65701212e+27\n",
            "   -3.87927532e+30  9.68478023e+29]]\n",
            "\n",
            " [[ 9.86322771e+29  1.33286424e+30  3.73126702e+29  1.12742725e+30\n",
            "    1.02488072e+30  8.74005725e+29  6.76376442e+29  1.90761370e+28\n",
            "   -8.54816726e+30  2.13408728e+30]]\n",
            "\n",
            " [[ 8.37252575e+29  1.13141869e+30  3.16733327e+29  9.57030896e+29\n",
            "    8.69982976e+29  7.41910829e+29  5.74150709e+29  1.61930205e+28\n",
            "   -7.25622004e+30  1.81154702e+30]]\n",
            "\n",
            " [[ 8.67194287e+29  1.17188033e+30  3.28060300e+29  9.91256104e+29\n",
            "    9.01095188e+29  7.68442942e+29  5.94683408e+29  1.67721131e+28\n",
            "   -7.51571599e+30  1.87633131e+30]]\n",
            "\n",
            " [[ 8.52930217e+29  1.15260462e+30  3.22664191e+29  9.74951400e+29\n",
            "    8.86273499e+29  7.55803186e+29  5.84901741e+29  1.64962365e+28\n",
            "   -7.39209352e+30  1.84546842e+30]]\n",
            "\n",
            " [[ 7.56448325e+29  1.02222412e+30  2.86165013e+29  8.64666696e+29\n",
            "    7.86019876e+29  6.70308124e+29  5.18738735e+29  1.46302127e+28\n",
            "   -6.55591354e+30  1.63671244e+30]]]\n",
            "loss: [41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167  0.         41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167  0.         41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167]\n",
            "loss grad: [[[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  0.e+00]]\n",
            "\n",
            " [[-1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[-1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e+00]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  0.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e+00]]]\n",
            "linear2 grad input: [[[-1.25418052e+12 -1.25418037e+12 -1.25418052e+12 -1.25418053e+12\n",
            "   -1.25418089e+12 -1.25418060e+12 -1.25418049e+12 -1.25418049e+12\n",
            "   -1.25418056e+12 -1.25418052e+12 -1.25418084e+12 -1.25418049e+12\n",
            "   -1.25418067e+12 -1.25418085e+12 -1.25418065e+12 -1.25418084e+12\n",
            "   -1.25418068e+12 -1.25418040e+12 -1.25418163e+12 -1.25418066e+12]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]\n",
            "\n",
            " [[-1.67212772e+13 -1.67212824e+13 -1.67212774e+13 -1.67212762e+13\n",
            "   -1.67212860e+13 -1.67212790e+13 -1.67212769e+13 -1.67212771e+13\n",
            "   -1.67212777e+13 -1.67212789e+13 -1.67212869e+13 -1.67212766e+13\n",
            "   -1.67212775e+13 -1.67212950e+13 -1.67212859e+13 -1.67212912e+13\n",
            "   -1.67212768e+13 -1.67212758e+13 -1.67213226e+13 -1.67212787e+13]]\n",
            "\n",
            " [[-1.97244662e+12 -1.97244669e+12 -1.97244656e+12 -1.97244652e+12\n",
            "   -1.97244714e+12 -1.97244668e+12 -1.97244656e+12 -1.97244653e+12\n",
            "   -1.97244659e+12 -1.97244658e+12 -1.97244726e+12 -1.97244652e+12\n",
            "   -1.97244675e+12 -1.97244767e+12 -1.97244699e+12 -1.97244744e+12\n",
            "   -1.97244667e+12 -1.97244639e+12 -1.97244947e+12 -1.97244675e+12]]\n",
            "\n",
            " [[-2.28180215e+12 -2.28180233e+12 -2.28180202e+12 -2.28180201e+12\n",
            "   -2.28180278e+12 -2.28180228e+12 -2.28180197e+12 -2.28180196e+12\n",
            "   -2.28180211e+12 -2.28180219e+12 -2.28180279e+12 -2.28180205e+12\n",
            "   -2.28180223e+12 -2.28180345e+12 -2.28180265e+12 -2.28180321e+12\n",
            "   -2.28180206e+12 -2.28180198e+12 -2.28180565e+12 -2.28180228e+12]]\n",
            "\n",
            " [[-3.31069423e+12 -3.31069598e+12 -3.31069436e+12 -3.31069400e+12\n",
            "   -3.31069733e+12 -3.31069490e+12 -3.31069428e+12 -3.31069439e+12\n",
            "   -3.31069447e+12 -3.31069485e+12 -3.31069768e+12 -3.31069414e+12\n",
            "   -3.31069449e+12 -3.31070029e+12 -3.31069721e+12 -3.31069889e+12\n",
            "   -3.31069429e+12 -3.31069381e+12 -3.31070953e+12 -3.31069483e+12]]\n",
            "\n",
            " [[-1.57575785e+12 -1.57575769e+12 -1.57575779e+12 -1.57575785e+12\n",
            "   -1.57575801e+12 -1.57575788e+12 -1.57575781e+12 -1.57575779e+12\n",
            "   -1.57575781e+12 -1.57575781e+12 -1.57575808e+12 -1.57575782e+12\n",
            "   -1.57575799e+12 -1.57575808e+12 -1.57575786e+12 -1.57575800e+12\n",
            "   -1.57575801e+12 -1.57575773e+12 -1.57575845e+12 -1.57575799e+12]]\n",
            "\n",
            " [[-1.67212772e+13 -1.67212824e+13 -1.67212774e+13 -1.67212762e+13\n",
            "   -1.67212860e+13 -1.67212790e+13 -1.67212769e+13 -1.67212771e+13\n",
            "   -1.67212777e+13 -1.67212789e+13 -1.67212869e+13 -1.67212766e+13\n",
            "   -1.67212775e+13 -1.67212950e+13 -1.67212859e+13 -1.67212912e+13\n",
            "   -1.67212768e+13 -1.67212758e+13 -1.67213226e+13 -1.67212787e+13]]\n",
            "\n",
            " [[-2.28180215e+12 -2.28180233e+12 -2.28180202e+12 -2.28180201e+12\n",
            "   -2.28180278e+12 -2.28180228e+12 -2.28180197e+12 -2.28180196e+12\n",
            "   -2.28180211e+12 -2.28180219e+12 -2.28180279e+12 -2.28180205e+12\n",
            "   -2.28180223e+12 -2.28180345e+12 -2.28180265e+12 -2.28180321e+12\n",
            "   -2.28180206e+12 -2.28180198e+12 -2.28180565e+12 -2.28180228e+12]]\n",
            "\n",
            " [[-1.97244662e+12 -1.97244669e+12 -1.97244656e+12 -1.97244652e+12\n",
            "   -1.97244714e+12 -1.97244668e+12 -1.97244656e+12 -1.97244653e+12\n",
            "   -1.97244659e+12 -1.97244658e+12 -1.97244726e+12 -1.97244652e+12\n",
            "   -1.97244675e+12 -1.97244767e+12 -1.97244699e+12 -1.97244744e+12\n",
            "   -1.97244667e+12 -1.97244639e+12 -1.97244947e+12 -1.97244675e+12]]\n",
            "\n",
            " [[-2.28180215e+12 -2.28180233e+12 -2.28180202e+12 -2.28180201e+12\n",
            "   -2.28180278e+12 -2.28180228e+12 -2.28180197e+12 -2.28180196e+12\n",
            "   -2.28180211e+12 -2.28180219e+12 -2.28180279e+12 -2.28180205e+12\n",
            "   -2.28180223e+12 -2.28180345e+12 -2.28180265e+12 -2.28180321e+12\n",
            "   -2.28180206e+12 -2.28180198e+12 -2.28180565e+12 -2.28180228e+12]]\n",
            "\n",
            " [[-1.25418052e+12 -1.25418037e+12 -1.25418052e+12 -1.25418053e+12\n",
            "   -1.25418089e+12 -1.25418060e+12 -1.25418049e+12 -1.25418049e+12\n",
            "   -1.25418056e+12 -1.25418052e+12 -1.25418084e+12 -1.25418049e+12\n",
            "   -1.25418067e+12 -1.25418085e+12 -1.25418065e+12 -1.25418084e+12\n",
            "   -1.25418068e+12 -1.25418040e+12 -1.25418163e+12 -1.25418066e+12]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]\n",
            "\n",
            " [[-1.73627720e+12 -1.73627721e+12 -1.73627714e+12 -1.73627719e+12\n",
            "   -1.73627747e+12 -1.73627727e+12 -1.73627717e+12 -1.73627714e+12\n",
            "   -1.73627725e+12 -1.73627727e+12 -1.73627742e+12 -1.73627724e+12\n",
            "   -1.73627730e+12 -1.73627770e+12 -1.73627737e+12 -1.73627756e+12\n",
            "   -1.73627716e+12 -1.73627721e+12 -1.73627861e+12 -1.73627724e+12]]\n",
            "\n",
            " [[-1.73627720e+12 -1.73627721e+12 -1.73627714e+12 -1.73627719e+12\n",
            "   -1.73627747e+12 -1.73627727e+12 -1.73627717e+12 -1.73627714e+12\n",
            "   -1.73627725e+12 -1.73627727e+12 -1.73627742e+12 -1.73627724e+12\n",
            "   -1.73627730e+12 -1.73627770e+12 -1.73627737e+12 -1.73627756e+12\n",
            "   -1.73627716e+12 -1.73627721e+12 -1.73627861e+12 -1.73627724e+12]]\n",
            "\n",
            " [[ 3.34055575e-06  3.34055648e-06  3.34055575e-06  3.34055559e-06\n",
            "    3.34055729e-06  3.34055607e-06  3.34055569e-06  3.34055570e-06\n",
            "    3.34055584e-06  3.34055599e-06  3.34055742e-06  3.34055566e-06\n",
            "    3.34055592e-06  3.34055876e-06  3.34055714e-06  3.34055808e-06\n",
            "    3.34055575e-06  3.34055549e-06  3.34056352e-06  3.34055606e-06]]\n",
            "\n",
            " [[-1.79663304e+12 -1.79663305e+12 -1.79663304e+12 -1.79663301e+12\n",
            "   -1.79663366e+12 -1.79663312e+12 -1.79663310e+12 -1.79663299e+12\n",
            "   -1.79663320e+12 -1.79663304e+12 -1.79663360e+12 -1.79663306e+12\n",
            "   -1.79663335e+12 -1.79663404e+12 -1.79663335e+12 -1.79663365e+12\n",
            "   -1.79663313e+12 -1.79663298e+12 -1.79663582e+12 -1.79663319e+12]]\n",
            "\n",
            " [[-1.79663304e+12 -1.79663305e+12 -1.79663304e+12 -1.79663301e+12\n",
            "   -1.79663366e+12 -1.79663312e+12 -1.79663310e+12 -1.79663299e+12\n",
            "   -1.79663320e+12 -1.79663304e+12 -1.79663360e+12 -1.79663306e+12\n",
            "   -1.79663335e+12 -1.79663404e+12 -1.79663335e+12 -1.79663365e+12\n",
            "   -1.79663313e+12 -1.79663298e+12 -1.79663582e+12 -1.79663319e+12]]\n",
            "\n",
            " [[-1.25418052e+12 -1.25418037e+12 -1.25418052e+12 -1.25418053e+12\n",
            "   -1.25418089e+12 -1.25418060e+12 -1.25418049e+12 -1.25418049e+12\n",
            "   -1.25418056e+12 -1.25418052e+12 -1.25418084e+12 -1.25418049e+12\n",
            "   -1.25418067e+12 -1.25418085e+12 -1.25418065e+12 -1.25418084e+12\n",
            "   -1.25418068e+12 -1.25418040e+12 -1.25418163e+12 -1.25418066e+12]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]\n",
            "\n",
            " [[-1.97244662e+12 -1.97244669e+12 -1.97244656e+12 -1.97244652e+12\n",
            "   -1.97244714e+12 -1.97244668e+12 -1.97244656e+12 -1.97244653e+12\n",
            "   -1.97244659e+12 -1.97244658e+12 -1.97244726e+12 -1.97244652e+12\n",
            "   -1.97244675e+12 -1.97244767e+12 -1.97244699e+12 -1.97244744e+12\n",
            "   -1.97244667e+12 -1.97244639e+12 -1.97244947e+12 -1.97244675e+12]]\n",
            "\n",
            " [[-2.28180215e+12 -2.28180233e+12 -2.28180202e+12 -2.28180201e+12\n",
            "   -2.28180278e+12 -2.28180228e+12 -2.28180197e+12 -2.28180196e+12\n",
            "   -2.28180211e+12 -2.28180219e+12 -2.28180279e+12 -2.28180205e+12\n",
            "   -2.28180223e+12 -2.28180345e+12 -2.28180265e+12 -2.28180321e+12\n",
            "   -2.28180206e+12 -2.28180198e+12 -2.28180565e+12 -2.28180228e+12]]\n",
            "\n",
            " [[-1.67212772e+13 -1.67212824e+13 -1.67212774e+13 -1.67212762e+13\n",
            "   -1.67212860e+13 -1.67212790e+13 -1.67212769e+13 -1.67212771e+13\n",
            "   -1.67212777e+13 -1.67212789e+13 -1.67212869e+13 -1.67212766e+13\n",
            "   -1.67212775e+13 -1.67212950e+13 -1.67212859e+13 -1.67212912e+13\n",
            "   -1.67212768e+13 -1.67212758e+13 -1.67213226e+13 -1.67212787e+13]]\n",
            "\n",
            " [[-1.25418052e+12 -1.25418037e+12 -1.25418052e+12 -1.25418053e+12\n",
            "   -1.25418089e+12 -1.25418060e+12 -1.25418049e+12 -1.25418049e+12\n",
            "   -1.25418056e+12 -1.25418052e+12 -1.25418084e+12 -1.25418049e+12\n",
            "   -1.25418067e+12 -1.25418085e+12 -1.25418065e+12 -1.25418084e+12\n",
            "   -1.25418068e+12 -1.25418040e+12 -1.25418163e+12 -1.25418066e+12]]\n",
            "\n",
            " [[-1.25418052e+12 -1.25418037e+12 -1.25418052e+12 -1.25418053e+12\n",
            "   -1.25418089e+12 -1.25418060e+12 -1.25418049e+12 -1.25418049e+12\n",
            "   -1.25418056e+12 -1.25418052e+12 -1.25418084e+12 -1.25418049e+12\n",
            "   -1.25418067e+12 -1.25418085e+12 -1.25418065e+12 -1.25418084e+12\n",
            "   -1.25418068e+12 -1.25418040e+12 -1.25418163e+12 -1.25418066e+12]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]\n",
            "\n",
            " [[ 3.34055575e-06  3.34055648e-06  3.34055575e-06  3.34055559e-06\n",
            "    3.34055729e-06  3.34055607e-06  3.34055569e-06  3.34055570e-06\n",
            "    3.34055584e-06  3.34055599e-06  3.34055742e-06  3.34055566e-06\n",
            "    3.34055592e-06  3.34055876e-06  3.34055714e-06  3.34055808e-06\n",
            "    3.34055575e-06  3.34055549e-06  3.34056352e-06  3.34055606e-06]]\n",
            "\n",
            " [[-1.73627720e+12 -1.73627721e+12 -1.73627714e+12 -1.73627719e+12\n",
            "   -1.73627747e+12 -1.73627727e+12 -1.73627717e+12 -1.73627714e+12\n",
            "   -1.73627725e+12 -1.73627727e+12 -1.73627742e+12 -1.73627724e+12\n",
            "   -1.73627730e+12 -1.73627770e+12 -1.73627737e+12 -1.73627756e+12\n",
            "   -1.73627716e+12 -1.73627721e+12 -1.73627861e+12 -1.73627724e+12]]\n",
            "\n",
            " [[-1.57575785e+12 -1.57575769e+12 -1.57575779e+12 -1.57575785e+12\n",
            "   -1.57575801e+12 -1.57575788e+12 -1.57575781e+12 -1.57575779e+12\n",
            "   -1.57575781e+12 -1.57575781e+12 -1.57575808e+12 -1.57575782e+12\n",
            "   -1.57575799e+12 -1.57575808e+12 -1.57575786e+12 -1.57575800e+12\n",
            "   -1.57575801e+12 -1.57575773e+12 -1.57575845e+12 -1.57575799e+12]]\n",
            "\n",
            " [[-1.57575785e+12 -1.57575769e+12 -1.57575779e+12 -1.57575785e+12\n",
            "   -1.57575801e+12 -1.57575788e+12 -1.57575781e+12 -1.57575779e+12\n",
            "   -1.57575781e+12 -1.57575781e+12 -1.57575808e+12 -1.57575782e+12\n",
            "   -1.57575799e+12 -1.57575808e+12 -1.57575786e+12 -1.57575800e+12\n",
            "   -1.57575801e+12 -1.57575773e+12 -1.57575845e+12 -1.57575799e+12]]\n",
            "\n",
            " [[-1.67212772e+13 -1.67212824e+13 -1.67212774e+13 -1.67212762e+13\n",
            "   -1.67212860e+13 -1.67212790e+13 -1.67212769e+13 -1.67212771e+13\n",
            "   -1.67212777e+13 -1.67212789e+13 -1.67212869e+13 -1.67212766e+13\n",
            "   -1.67212775e+13 -1.67212950e+13 -1.67212859e+13 -1.67212912e+13\n",
            "   -1.67212768e+13 -1.67212758e+13 -1.67213226e+13 -1.67212787e+13]]\n",
            "\n",
            " [[-2.75648870e+12 -2.75648903e+12 -2.75648864e+12 -2.75648860e+12\n",
            "   -2.75648965e+12 -2.75648887e+12 -2.75648862e+12 -2.75648860e+12\n",
            "   -2.75648876e+12 -2.75648878e+12 -2.75648966e+12 -2.75648865e+12\n",
            "   -2.75648888e+12 -2.75649052e+12 -2.75648945e+12 -2.75649006e+12\n",
            "   -2.75648870e+12 -2.75648854e+12 -2.75649347e+12 -2.75648891e+12]]]\n",
            "linear2 input: [[[-1.88428195e+16 -1.88428215e+16 -1.88428196e+16 -1.88428193e+16\n",
            "   -1.88428222e+16 -1.88428201e+16 -1.88428196e+16 -1.88428197e+16\n",
            "   -1.88428197e+16 -1.88428200e+16 -1.88428226e+16 -1.88428194e+16\n",
            "   -1.88428197e+16 -1.88428254e+16 -1.88428224e+16 -1.88428239e+16\n",
            "   -1.88428193e+16 -1.88428192e+16 -1.88428341e+16 -1.88428199e+16]]\n",
            "\n",
            " [[-2.46310720e+16 -2.46310748e+16 -2.46310727e+16 -2.46310713e+16\n",
            "   -2.46310759e+16 -2.46310730e+16 -2.46310721e+16 -2.46310720e+16\n",
            "   -2.46310722e+16 -2.46310732e+16 -2.46310765e+16 -2.46310715e+16\n",
            "   -2.46310713e+16 -2.46310791e+16 -2.46310763e+16 -2.46310783e+16\n",
            "   -2.46310717e+16 -2.46310714e+16 -2.46310906e+16 -2.46310723e+16]]\n",
            "\n",
            " [[-2.10446314e+16 -2.10446335e+16 -2.10446317e+16 -2.10446308e+16\n",
            "   -2.10446345e+16 -2.10446320e+16 -2.10446313e+16 -2.10446314e+16\n",
            "   -2.10446315e+16 -2.10446319e+16 -2.10446350e+16 -2.10446310e+16\n",
            "   -2.10446312e+16 -2.10446377e+16 -2.10446347e+16 -2.10446364e+16\n",
            "   -2.10446312e+16 -2.10446307e+16 -2.10446474e+16 -2.10446318e+16]]\n",
            "\n",
            " [[-3.43860070e+16 -3.43860104e+16 -3.43860075e+16 -3.43860069e+16\n",
            "   -3.43860128e+16 -3.43860082e+16 -3.43860074e+16 -3.43860077e+16\n",
            "   -3.43860079e+16 -3.43860082e+16 -3.43860129e+16 -3.43860072e+16\n",
            "   -3.43860077e+16 -3.43860180e+16 -3.43860125e+16 -3.43860151e+16\n",
            "   -3.43860069e+16 -3.43860067e+16 -3.43860347e+16 -3.43860079e+16]]\n",
            "\n",
            " [[-2.78103690e+16 -2.78103720e+16 -2.78103690e+16 -2.78103683e+16\n",
            "   -2.78103727e+16 -2.78103699e+16 -2.78103686e+16 -2.78103689e+16\n",
            "   -2.78103689e+16 -2.78103698e+16 -2.78103733e+16 -2.78103687e+16\n",
            "   -2.78103688e+16 -2.78103774e+16 -2.78103732e+16 -2.78103757e+16\n",
            "   -2.78103685e+16 -2.78103685e+16 -2.78103901e+16 -2.78103696e+16]]\n",
            "\n",
            " [[-1.65357468e+16 -1.65357486e+16 -1.65357472e+16 -1.65357468e+16\n",
            "   -1.65357497e+16 -1.65357476e+16 -1.65357472e+16 -1.65357472e+16\n",
            "   -1.65357473e+16 -1.65357477e+16 -1.65357500e+16 -1.65357470e+16\n",
            "   -1.65357472e+16 -1.65357522e+16 -1.65357497e+16 -1.65357509e+16\n",
            "   -1.65357472e+16 -1.65357466e+16 -1.65357601e+16 -1.65357476e+16]]\n",
            "\n",
            " [[-2.96603940e+16 -2.96603964e+16 -2.96603944e+16 -2.96603933e+16\n",
            "   -2.96603988e+16 -2.96603947e+16 -2.96603938e+16 -2.96603938e+16\n",
            "   -2.96603944e+16 -2.96603942e+16 -2.96603991e+16 -2.96603936e+16\n",
            "   -2.96603943e+16 -2.96604030e+16 -2.96603984e+16 -2.96604011e+16\n",
            "   -2.96603939e+16 -2.96603929e+16 -2.96604172e+16 -2.96603948e+16]]\n",
            "\n",
            " [[-4.08207751e+16 -4.08207794e+16 -4.08207754e+16 -4.08207747e+16\n",
            "   -4.08207812e+16 -4.08207764e+16 -4.08207752e+16 -4.08207758e+16\n",
            "   -4.08207754e+16 -4.08207764e+16 -4.08207822e+16 -4.08207749e+16\n",
            "   -4.08207751e+16 -4.08207879e+16 -4.08207816e+16 -4.08207852e+16\n",
            "   -4.08207749e+16 -4.08207745e+16 -4.08208069e+16 -4.08207760e+16]]\n",
            "\n",
            " [[-5.68179999e+16 -5.68180055e+16 -5.68180012e+16 -5.68179992e+16\n",
            "   -5.68180103e+16 -5.68180028e+16 -5.68180005e+16 -5.68180008e+16\n",
            "   -5.68180019e+16 -5.68180028e+16 -5.68180103e+16 -5.68180002e+16\n",
            "   -5.68180010e+16 -5.68180175e+16 -5.68180094e+16 -5.68180137e+16\n",
            "   -5.68180000e+16 -5.68179997e+16 -5.68180457e+16 -5.68180015e+16]]\n",
            "\n",
            " [[-1.87889899e+16 -1.87889919e+16 -1.87889905e+16 -1.87889899e+16\n",
            "   -1.87889932e+16 -1.87889907e+16 -1.87889904e+16 -1.87889904e+16\n",
            "   -1.87889904e+16 -1.87889910e+16 -1.87889936e+16 -1.87889900e+16\n",
            "   -1.87889901e+16 -1.87889958e+16 -1.87889932e+16 -1.87889946e+16\n",
            "   -1.87889902e+16 -1.87889896e+16 -1.87890051e+16 -1.87889905e+16]]\n",
            "\n",
            " [[-1.32726257e+16 -1.32726272e+16 -1.32726258e+16 -1.32726255e+16\n",
            "   -1.32726277e+16 -1.32726260e+16 -1.32726257e+16 -1.32726261e+16\n",
            "   -1.32726258e+16 -1.32726258e+16 -1.32726282e+16 -1.32726256e+16\n",
            "   -1.32726257e+16 -1.32726299e+16 -1.32726278e+16 -1.32726288e+16\n",
            "   -1.32726259e+16 -1.32726253e+16 -1.32726357e+16 -1.32726260e+16]]\n",
            "\n",
            " [[-2.89238904e+16 -2.89238935e+16 -2.89238909e+16 -2.89238902e+16\n",
            "   -2.89238952e+16 -2.89238917e+16 -2.89238908e+16 -2.89238915e+16\n",
            "   -2.89238908e+16 -2.89238918e+16 -2.89238963e+16 -2.89238902e+16\n",
            "   -2.89238906e+16 -2.89238997e+16 -2.89238955e+16 -2.89238979e+16\n",
            "   -2.89238908e+16 -2.89238895e+16 -2.89239133e+16 -2.89238913e+16]]\n",
            "\n",
            " [[-1.37625554e+16 -1.37625569e+16 -1.37625553e+16 -1.37625548e+16\n",
            "   -1.37625571e+16 -1.37625558e+16 -1.37625550e+16 -1.37625551e+16\n",
            "   -1.37625553e+16 -1.37625555e+16 -1.37625574e+16 -1.37625551e+16\n",
            "   -1.37625552e+16 -1.37625595e+16 -1.37625572e+16 -1.37625586e+16\n",
            "   -1.37625550e+16 -1.37625551e+16 -1.37625656e+16 -1.37625556e+16]]\n",
            "\n",
            " [[-3.65149530e+16 -3.65149569e+16 -3.65149530e+16 -3.65149520e+16\n",
            "   -3.65149580e+16 -3.65149543e+16 -3.65149526e+16 -3.65149529e+16\n",
            "   -3.65149530e+16 -3.65149540e+16 -3.65149590e+16 -3.65149524e+16\n",
            "   -3.65149527e+16 -3.65149639e+16 -3.65149583e+16 -3.65149619e+16\n",
            "   -3.65149527e+16 -3.65149522e+16 -3.65149808e+16 -3.65149538e+16]]\n",
            "\n",
            " [[-3.00554592e+16 -3.00554615e+16 -3.00554599e+16 -3.00554583e+16\n",
            "   -3.00554645e+16 -3.00554600e+16 -3.00554593e+16 -3.00554591e+16\n",
            "   -3.00554597e+16 -3.00554598e+16 -3.00554648e+16 -3.00554587e+16\n",
            "   -3.00554596e+16 -3.00554683e+16 -3.00554640e+16 -3.00554664e+16\n",
            "   -3.00554591e+16 -3.00554579e+16 -3.00554830e+16 -3.00554601e+16]]\n",
            "\n",
            " [[-1.61372080e+16 -1.61372091e+16 -1.61372080e+16 -1.61372075e+16\n",
            "   -1.61372106e+16 -1.61372086e+16 -1.61372075e+16 -1.61372076e+16\n",
            "   -1.61372080e+16 -1.61372082e+16 -1.61372108e+16 -1.61372077e+16\n",
            "   -1.61372081e+16 -1.61372128e+16 -1.61372103e+16 -1.61372123e+16\n",
            "   -1.61372080e+16 -1.61372073e+16 -1.61372208e+16 -1.61372085e+16]]\n",
            "\n",
            " [[-3.06137441e+16 -3.06137467e+16 -3.06137441e+16 -3.06137437e+16\n",
            "   -3.06137488e+16 -3.06137449e+16 -3.06137442e+16 -3.06137441e+16\n",
            "   -3.06137446e+16 -3.06137447e+16 -3.06137491e+16 -3.06137440e+16\n",
            "   -3.06137448e+16 -3.06137537e+16 -3.06137483e+16 -3.06137511e+16\n",
            "   -3.06137441e+16 -3.06137436e+16 -3.06137686e+16 -3.06137449e+16]]\n",
            "\n",
            " [[-8.68671775e+15 -8.68671868e+15 -8.68671769e+15 -8.68671780e+15\n",
            "   -8.68671913e+15 -8.68671829e+15 -8.68671768e+15 -8.68671781e+15\n",
            "   -8.68671766e+15 -8.68671829e+15 -8.68671946e+15 -8.68671779e+15\n",
            "   -8.68671768e+15 -8.68672053e+15 -8.68671935e+15 -8.68672020e+15\n",
            "   -8.68671774e+15 -8.68671763e+15 -8.68672434e+15 -8.68671810e+15]]\n",
            "\n",
            " [[-2.00364227e+16 -2.00364249e+16 -2.00364228e+16 -2.00364222e+16\n",
            "   -2.00364259e+16 -2.00364234e+16 -2.00364223e+16 -2.00364227e+16\n",
            "   -2.00364228e+16 -2.00364236e+16 -2.00364260e+16 -2.00364225e+16\n",
            "   -2.00364222e+16 -2.00364287e+16 -2.00364261e+16 -2.00364280e+16\n",
            "   -2.00364224e+16 -2.00364222e+16 -2.00364379e+16 -2.00364231e+16]]\n",
            "\n",
            " [[-7.74580813e+15 -7.74580883e+15 -7.74580806e+15 -7.74580800e+15\n",
            "   -7.74580912e+15 -7.74580835e+15 -7.74580807e+15 -7.74580802e+15\n",
            "   -7.74580815e+15 -7.74580838e+15 -7.74580923e+15 -7.74580810e+15\n",
            "   -7.74580813e+15 -7.74581051e+15 -7.74580923e+15 -7.74580991e+15\n",
            "   -7.74580788e+15 -7.74580804e+15 -7.74581407e+15 -7.74580826e+15]]\n",
            "\n",
            " [[-1.53284407e+16 -1.53284426e+16 -1.53284408e+16 -1.53284407e+16\n",
            "   -1.53284429e+16 -1.53284414e+16 -1.53284407e+16 -1.53284412e+16\n",
            "   -1.53284408e+16 -1.53284413e+16 -1.53284435e+16 -1.53284407e+16\n",
            "   -1.53284407e+16 -1.53284456e+16 -1.53284432e+16 -1.53284447e+16\n",
            "   -1.53284407e+16 -1.53284406e+16 -1.53284526e+16 -1.53284410e+16]]\n",
            "\n",
            " [[-2.65070348e+16 -2.65070377e+16 -2.65070353e+16 -2.65070343e+16\n",
            "   -2.65070393e+16 -2.65070356e+16 -2.65070351e+16 -2.65070349e+16\n",
            "   -2.65070354e+16 -2.65070360e+16 -2.65070392e+16 -2.65070348e+16\n",
            "   -2.65070350e+16 -2.65070430e+16 -2.65070393e+16 -2.65070407e+16\n",
            "   -2.65070344e+16 -2.65070344e+16 -2.65070553e+16 -2.65070355e+16]]\n",
            "\n",
            " [[-1.99509354e+16 -1.99509375e+16 -1.99509360e+16 -1.99509354e+16\n",
            "   -1.99509389e+16 -1.99509362e+16 -1.99509359e+16 -1.99509360e+16\n",
            "   -1.99509359e+16 -1.99509365e+16 -1.99509393e+16 -1.99509355e+16\n",
            "   -1.99509357e+16 -1.99509418e+16 -1.99509390e+16 -1.99509403e+16\n",
            "   -1.99509357e+16 -1.99509350e+16 -1.99509514e+16 -1.99509361e+16]]\n",
            "\n",
            " [[-2.47778815e+16 -2.47778840e+16 -2.47778820e+16 -2.47778813e+16\n",
            "   -2.47778857e+16 -2.47778822e+16 -2.47778819e+16 -2.47778820e+16\n",
            "   -2.47778821e+16 -2.47778823e+16 -2.47778860e+16 -2.47778815e+16\n",
            "   -2.47778818e+16 -2.47778893e+16 -2.47778857e+16 -2.47778872e+16\n",
            "   -2.47778815e+16 -2.47778811e+16 -2.47779010e+16 -2.47778821e+16]]\n",
            "\n",
            " [[-2.11520240e+16 -2.11520261e+16 -2.11520239e+16 -2.11520234e+16\n",
            "   -2.11520268e+16 -2.11520245e+16 -2.11520237e+16 -2.11520239e+16\n",
            "   -2.11520240e+16 -2.11520243e+16 -2.11520273e+16 -2.11520237e+16\n",
            "   -2.11520239e+16 -2.11520304e+16 -2.11520269e+16 -2.11520289e+16\n",
            "   -2.11520237e+16 -2.11520236e+16 -2.11520400e+16 -2.11520244e+16]]\n",
            "\n",
            " [[-1.90672087e+16 -1.90672104e+16 -1.90672088e+16 -1.90672084e+16\n",
            "   -1.90672114e+16 -1.90672092e+16 -1.90672088e+16 -1.90672085e+16\n",
            "   -1.90672088e+16 -1.90672093e+16 -1.90672116e+16 -1.90672085e+16\n",
            "   -1.90672089e+16 -1.90672145e+16 -1.90672114e+16 -1.90672131e+16\n",
            "   -1.90672083e+16 -1.90672084e+16 -1.90672237e+16 -1.90672091e+16]]\n",
            "\n",
            " [[-3.96584279e+16 -3.96584324e+16 -3.96584290e+16 -3.96584276e+16\n",
            "   -3.96584348e+16 -3.96584295e+16 -3.96584286e+16 -3.96584296e+16\n",
            "   -3.96584291e+16 -3.96584297e+16 -3.96584358e+16 -3.96584280e+16\n",
            "   -3.96584279e+16 -3.96584406e+16 -3.96584349e+16 -3.96584380e+16\n",
            "   -3.96584286e+16 -3.96584273e+16 -3.96584597e+16 -3.96584288e+16]]\n",
            "\n",
            " [[-1.44957565e+16 -1.44957578e+16 -1.44957565e+16 -1.44957563e+16\n",
            "   -1.44957586e+16 -1.44957569e+16 -1.44957565e+16 -1.44957564e+16\n",
            "   -1.44957567e+16 -1.44957568e+16 -1.44957587e+16 -1.44957565e+16\n",
            "   -1.44957567e+16 -1.44957610e+16 -1.44957585e+16 -1.44957598e+16\n",
            "   -1.44957565e+16 -1.44957564e+16 -1.44957680e+16 -1.44957569e+16]]\n",
            "\n",
            " [[-3.19420875e+16 -3.19420900e+16 -3.19420879e+16 -3.19420864e+16\n",
            "   -3.19420927e+16 -3.19420884e+16 -3.19420872e+16 -3.19420870e+16\n",
            "   -3.19420874e+16 -3.19420880e+16 -3.19420935e+16 -3.19420865e+16\n",
            "   -3.19420876e+16 -3.19420969e+16 -3.19420923e+16 -3.19420953e+16\n",
            "   -3.19420875e+16 -3.19420858e+16 -3.19421119e+16 -3.19420885e+16]]\n",
            "\n",
            " [[-2.71144455e+16 -2.71144481e+16 -2.71144456e+16 -2.71144449e+16\n",
            "   -2.71144494e+16 -2.71144462e+16 -2.71144454e+16 -2.71144452e+16\n",
            "   -2.71144457e+16 -2.71144461e+16 -2.71144497e+16 -2.71144452e+16\n",
            "   -2.71144455e+16 -2.71144537e+16 -2.71144494e+16 -2.71144518e+16\n",
            "   -2.71144451e+16 -2.71144450e+16 -2.71144663e+16 -2.71144461e+16]]\n",
            "\n",
            " [[-2.80841085e+16 -2.80841113e+16 -2.80841088e+16 -2.80841078e+16\n",
            "   -2.80841129e+16 -2.80841093e+16 -2.80841083e+16 -2.80841083e+16\n",
            "   -2.80841092e+16 -2.80841093e+16 -2.80841126e+16 -2.80841083e+16\n",
            "   -2.80841084e+16 -2.80841169e+16 -2.80841128e+16 -2.80841153e+16\n",
            "   -2.80841077e+16 -2.80841081e+16 -2.80841306e+16 -2.80841088e+16]]\n",
            "\n",
            " [[-2.76221660e+16 -2.76221685e+16 -2.76221667e+16 -2.76221655e+16\n",
            "   -2.76221709e+16 -2.76221672e+16 -2.76221661e+16 -2.76221660e+16\n",
            "   -2.76221669e+16 -2.76221670e+16 -2.76221708e+16 -2.76221660e+16\n",
            "   -2.76221663e+16 -2.76221743e+16 -2.76221706e+16 -2.76221728e+16\n",
            "   -2.76221656e+16 -2.76221657e+16 -2.76221879e+16 -2.76221665e+16]]\n",
            "\n",
            " [[-2.44975977e+16 -2.44976003e+16 -2.44975980e+16 -2.44975970e+16\n",
            "   -2.44976015e+16 -2.44975987e+16 -2.44975977e+16 -2.44975979e+16\n",
            "   -2.44975978e+16 -2.44975988e+16 -2.44976022e+16 -2.44975973e+16\n",
            "   -2.44975974e+16 -2.44976050e+16 -2.44976017e+16 -2.44976038e+16\n",
            "   -2.44975975e+16 -2.44975971e+16 -2.44976163e+16 -2.44975982e+16]]]\n",
            "---------------------------------\n",
            "True value : [1 2 2 8 5 6 7 3 8 6 5 6 1 2 4 4 9 0 0 1 2 5 6 8 1 1 2 9 4 3 3 8 2]\n",
            "Prediction : [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "Batch: 60\n",
            "y_true: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "y_pred: [[[-2.90896262e+118  1.60989291e+117  1.35243204e+117 -5.93245960e+116\n",
            "    4.46043509e+117  3.31198887e+117  6.75924323e+117  5.40290644e+117\n",
            "    2.73179516e+117  4.05417840e+117]]\n",
            "\n",
            " [[-6.35862435e+118  3.51902227e+117  2.95624537e+117 -1.29676063e+117\n",
            "    9.74994694e+117  7.23958876e+117  1.47748508e+118  1.18100701e+118\n",
            "    5.97135870e+117  8.86192119e+117]]\n",
            "\n",
            " [[-7.32353984e+118  4.05303072e+117  3.40485293e+117 -1.49354288e+117\n",
            "    1.12294926e+118  8.33818980e+117  1.70169210e+118  1.36022376e+118\n",
            "    6.87750698e+117  1.02067097e+118]]\n",
            "\n",
            " [[-1.92915451e+118  1.06764251e+117  8.96900616e+116 -3.93426547e+116\n",
            "    2.95805399e+117  2.19643189e+117  4.48256862e+117  3.58307847e+117\n",
            "    1.81166129e+117  2.68863425e+117]]\n",
            "\n",
            " [[-2.57367525e+118  1.42433646e+117  1.19655056e+117 -5.24868361e+116\n",
            "    3.94632482e+117  2.93024865e+117  5.98017206e+117  4.78016682e+117\n",
            "    2.41692813e+117  3.58689332e+117]]\n",
            "\n",
            " [[-2.34151182e+118  1.29585139e+117  1.08861337e+117 -4.77521579e+116\n",
            "    3.59033884e+117  2.66591981e+117  5.44071889e+117  4.34896248e+117\n",
            "    2.19890438e+117  3.26333056e+117]]\n",
            "\n",
            " [[-2.18570460e+118  1.20962377e+117  1.01617563e+117 -4.45746678e+116\n",
            "    3.35143307e+117  2.48852607e+117  5.07868645e+117  4.05957691e+117\n",
            "    2.05258645e+117  3.04618434e+117]]\n",
            "\n",
            " [[-4.99612631e+118  2.76498167e+117  2.32279413e+117 -1.01889647e+117\n",
            "    7.66077123e+117  5.68832154e+117  1.16089608e+118  9.27946027e+117\n",
            "    4.69184224e+117  6.96302772e+117]]\n",
            "\n",
            " [[-2.71206123e+118  1.50092274e+117  1.26088884e+117 -5.53090423e+116\n",
            "    4.15851788e+117  3.08780750e+117  6.30172467e+117  5.03719539e+117\n",
            "    2.54688586e+117  3.77975983e+117]]\n",
            "\n",
            " [[-2.76147621e+118  1.52827023e+117  1.28386280e+117 -5.63167981e+116\n",
            "    4.23428796e+117  3.14406875e+117  6.41654494e+117  5.12897537e+117\n",
            "    2.59329127e+117  3.84862876e+117]]\n",
            "\n",
            " [[-3.76363513e+118  2.08289012e+117  1.74978554e+117 -7.67545558e+116\n",
            "    5.77094051e+117  4.28507317e+117  8.74515373e+117  6.99031620e+117\n",
            "    3.53441471e+117  5.24532291e+117]]\n",
            "\n",
            " [[-3.97922597e+118  2.20220350e+117  1.85001782e+117 -8.11512570e+116\n",
            "    6.10151504e+117  4.53053334e+117  9.24609895e+117  7.39073975e+117\n",
            "    3.73687520e+117  5.54578869e+117]]\n",
            "\n",
            " [[-2.54252903e+118  1.40709936e+117  1.18207009e+117 -5.18516485e+116\n",
            "    3.89856701e+117  2.89478723e+117  5.90780094e+117  4.72231797e+117\n",
            "    2.38767884e+117  3.54348529e+117]]\n",
            "\n",
            " [[-7.07721941e+118  3.91671081e+117  3.29033388e+117 -1.44330896e+117\n",
            "    1.08517991e+118  8.05774256e+117  1.64445727e+118  1.31447390e+118\n",
            "    6.64618845e+117  9.86341655e+117]]\n",
            "\n",
            " [[-3.08854196e+118  1.70927662e+117  1.43592189e+117 -6.29868885e+116\n",
            "    4.73579168e+117  3.51644828e+117  7.17651242e+117  5.73644473e+117\n",
            "    2.90043741e+117  4.30445549e+117]]\n",
            "\n",
            " [[-2.35052921e+118  1.30084184e+117  1.09280573e+117 -4.79360562e+116\n",
            "    3.60416559e+117  2.67618653e+117  5.46167164e+117  4.36571076e+117\n",
            "    2.20737258e+117  3.27589797e+117]]\n",
            "\n",
            " [[-4.66178274e+118  2.57994755e+117  2.16735144e+117 -9.50711349e+116\n",
            "    7.14810813e+117  5.30765588e+117  1.08320826e+118  8.65847359e+117\n",
            "    4.37786153e+117  6.49705801e+117]]\n",
            "\n",
            " [[-4.34335430e+118  2.40372126e+117  2.01930800e+117 -8.85771915e+116\n",
            "    6.65984836e+117  4.94511033e+117  1.00921847e+118  8.06704657e+117\n",
            "    4.07882665e+117  6.05326897e+117]]\n",
            "\n",
            " [[-2.94207165e+118  1.62821628e+117  1.36782506e+117 -5.99998126e+116\n",
            "    4.51120257e+117  3.34968504e+117  6.83617512e+117  5.46440088e+117\n",
            "    2.76288772e+117  4.10032197e+117]]\n",
            "\n",
            " [[-1.89165414e+118  1.04688887e+117  8.79465979e+116 -3.85778822e+116\n",
            "    2.90055308e+117  2.15373598e+117  4.39543304e+117  3.51342786e+117\n",
            "    1.77644484e+117  2.63637053e+117]]\n",
            "\n",
            " [[-2.52109022e+118  1.39523459e+117  1.17210278e+117 -5.14144312e+116\n",
            "    3.86569398e+117  2.87037815e+117  5.85798589e+117  4.68249901e+117\n",
            "    2.36754574e+117  3.51360634e+117]]\n",
            "\n",
            " [[-1.89082805e+118  1.04643169e+117  8.79081915e+116 -3.85610352e+116\n",
            "    2.89928641e+117  2.15279544e+117  4.39351355e+117  3.51189354e+117\n",
            "    1.77566906e+117  2.63521923e+117]]\n",
            "\n",
            " [[-4.29837792e+118  2.37883020e+117  1.99839763e+117 -8.76599555e+116\n",
            "    6.59088419e+117  4.89390264e+117  9.98767798e+117  7.98351056e+117\n",
            "    4.03658952e+117  5.99058606e+117]]\n",
            "\n",
            " [[-2.77344256e+118  1.53489271e+117  1.28942619e+117 -5.65608366e+116\n",
            "    4.25263648e+117  3.15769300e+117  6.44434987e+117  5.15120084e+117\n",
            "    2.60452882e+117  3.86530609e+117]]\n",
            "\n",
            " [[-4.43723744e+118  2.45567854e+117  2.06295606e+117 -9.04918189e+116\n",
            "    6.80380335e+117  5.05200064e+117  1.03103309e+118  8.24141864e+117\n",
            "    4.16699194e+117  6.18411253e+117]]\n",
            "\n",
            " [[-1.03948863e+118  5.75279089e+116  4.83278029e+116 -2.11990496e+116\n",
            "    1.59389176e+117  1.18350602e+117  2.41534780e+117  1.93067445e+117\n",
            "    9.76179612e+116  1.44872000e+117]]\n",
            "\n",
            " [[-1.07789252e+118  5.96532766e+116  5.01132727e+116 -2.19822481e+116\n",
            "    1.65277806e+117  1.22723063e+117  2.50458278e+117  2.00200318e+117\n",
            "    1.01224455e+117  1.50224294e+117]]\n",
            "\n",
            " [[-3.33409277e+118  1.84517060e+117  1.55008313e+117 -6.79945852e+116\n",
            "    5.11230509e+117  3.79601927e+117  7.74707239e+117  6.19251386e+117\n",
            "    3.13103319e+117  4.64667604e+117]]\n",
            "\n",
            " [[-4.09224706e+118  2.26475221e+117  1.90256347e+117 -8.34561784e+116\n",
            "    6.27481504e+117  4.65921309e+117  9.50871388e+117  7.60065731e+117\n",
            "    3.84301285e+117  5.70330451e+117]]\n",
            "\n",
            " [[-5.09526912e+118  2.81984978e+117  2.36888750e+117 -1.03911539e+117\n",
            "    7.81279107e+117  5.80120023e+117  1.18393282e+118  9.46360127e+117\n",
            "    4.78494685e+117  7.10120159e+117]]\n",
            "\n",
            " [[-3.47329339e+118  1.92220772e+117  1.61480014e+117 -7.08334049e+116\n",
            "    5.32574727e+117  3.95450563e+117  8.07051788e+117  6.45105548e+117\n",
            "    3.26175593e+117  4.84067789e+117]]\n",
            "\n",
            " [[-6.37367381e+118  3.52735102e+117  2.96324216e+117 -1.29982978e+117\n",
            "    9.77302292e+117  7.25672327e+117  1.48098196e+118  1.18380219e+118\n",
            "    5.98549159e+117  8.88289541e+117]]\n",
            "\n",
            " [[-2.11229646e+118  1.16899786e+117  9.82046791e+116 -4.30776020e+116\n",
            "    3.23887327e+117  2.40494750e+117  4.90811585e+117  3.92323369e+117\n",
            "    1.98364916e+117  2.94387650e+117]]]\n",
            "loss: [41.44653167 41.44653167  0.         41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            "  0.         41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167]\n",
            "loss grad: [[[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  0.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[-1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  0.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18 -1.e+00  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18 -1.e+00]]\n",
            "\n",
            " [[-1.e+00  1.e-18  1.e-18  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18 -1.e+00  1.e-18  1.e-18  1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]]\n",
            "linear2 grad input: [[[-2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55]]\n",
            "\n",
            " [[-9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55]]\n",
            "\n",
            " [[ 1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38]]\n",
            "\n",
            " [[-9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[-8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55]]\n",
            "\n",
            " [[-6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56]]\n",
            "\n",
            " [[-2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[-6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55]]\n",
            "\n",
            " [[-4.00729755e+55 -4.00729755e+55 -4.00729755e+55 -4.00729755e+55\n",
            "   -4.00729755e+55 -4.00729755e+55 -4.00729755e+55 -4.00729755e+55\n",
            "   -4.00729755e+55 -4.00729755e+55 -4.00729755e+55 -4.00729755e+55\n",
            "   -4.00729755e+55 -4.00729755e+55 -4.00729755e+55 -4.00729755e+55\n",
            "   -4.00729755e+55 -4.00729755e+55 -4.00729755e+55 -4.00729755e+55]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[ 1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38\n",
            "    1.17827575e+38  1.17827575e+38  1.17827575e+38  1.17827575e+38]]\n",
            "\n",
            " [[-6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55]]\n",
            "\n",
            " [[-8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55]]\n",
            "\n",
            " [[-2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55\n",
            "   -2.36437526e+55 -2.36437526e+55 -2.36437526e+55 -2.36437526e+55]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[-8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55\n",
            "   -8.97638155e+55 -8.97638155e+55 -8.97638155e+55 -8.97638155e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]\n",
            "\n",
            " [[-7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55\n",
            "   -7.02067411e+55 -7.02067411e+55 -7.02067411e+55 -7.02067411e+55]]\n",
            "\n",
            " [[-6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55\n",
            "   -6.00927660e+55 -6.00927660e+55 -6.00927660e+55 -6.00927660e+55]]\n",
            "\n",
            " [[-4.71548691e+55 -4.71548691e+55 -4.71548691e+55 -4.71548691e+55\n",
            "   -4.71548691e+55 -4.71548691e+55 -4.71548691e+55 -4.71548691e+55\n",
            "   -4.71548691e+55 -4.71548691e+55 -4.71548691e+55 -4.71548691e+55\n",
            "   -4.71548691e+55 -4.71548691e+55 -4.71548691e+55 -4.71548691e+55\n",
            "   -4.71548691e+55 -4.71548691e+55 -4.71548691e+55 -4.71548691e+55]]\n",
            "\n",
            " [[-6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56\n",
            "   -6.24919863e+56 -6.24919863e+56 -6.24919863e+56 -6.24919863e+56]]\n",
            "\n",
            " [[-9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55\n",
            "   -9.42518904e+55 -9.42518904e+55 -9.42518904e+55 -9.42518904e+55]]\n",
            "\n",
            " [[-1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56\n",
            "   -1.28169078e+56 -1.28169078e+56 -1.28169078e+56 -1.28169078e+56]]]\n",
            "linear2 input: [[[-2.86827732e+60 -2.86827732e+60 -2.86827732e+60 -2.86827732e+60\n",
            "   -2.86827732e+60 -2.86827732e+60 -2.86827732e+60 -2.86827732e+60\n",
            "   -2.86827732e+60 -2.86827732e+60 -2.86827732e+60 -2.86827732e+60\n",
            "   -2.86827732e+60 -2.86827732e+60 -2.86827732e+60 -2.86827732e+60\n",
            "   -2.86827732e+60 -2.86827732e+60 -2.86827732e+60 -2.86827732e+60]]\n",
            "\n",
            " [[-6.26969143e+60 -6.26969143e+60 -6.26969143e+60 -6.26969143e+60\n",
            "   -6.26969143e+60 -6.26969143e+60 -6.26969143e+60 -6.26969143e+60\n",
            "   -6.26969143e+60 -6.26969143e+60 -6.26969143e+60 -6.26969143e+60\n",
            "   -6.26969143e+60 -6.26969143e+60 -6.26969143e+60 -6.26969143e+60\n",
            "   -6.26969143e+60 -6.26969143e+60 -6.26969143e+60 -6.26969143e+60]]\n",
            "\n",
            " [[-7.22111143e+60 -7.22111143e+60 -7.22111143e+60 -7.22111143e+60\n",
            "   -7.22111143e+60 -7.22111143e+60 -7.22111143e+60 -7.22111143e+60\n",
            "   -7.22111143e+60 -7.22111143e+60 -7.22111143e+60 -7.22111143e+60\n",
            "   -7.22111143e+60 -7.22111143e+60 -7.22111143e+60 -7.22111143e+60\n",
            "   -7.22111143e+60 -7.22111143e+60 -7.22111143e+60 -7.22111143e+60]]\n",
            "\n",
            " [[-1.90217299e+60 -1.90217299e+60 -1.90217299e+60 -1.90217299e+60\n",
            "   -1.90217299e+60 -1.90217299e+60 -1.90217299e+60 -1.90217299e+60\n",
            "   -1.90217299e+60 -1.90217299e+60 -1.90217299e+60 -1.90217299e+60\n",
            "   -1.90217299e+60 -1.90217299e+60 -1.90217299e+60 -1.90217299e+60\n",
            "   -1.90217299e+60 -1.90217299e+60 -1.90217299e+60 -1.90217299e+60]]\n",
            "\n",
            " [[-2.53767934e+60 -2.53767934e+60 -2.53767934e+60 -2.53767934e+60\n",
            "   -2.53767934e+60 -2.53767934e+60 -2.53767934e+60 -2.53767934e+60\n",
            "   -2.53767934e+60 -2.53767934e+60 -2.53767934e+60 -2.53767934e+60\n",
            "   -2.53767934e+60 -2.53767934e+60 -2.53767934e+60 -2.53767934e+60\n",
            "   -2.53767934e+60 -2.53767934e+60 -2.53767934e+60 -2.53767934e+60]]\n",
            "\n",
            " [[-2.30876299e+60 -2.30876299e+60 -2.30876299e+60 -2.30876299e+60\n",
            "   -2.30876299e+60 -2.30876299e+60 -2.30876299e+60 -2.30876299e+60\n",
            "   -2.30876299e+60 -2.30876299e+60 -2.30876299e+60 -2.30876299e+60\n",
            "   -2.30876299e+60 -2.30876299e+60 -2.30876299e+60 -2.30876299e+60\n",
            "   -2.30876299e+60 -2.30876299e+60 -2.30876299e+60 -2.30876299e+60]]\n",
            "\n",
            " [[-2.15513493e+60 -2.15513493e+60 -2.15513493e+60 -2.15513493e+60\n",
            "   -2.15513493e+60 -2.15513493e+60 -2.15513493e+60 -2.15513493e+60\n",
            "   -2.15513493e+60 -2.15513493e+60 -2.15513493e+60 -2.15513493e+60\n",
            "   -2.15513493e+60 -2.15513493e+60 -2.15513493e+60 -2.15513493e+60\n",
            "   -2.15513493e+60 -2.15513493e+60 -2.15513493e+60 -2.15513493e+60]]\n",
            "\n",
            " [[-4.92624955e+60 -4.92624955e+60 -4.92624955e+60 -4.92624955e+60\n",
            "   -4.92624955e+60 -4.92624955e+60 -4.92624955e+60 -4.92624955e+60\n",
            "   -4.92624955e+60 -4.92624955e+60 -4.92624955e+60 -4.92624955e+60\n",
            "   -4.92624955e+60 -4.92624955e+60 -4.92624955e+60 -4.92624955e+60\n",
            "   -4.92624955e+60 -4.92624955e+60 -4.92624955e+60 -4.92624955e+60]]\n",
            "\n",
            " [[-2.67412983e+60 -2.67412983e+60 -2.67412983e+60 -2.67412983e+60\n",
            "   -2.67412983e+60 -2.67412983e+60 -2.67412983e+60 -2.67412983e+60\n",
            "   -2.67412983e+60 -2.67412983e+60 -2.67412983e+60 -2.67412983e+60\n",
            "   -2.67412983e+60 -2.67412983e+60 -2.67412983e+60 -2.67412983e+60\n",
            "   -2.67412983e+60 -2.67412983e+60 -2.67412983e+60 -2.67412983e+60]]\n",
            "\n",
            " [[-2.72285369e+60 -2.72285369e+60 -2.72285369e+60 -2.72285369e+60\n",
            "   -2.72285369e+60 -2.72285369e+60 -2.72285369e+60 -2.72285369e+60\n",
            "   -2.72285369e+60 -2.72285369e+60 -2.72285369e+60 -2.72285369e+60\n",
            "   -2.72285369e+60 -2.72285369e+60 -2.72285369e+60 -2.72285369e+60\n",
            "   -2.72285369e+60 -2.72285369e+60 -2.72285369e+60 -2.72285369e+60]]\n",
            "\n",
            " [[-3.71099623e+60 -3.71099623e+60 -3.71099623e+60 -3.71099623e+60\n",
            "   -3.71099623e+60 -3.71099623e+60 -3.71099623e+60 -3.71099623e+60\n",
            "   -3.71099623e+60 -3.71099623e+60 -3.71099623e+60 -3.71099623e+60\n",
            "   -3.71099623e+60 -3.71099623e+60 -3.71099623e+60 -3.71099623e+60\n",
            "   -3.71099623e+60 -3.71099623e+60 -3.71099623e+60 -3.71099623e+60]]\n",
            "\n",
            " [[-3.92357177e+60 -3.92357177e+60 -3.92357177e+60 -3.92357177e+60\n",
            "   -3.92357177e+60 -3.92357177e+60 -3.92357177e+60 -3.92357177e+60\n",
            "   -3.92357177e+60 -3.92357177e+60 -3.92357177e+60 -3.92357177e+60\n",
            "   -3.92357177e+60 -3.92357177e+60 -3.92357177e+60 -3.92357177e+60\n",
            "   -3.92357177e+60 -3.92357177e+60 -3.92357177e+60 -3.92357177e+60]]\n",
            "\n",
            " [[-2.50696874e+60 -2.50696874e+60 -2.50696874e+60 -2.50696874e+60\n",
            "   -2.50696874e+60 -2.50696874e+60 -2.50696874e+60 -2.50696874e+60\n",
            "   -2.50696874e+60 -2.50696874e+60 -2.50696874e+60 -2.50696874e+60\n",
            "   -2.50696874e+60 -2.50696874e+60 -2.50696874e+60 -2.50696874e+60\n",
            "   -2.50696874e+60 -2.50696874e+60 -2.50696874e+60 -2.50696874e+60]]\n",
            "\n",
            " [[-6.97823609e+60 -6.97823609e+60 -6.97823609e+60 -6.97823609e+60\n",
            "   -6.97823609e+60 -6.97823609e+60 -6.97823609e+60 -6.97823609e+60\n",
            "   -6.97823609e+60 -6.97823609e+60 -6.97823609e+60 -6.97823609e+60\n",
            "   -6.97823609e+60 -6.97823609e+60 -6.97823609e+60 -6.97823609e+60\n",
            "   -6.97823609e+60 -6.97823609e+60 -6.97823609e+60 -6.97823609e+60]]\n",
            "\n",
            " [[-3.04534503e+60 -3.04534503e+60 -3.04534503e+60 -3.04534503e+60\n",
            "   -3.04534503e+60 -3.04534503e+60 -3.04534503e+60 -3.04534503e+60\n",
            "   -3.04534503e+60 -3.04534503e+60 -3.04534503e+60 -3.04534503e+60\n",
            "   -3.04534503e+60 -3.04534503e+60 -3.04534503e+60 -3.04534503e+60\n",
            "   -3.04534503e+60 -3.04534503e+60 -3.04534503e+60 -3.04534503e+60]]\n",
            "\n",
            " [[-2.31765426e+60 -2.31765426e+60 -2.31765426e+60 -2.31765426e+60\n",
            "   -2.31765426e+60 -2.31765426e+60 -2.31765426e+60 -2.31765426e+60\n",
            "   -2.31765426e+60 -2.31765426e+60 -2.31765426e+60 -2.31765426e+60\n",
            "   -2.31765426e+60 -2.31765426e+60 -2.31765426e+60 -2.31765426e+60\n",
            "   -2.31765426e+60 -2.31765426e+60 -2.31765426e+60 -2.31765426e+60]]\n",
            "\n",
            " [[-4.59658217e+60 -4.59658217e+60 -4.59658217e+60 -4.59658217e+60\n",
            "   -4.59658217e+60 -4.59658217e+60 -4.59658217e+60 -4.59658217e+60\n",
            "   -4.59658217e+60 -4.59658217e+60 -4.59658217e+60 -4.59658217e+60\n",
            "   -4.59658217e+60 -4.59658217e+60 -4.59658217e+60 -4.59658217e+60\n",
            "   -4.59658217e+60 -4.59658217e+60 -4.59658217e+60 -4.59658217e+60]]\n",
            "\n",
            " [[-4.28260733e+60 -4.28260733e+60 -4.28260733e+60 -4.28260733e+60\n",
            "   -4.28260733e+60 -4.28260733e+60 -4.28260733e+60 -4.28260733e+60\n",
            "   -4.28260733e+60 -4.28260733e+60 -4.28260733e+60 -4.28260733e+60\n",
            "   -4.28260733e+60 -4.28260733e+60 -4.28260733e+60 -4.28260733e+60\n",
            "   -4.28260733e+60 -4.28260733e+60 -4.28260733e+60 -4.28260733e+60]]\n",
            "\n",
            " [[-2.90092328e+60 -2.90092328e+60 -2.90092328e+60 -2.90092328e+60\n",
            "   -2.90092328e+60 -2.90092328e+60 -2.90092328e+60 -2.90092328e+60\n",
            "   -2.90092328e+60 -2.90092328e+60 -2.90092328e+60 -2.90092328e+60\n",
            "   -2.90092328e+60 -2.90092328e+60 -2.90092328e+60 -2.90092328e+60\n",
            "   -2.90092328e+60 -2.90092328e+60 -2.90092328e+60 -2.90092328e+60]]\n",
            "\n",
            " [[-1.86519711e+60 -1.86519711e+60 -1.86519711e+60 -1.86519711e+60\n",
            "   -1.86519711e+60 -1.86519711e+60 -1.86519711e+60 -1.86519711e+60\n",
            "   -1.86519711e+60 -1.86519711e+60 -1.86519711e+60 -1.86519711e+60\n",
            "   -1.86519711e+60 -1.86519711e+60 -1.86519711e+60 -1.86519711e+60\n",
            "   -1.86519711e+60 -1.86519711e+60 -1.86519711e+60 -1.86519711e+60]]\n",
            "\n",
            " [[-2.48582977e+60 -2.48582977e+60 -2.48582977e+60 -2.48582977e+60\n",
            "   -2.48582977e+60 -2.48582977e+60 -2.48582977e+60 -2.48582977e+60\n",
            "   -2.48582977e+60 -2.48582977e+60 -2.48582977e+60 -2.48582977e+60\n",
            "   -2.48582977e+60 -2.48582977e+60 -2.48582977e+60 -2.48582977e+60\n",
            "   -2.48582977e+60 -2.48582977e+60 -2.48582977e+60 -2.48582977e+60]]\n",
            "\n",
            " [[-1.86438257e+60 -1.86438257e+60 -1.86438257e+60 -1.86438257e+60\n",
            "   -1.86438257e+60 -1.86438257e+60 -1.86438257e+60 -1.86438257e+60\n",
            "   -1.86438257e+60 -1.86438257e+60 -1.86438257e+60 -1.86438257e+60\n",
            "   -1.86438257e+60 -1.86438257e+60 -1.86438257e+60 -1.86438257e+60\n",
            "   -1.86438257e+60 -1.86438257e+60 -1.86438257e+60 -1.86438257e+60]]\n",
            "\n",
            " [[-4.23826000e+60 -4.23826000e+60 -4.23826000e+60 -4.23826000e+60\n",
            "   -4.23826000e+60 -4.23826000e+60 -4.23826000e+60 -4.23826000e+60\n",
            "   -4.23826000e+60 -4.23826000e+60 -4.23826000e+60 -4.23826000e+60\n",
            "   -4.23826000e+60 -4.23826000e+60 -4.23826000e+60 -4.23826000e+60\n",
            "   -4.23826000e+60 -4.23826000e+60 -4.23826000e+60 -4.23826000e+60]]\n",
            "\n",
            " [[-2.73465267e+60 -2.73465267e+60 -2.73465267e+60 -2.73465267e+60\n",
            "   -2.73465267e+60 -2.73465267e+60 -2.73465267e+60 -2.73465267e+60\n",
            "   -2.73465267e+60 -2.73465267e+60 -2.73465267e+60 -2.73465267e+60\n",
            "   -2.73465267e+60 -2.73465267e+60 -2.73465267e+60 -2.73465267e+60\n",
            "   -2.73465267e+60 -2.73465267e+60 -2.73465267e+60 -2.73465267e+60]]\n",
            "\n",
            " [[-4.37517740e+60 -4.37517740e+60 -4.37517740e+60 -4.37517740e+60\n",
            "   -4.37517740e+60 -4.37517740e+60 -4.37517740e+60 -4.37517740e+60\n",
            "   -4.37517740e+60 -4.37517740e+60 -4.37517740e+60 -4.37517740e+60\n",
            "   -4.37517740e+60 -4.37517740e+60 -4.37517740e+60 -4.37517740e+60\n",
            "   -4.37517740e+60 -4.37517740e+60 -4.37517740e+60 -4.37517740e+60]]\n",
            "\n",
            " [[-1.02495014e+60 -1.02495014e+60 -1.02495014e+60 -1.02495014e+60\n",
            "   -1.02495014e+60 -1.02495014e+60 -1.02495014e+60 -1.02495014e+60\n",
            "   -1.02495014e+60 -1.02495014e+60 -1.02495014e+60 -1.02495014e+60\n",
            "   -1.02495014e+60 -1.02495014e+60 -1.02495014e+60 -1.02495014e+60\n",
            "   -1.02495014e+60 -1.02495014e+60 -1.02495014e+60 -1.02495014e+60]]\n",
            "\n",
            " [[-1.06281691e+60 -1.06281691e+60 -1.06281691e+60 -1.06281691e+60\n",
            "   -1.06281691e+60 -1.06281691e+60 -1.06281691e+60 -1.06281691e+60\n",
            "   -1.06281691e+60 -1.06281691e+60 -1.06281691e+60 -1.06281691e+60\n",
            "   -1.06281691e+60 -1.06281691e+60 -1.06281691e+60 -1.06281691e+60\n",
            "   -1.06281691e+60 -1.06281691e+60 -1.06281691e+60 -1.06281691e+60]]\n",
            "\n",
            " [[-3.28746152e+60 -3.28746152e+60 -3.28746152e+60 -3.28746152e+60\n",
            "   -3.28746152e+60 -3.28746152e+60 -3.28746152e+60 -3.28746152e+60\n",
            "   -3.28746152e+60 -3.28746152e+60 -3.28746152e+60 -3.28746152e+60\n",
            "   -3.28746152e+60 -3.28746152e+60 -3.28746152e+60 -3.28746152e+60\n",
            "   -3.28746152e+60 -3.28746152e+60 -3.28746152e+60 -3.28746152e+60]]\n",
            "\n",
            " [[-4.03501212e+60 -4.03501212e+60 -4.03501212e+60 -4.03501212e+60\n",
            "   -4.03501212e+60 -4.03501212e+60 -4.03501212e+60 -4.03501212e+60\n",
            "   -4.03501212e+60 -4.03501212e+60 -4.03501212e+60 -4.03501212e+60\n",
            "   -4.03501212e+60 -4.03501212e+60 -4.03501212e+60 -4.03501212e+60\n",
            "   -4.03501212e+60 -4.03501212e+60 -4.03501212e+60 -4.03501212e+60]]\n",
            "\n",
            " [[-5.02400572e+60 -5.02400572e+60 -5.02400572e+60 -5.02400572e+60\n",
            "   -5.02400572e+60 -5.02400572e+60 -5.02400572e+60 -5.02400572e+60\n",
            "   -5.02400572e+60 -5.02400572e+60 -5.02400572e+60 -5.02400572e+60\n",
            "   -5.02400572e+60 -5.02400572e+60 -5.02400572e+60 -5.02400572e+60\n",
            "   -5.02400572e+60 -5.02400572e+60 -5.02400572e+60 -5.02400572e+60]]\n",
            "\n",
            " [[-3.42471525e+60 -3.42471525e+60 -3.42471525e+60 -3.42471525e+60\n",
            "   -3.42471525e+60 -3.42471525e+60 -3.42471525e+60 -3.42471525e+60\n",
            "   -3.42471525e+60 -3.42471525e+60 -3.42471525e+60 -3.42471525e+60\n",
            "   -3.42471525e+60 -3.42471525e+60 -3.42471525e+60 -3.42471525e+60\n",
            "   -3.42471525e+60 -3.42471525e+60 -3.42471525e+60 -3.42471525e+60]]\n",
            "\n",
            " [[-6.28453041e+60 -6.28453041e+60 -6.28453041e+60 -6.28453041e+60\n",
            "   -6.28453041e+60 -6.28453041e+60 -6.28453041e+60 -6.28453041e+60\n",
            "   -6.28453041e+60 -6.28453041e+60 -6.28453041e+60 -6.28453041e+60\n",
            "   -6.28453041e+60 -6.28453041e+60 -6.28453041e+60 -6.28453041e+60\n",
            "   -6.28453041e+60 -6.28453041e+60 -6.28453041e+60 -6.28453041e+60]]\n",
            "\n",
            " [[-2.08275348e+60 -2.08275348e+60 -2.08275348e+60 -2.08275348e+60\n",
            "   -2.08275348e+60 -2.08275348e+60 -2.08275348e+60 -2.08275348e+60\n",
            "   -2.08275348e+60 -2.08275348e+60 -2.08275348e+60 -2.08275348e+60\n",
            "   -2.08275348e+60 -2.08275348e+60 -2.08275348e+60 -2.08275348e+60\n",
            "   -2.08275348e+60 -2.08275348e+60 -2.08275348e+60 -2.08275348e+60]]]\n",
            "---------------------------------\n",
            "True value : [7 2 6 2 3 7 3 8 1 0 7 8 3 3 8 5 4 8 6 5 1 3 2 7 8 1 3 8 5 9 0 2 3]\n",
            "Prediction : [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "Batch: 90\n",
            "y_true: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "y_pred: [[[ 5.08760783e+238  1.95392806e+238  7.31009753e+237  6.48998065e+238\n",
            "    5.48132539e+238  5.18203187e+238  5.12489383e+238  5.01705683e+238\n",
            "   -3.64819041e+239  1.41406987e+238]]\n",
            "\n",
            " [[ 1.53959681e+238  5.91291922e+237  2.21216007e+237  1.96397873e+238\n",
            "    1.65874246e+238  1.56817114e+238  1.55088019e+238  1.51824688e+238\n",
            "   -1.10400458e+239  4.27921636e+237]]\n",
            "\n",
            " [[ 2.88284552e+238  1.10717511e+238  4.14219858e+237  3.67748700e+238\n",
            "    3.10594191e+238  2.93635003e+238  2.90397329e+238  2.84286846e+238\n",
            "   -2.06721307e+239  8.01269502e+237]]\n",
            "\n",
            " [[ 3.39084161e+238  1.30227423e+238  4.87210958e+237  4.32550958e+238\n",
            "    3.65325057e+238  3.45377432e+238  3.41569237e+238  3.34382005e+238\n",
            "   -2.43148376e+239  9.42463945e+237]]\n",
            "\n",
            " [[ 4.57667897e+238  1.75770259e+238  6.57597259e+237  5.83821689e+238\n",
            "    4.93085699e+238  4.66162037e+238  4.61022048e+238  4.51321314e+238\n",
            "   -3.28181670e+239  1.27206028e+238]]\n",
            "\n",
            " [[ 4.24292335e+238  1.62952163e+238  6.09641792e+237  5.41246326e+238\n",
            "    4.57127285e+238  4.32167038e+238  4.27401884e+238  4.18408578e+238\n",
            "   -3.04248928e+239  1.17929492e+238]]\n",
            "\n",
            " [[ 2.36125866e+238  9.06856364e+237  3.39275975e+237  3.01212741e+238\n",
            "    2.54399071e+238  2.40508271e+238  2.37856382e+238  2.32851455e+238\n",
            "   -1.69319678e+239  6.56297583e+237]]\n",
            "\n",
            " [[ 2.75395897e+238  1.05767541e+238  3.95700874e+237  3.51307354e+238\n",
            "    2.96708114e+238  2.80507139e+238  2.77414215e+238  2.71576920e+238\n",
            "   -1.97479189e+239  7.65446263e+237]]\n",
            "\n",
            " [[ 8.26652266e+238  3.17481046e+238  1.18777015e+238  1.05451469e+239\n",
            "    8.90624870e+238  8.41994614e+238  8.32710626e+238  8.15188894e+238\n",
            "   -5.92770703e+239  2.29763005e+238]]\n",
            "\n",
            " [[ 5.51075062e+238  2.11643873e+238  7.91808760e+237  7.02976056e+238\n",
            "    5.93721417e+238  5.61302803e+238  5.55113774e+238  5.43433180e+238\n",
            "   -3.95161503e+239  1.53167985e+238]]\n",
            "\n",
            " [[ 3.04290652e+238  1.16864755e+238  4.37218122e+237  3.88166798e+238\n",
            "    3.27838963e+238  3.09938169e+238  3.06520734e+238  3.00070985e+238\n",
            "   -2.18198862e+239  8.45757489e+237]]\n",
            "\n",
            " [[ 3.80406040e+238  1.46097352e+238  5.46584042e+237  4.85263000e+238\n",
            "    4.09844735e+238  3.87466229e+238  3.83193955e+238  3.75130864e+238\n",
            "   -2.72779214e+239  1.05731561e+238]]\n",
            "\n",
            " [[ 3.46716985e+238  1.33158857e+238  4.98178134e+237  4.42287731e+238\n",
            "    3.73548567e+238  3.53151918e+238  3.49258000e+238  3.41908983e+238\n",
            "   -2.48621675e+239  9.63678919e+237]]\n",
            "\n",
            " [[ 3.25219965e+238  1.24902790e+238  4.67290275e+237  4.14865168e+238\n",
            "    3.50387945e+238  3.31255922e+238  3.27603433e+238  3.20710067e+238\n",
            "   -2.33206724e+239  9.03929250e+237]]\n",
            "\n",
            " [[ 2.64679602e+238  1.01651880e+238  3.80303233e+237  3.37637167e+238\n",
            "    2.85162511e+238  2.69591954e+238  2.66619383e+238  2.61009231e+238\n",
            "   -1.89794815e+239  7.35660969e+237]]\n",
            "\n",
            " [[ 4.44506131e+238  1.70715399e+238  6.38685857e+237  5.67031950e+238\n",
            "    4.78905376e+238  4.52755993e+238  4.47763822e+238  4.38342064e+238\n",
            "   -3.18743712e+239  1.23547794e+238]]\n",
            "\n",
            " [[ 1.03749506e+238  3.98456557e+237  1.49071829e+237  1.32347521e+238\n",
            "    1.11778427e+238  1.05675057e+238  1.04509864e+238  1.02310788e+238\n",
            "   -7.43960549e+238  2.88365485e+237]]\n",
            "\n",
            " [[ 2.95495915e+238  1.13487080e+238  4.24581459e+237  3.76947837e+238\n",
            "    3.18363623e+238  3.00980205e+238  2.97661542e+238  2.91398207e+238\n",
            "   -2.11892386e+239  8.21313048e+237]]\n",
            "\n",
            " [[ 3.42494204e+238  1.31537072e+238  4.92110658e+237  4.36900963e+238\n",
            "    3.68998996e+238  3.48850765e+238  3.45004272e+238  3.37744760e+238\n",
            "   -2.45593629e+239  9.51941956e+237]]\n",
            "\n",
            " [[ 3.49073771e+238  1.34063996e+238  5.01564467e+237  4.45294153e+238\n",
            "    3.76087739e+238  3.55552446e+238  3.51632059e+238  3.44233087e+238\n",
            "   -2.50311664e+239  9.70229464e+237]]\n",
            "\n",
            " [[ 3.08703148e+238  1.18559402e+238  4.43558190e+237  3.93795576e+238\n",
            "    3.32592932e+238  3.14432560e+238  3.10965568e+238  3.04422292e+238\n",
            "   -2.21362947e+239  8.58021756e+237]]\n",
            "\n",
            " [[ 3.86353241e+238  1.48381413e+238  5.55129242e+237  4.92849517e+238\n",
            "    4.16252177e+238  3.93523809e+238  3.89184743e+238  3.80995595e+238\n",
            "   -2.77043797e+239  1.07384550e+238]]\n",
            "\n",
            " [[ 3.17085575e+238  1.21778726e+238  4.55602427e+237  4.04488576e+238\n",
            "    3.41624054e+238  3.22970561e+238  3.19409428e+238  3.12688478e+238\n",
            "   -2.27373766e+239  8.81320204e+237]]\n",
            "\n",
            " [[ 2.47748215e+238  9.51492735e+237  3.55975475e+237  3.16038731e+238\n",
            "    2.66920845e+238  2.52346327e+238  2.49563909e+238  2.44312635e+238\n",
            "   -1.77653760e+239  6.88601201e+237]]\n",
            "\n",
            " [[ 6.39495855e+238  2.45602439e+238  9.18855625e+237  8.15769584e+238\n",
            "    6.88984880e+238  6.51364652e+238  6.44182585e+238  6.30627823e+238\n",
            "   -4.58565739e+239  1.77744011e+238]]\n",
            "\n",
            " [[ 5.59385944e+238  2.14835720e+238  8.03750200e+237  7.13577791e+238\n",
            "    6.02675459e+238  5.69767932e+238  5.63485565e+238  5.51628814e+238\n",
            "   -4.01121019e+239  1.55477945e+238]]\n",
            "\n",
            " [[ 2.97082188e+238  1.14096299e+238  4.26860687e+237  3.78971359e+238\n",
            "    3.20072654e+238  3.02595919e+238  2.99259441e+238  2.92962483e+238\n",
            "   -2.13029861e+239  8.25722000e+237]]\n",
            "\n",
            " [[ 1.56866571e+238  6.02456017e+237  2.25392753e+237  2.00106031e+238\n",
            "    1.69006093e+238  1.59777954e+238  1.58016213e+238  1.54691267e+238\n",
            "   -1.12484912e+239  4.36001161e+237]]\n",
            "\n",
            " [[ 1.79893417e+238  6.90892077e+237  2.58478732e+237  2.29480108e+238\n",
            "    1.93814929e+238  1.83232169e+238  1.81211817e+238  1.77398794e+238\n",
            "   -1.28996860e+239  5.00002886e+237]]\n",
            "\n",
            " [[ 3.82825794e+238  1.47026674e+238  5.50060851e+237  4.88349747e+238\n",
            "    4.12451749e+238  3.89930893e+238  3.85631443e+238  3.77517063e+238\n",
            "   -2.74514357e+239  1.06404117e+238]]\n",
            "\n",
            " [[ 2.37689688e+238  9.12862325e+237  3.41522944e+237  3.03207623e+238\n",
            "    2.56083914e+238  2.42101117e+238  2.39431666e+238  2.34393592e+238\n",
            "   -1.70441054e+239  6.60644133e+237]]\n",
            "\n",
            " [[ 2.51755715e+238  9.66883792e+237  3.61733627e+237  3.21150878e+238\n",
            "    2.71238476e+238  2.56428204e+238  2.53600779e+238  2.48264562e+238\n",
            "   -1.80527434e+239  6.99739805e+237]]\n",
            "\n",
            " [[ 4.48671016e+238  1.72314949e+238  6.44670146e+237  5.72344864e+238\n",
            "    4.83392571e+238  4.56998177e+238  4.51959230e+238  4.42449194e+238\n",
            "   -3.21730242e+239  1.24705399e+238]]]\n",
            "loss: [41.44653167  0.         41.44653167 41.44653167 41.44653167  0.\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167  0.\n",
            " 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167 41.44653167\n",
            " 41.44653167 41.44653167 41.44653167]\n",
            "loss grad: [[[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  0.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  0.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18 -1.e+00  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18 -1.e+00  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18 -1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  0.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "   -1.e+00  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18 -1.e+00]]\n",
            "\n",
            " [[ 1.e-18 -1.e+00  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18 -1.e+00]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18 -1.e+00  1.e-18\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00  1.e-18  1.e-18  1.e-18 -1.e+00\n",
            "    1.e-18  1.e-18]]\n",
            "\n",
            " [[ 1.e-18  1.e-18  1.e-18  1.e+00 -1.e+00  1.e-18  1.e-18  1.e-18\n",
            "    1.e-18  1.e-18]]]\n",
            "linear2 grad input: [[[-9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116]]\n",
            "\n",
            " [[ 1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098]]\n",
            "\n",
            " [[-3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[ 1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098]]\n",
            "\n",
            " [[-1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116]]\n",
            "\n",
            " [[-2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115]]\n",
            "\n",
            " [[-3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115]]\n",
            "\n",
            " [[-1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116\n",
            "   -1.31306647e+116 -1.31306647e+116 -1.31306647e+116 -1.31306647e+116]]\n",
            "\n",
            " [[-3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115]]\n",
            "\n",
            " [[-1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116]]\n",
            "\n",
            " [[-2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115]]\n",
            "\n",
            " [[-2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115]]\n",
            "\n",
            " [[-2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115]]\n",
            "\n",
            " [[-2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115\n",
            "   -2.98217115e+115 -2.98217115e+115 -2.98217115e+115 -2.98217115e+115]]\n",
            "\n",
            " [[-1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116]]\n",
            "\n",
            " [[-2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115]]\n",
            "\n",
            " [[ 1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098\n",
            "    1.47973937e+098  1.47973937e+098  1.47973937e+098  1.47973937e+098]]\n",
            "\n",
            " [[-9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116\n",
            "   -9.79774721e+116 -9.79774721e+116 -9.79774721e+116 -9.79774721e+116]]\n",
            "\n",
            " [[-3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115]]\n",
            "\n",
            " [[-1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116]]\n",
            "\n",
            " [[-1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116\n",
            "   -1.03423662e+116 -1.03423662e+116 -1.03423662e+116 -1.03423662e+116]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116\n",
            "   -1.15732626e+116 -1.15732626e+116 -1.15732626e+116 -1.15732626e+116]]\n",
            "\n",
            " [[-3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115\n",
            "   -3.11244798e+115 -3.11244798e+115 -3.11244798e+115 -3.11244798e+115]]\n",
            "\n",
            " [[-3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115\n",
            "   -3.35832029e+115 -3.35832029e+115 -3.35832029e+115 -3.35832029e+115]]\n",
            "\n",
            " [[-2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115\n",
            "   -2.29977095e+115 -2.29977095e+115 -2.29977095e+115 -2.29977095e+115]]]\n",
            "linear2 input: [[[-2.19294721e+121 -2.19294721e+121 -2.19294721e+121 -2.19294721e+121\n",
            "   -2.19294721e+121 -2.19294721e+121 -2.19294721e+121 -2.19294721e+121\n",
            "   -2.19294721e+121 -2.19294721e+121 -2.19294721e+121 -2.19294721e+121\n",
            "   -2.19294721e+121 -2.19294721e+121 -2.19294721e+121 -2.19294721e+121\n",
            "   -2.19294721e+121 -2.19294721e+121 -2.19294721e+121 -2.19294721e+121]]\n",
            "\n",
            " [[-6.63623189e+120 -6.63623189e+120 -6.63623189e+120 -6.63623189e+120\n",
            "   -6.63623189e+120 -6.63623189e+120 -6.63623189e+120 -6.63623189e+120\n",
            "   -6.63623189e+120 -6.63623189e+120 -6.63623189e+120 -6.63623189e+120\n",
            "   -6.63623189e+120 -6.63623189e+120 -6.63623189e+120 -6.63623189e+120\n",
            "   -6.63623189e+120 -6.63623189e+120 -6.63623189e+120 -6.63623189e+120]]\n",
            "\n",
            " [[-1.24261308e+121 -1.24261308e+121 -1.24261308e+121 -1.24261308e+121\n",
            "   -1.24261308e+121 -1.24261308e+121 -1.24261308e+121 -1.24261308e+121\n",
            "   -1.24261308e+121 -1.24261308e+121 -1.24261308e+121 -1.24261308e+121\n",
            "   -1.24261308e+121 -1.24261308e+121 -1.24261308e+121 -1.24261308e+121\n",
            "   -1.24261308e+121 -1.24261308e+121 -1.24261308e+121 -1.24261308e+121]]\n",
            "\n",
            " [[-1.46157819e+121 -1.46157819e+121 -1.46157819e+121 -1.46157819e+121\n",
            "   -1.46157819e+121 -1.46157819e+121 -1.46157819e+121 -1.46157819e+121\n",
            "   -1.46157819e+121 -1.46157819e+121 -1.46157819e+121 -1.46157819e+121\n",
            "   -1.46157819e+121 -1.46157819e+121 -1.46157819e+121 -1.46157819e+121\n",
            "   -1.46157819e+121 -1.46157819e+121 -1.46157819e+121 -1.46157819e+121]]\n",
            "\n",
            " [[-1.97271796e+121 -1.97271796e+121 -1.97271796e+121 -1.97271796e+121\n",
            "   -1.97271796e+121 -1.97271796e+121 -1.97271796e+121 -1.97271796e+121\n",
            "   -1.97271796e+121 -1.97271796e+121 -1.97271796e+121 -1.97271796e+121\n",
            "   -1.97271796e+121 -1.97271796e+121 -1.97271796e+121 -1.97271796e+121\n",
            "   -1.97271796e+121 -1.97271796e+121 -1.97271796e+121 -1.97271796e+121]]\n",
            "\n",
            " [[-1.82885695e+121 -1.82885695e+121 -1.82885695e+121 -1.82885695e+121\n",
            "   -1.82885695e+121 -1.82885695e+121 -1.82885695e+121 -1.82885695e+121\n",
            "   -1.82885695e+121 -1.82885695e+121 -1.82885695e+121 -1.82885695e+121\n",
            "   -1.82885695e+121 -1.82885695e+121 -1.82885695e+121 -1.82885695e+121\n",
            "   -1.82885695e+121 -1.82885695e+121 -1.82885695e+121 -1.82885695e+121]]\n",
            "\n",
            " [[-1.01778984e+121 -1.01778984e+121 -1.01778984e+121 -1.01778984e+121\n",
            "   -1.01778984e+121 -1.01778984e+121 -1.01778984e+121 -1.01778984e+121\n",
            "   -1.01778984e+121 -1.01778984e+121 -1.01778984e+121 -1.01778984e+121\n",
            "   -1.01778984e+121 -1.01778984e+121 -1.01778984e+121 -1.01778984e+121\n",
            "   -1.01778984e+121 -1.01778984e+121 -1.01778984e+121 -1.01778984e+121]]\n",
            "\n",
            " [[-1.18705821e+121 -1.18705821e+121 -1.18705821e+121 -1.18705821e+121\n",
            "   -1.18705821e+121 -1.18705821e+121 -1.18705821e+121 -1.18705821e+121\n",
            "   -1.18705821e+121 -1.18705821e+121 -1.18705821e+121 -1.18705821e+121\n",
            "   -1.18705821e+121 -1.18705821e+121 -1.18705821e+121 -1.18705821e+121\n",
            "   -1.18705821e+121 -1.18705821e+121 -1.18705821e+121 -1.18705821e+121]]\n",
            "\n",
            " [[-3.56317711e+121 -3.56317711e+121 -3.56317711e+121 -3.56317711e+121\n",
            "   -3.56317711e+121 -3.56317711e+121 -3.56317711e+121 -3.56317711e+121\n",
            "   -3.56317711e+121 -3.56317711e+121 -3.56317711e+121 -3.56317711e+121\n",
            "   -3.56317711e+121 -3.56317711e+121 -3.56317711e+121 -3.56317711e+121\n",
            "   -3.56317711e+121 -3.56317711e+121 -3.56317711e+121 -3.56317711e+121]]\n",
            "\n",
            " [[-2.37533740e+121 -2.37533740e+121 -2.37533740e+121 -2.37533740e+121\n",
            "   -2.37533740e+121 -2.37533740e+121 -2.37533740e+121 -2.37533740e+121\n",
            "   -2.37533740e+121 -2.37533740e+121 -2.37533740e+121 -2.37533740e+121\n",
            "   -2.37533740e+121 -2.37533740e+121 -2.37533740e+121 -2.37533740e+121\n",
            "   -2.37533740e+121 -2.37533740e+121 -2.37533740e+121 -2.37533740e+121]]\n",
            "\n",
            " [[-1.31160529e+121 -1.31160529e+121 -1.31160529e+121 -1.31160529e+121\n",
            "   -1.31160529e+121 -1.31160529e+121 -1.31160529e+121 -1.31160529e+121\n",
            "   -1.31160529e+121 -1.31160529e+121 -1.31160529e+121 -1.31160529e+121\n",
            "   -1.31160529e+121 -1.31160529e+121 -1.31160529e+121 -1.31160529e+121\n",
            "   -1.31160529e+121 -1.31160529e+121 -1.31160529e+121 -1.31160529e+121]]\n",
            "\n",
            " [[-1.63969077e+121 -1.63969077e+121 -1.63969077e+121 -1.63969077e+121\n",
            "   -1.63969077e+121 -1.63969077e+121 -1.63969077e+121 -1.63969077e+121\n",
            "   -1.63969077e+121 -1.63969077e+121 -1.63969077e+121 -1.63969077e+121\n",
            "   -1.63969077e+121 -1.63969077e+121 -1.63969077e+121 -1.63969077e+121\n",
            "   -1.63969077e+121 -1.63969077e+121 -1.63969077e+121 -1.63969077e+121]]\n",
            "\n",
            " [[-1.49447848e+121 -1.49447848e+121 -1.49447848e+121 -1.49447848e+121\n",
            "   -1.49447848e+121 -1.49447848e+121 -1.49447848e+121 -1.49447848e+121\n",
            "   -1.49447848e+121 -1.49447848e+121 -1.49447848e+121 -1.49447848e+121\n",
            "   -1.49447848e+121 -1.49447848e+121 -1.49447848e+121 -1.49447848e+121\n",
            "   -1.49447848e+121 -1.49447848e+121 -1.49447848e+121 -1.49447848e+121]]\n",
            "\n",
            " [[-1.40181837e+121 -1.40181837e+121 -1.40181837e+121 -1.40181837e+121\n",
            "   -1.40181837e+121 -1.40181837e+121 -1.40181837e+121 -1.40181837e+121\n",
            "   -1.40181837e+121 -1.40181837e+121 -1.40181837e+121 -1.40181837e+121\n",
            "   -1.40181837e+121 -1.40181837e+121 -1.40181837e+121 -1.40181837e+121\n",
            "   -1.40181837e+121 -1.40181837e+121 -1.40181837e+121 -1.40181837e+121]]\n",
            "\n",
            " [[-1.14086701e+121 -1.14086701e+121 -1.14086701e+121 -1.14086701e+121\n",
            "   -1.14086701e+121 -1.14086701e+121 -1.14086701e+121 -1.14086701e+121\n",
            "   -1.14086701e+121 -1.14086701e+121 -1.14086701e+121 -1.14086701e+121\n",
            "   -1.14086701e+121 -1.14086701e+121 -1.14086701e+121 -1.14086701e+121\n",
            "   -1.14086701e+121 -1.14086701e+121 -1.14086701e+121 -1.14086701e+121]]\n",
            "\n",
            " [[-1.91598588e+121 -1.91598588e+121 -1.91598588e+121 -1.91598588e+121\n",
            "   -1.91598588e+121 -1.91598588e+121 -1.91598588e+121 -1.91598588e+121\n",
            "   -1.91598588e+121 -1.91598588e+121 -1.91598588e+121 -1.91598588e+121\n",
            "   -1.91598588e+121 -1.91598588e+121 -1.91598588e+121 -1.91598588e+121\n",
            "   -1.91598588e+121 -1.91598588e+121 -1.91598588e+121 -1.91598588e+121]]\n",
            "\n",
            " [[-4.47198755e+120 -4.47198755e+120 -4.47198755e+120 -4.47198755e+120\n",
            "   -4.47198755e+120 -4.47198755e+120 -4.47198755e+120 -4.47198755e+120\n",
            "   -4.47198755e+120 -4.47198755e+120 -4.47198755e+120 -4.47198755e+120\n",
            "   -4.47198755e+120 -4.47198755e+120 -4.47198755e+120 -4.47198755e+120\n",
            "   -4.47198755e+120 -4.47198755e+120 -4.47198755e+120 -4.47198755e+120]]\n",
            "\n",
            " [[-1.27369672e+121 -1.27369672e+121 -1.27369672e+121 -1.27369672e+121\n",
            "   -1.27369672e+121 -1.27369672e+121 -1.27369672e+121 -1.27369672e+121\n",
            "   -1.27369672e+121 -1.27369672e+121 -1.27369672e+121 -1.27369672e+121\n",
            "   -1.27369672e+121 -1.27369672e+121 -1.27369672e+121 -1.27369672e+121\n",
            "   -1.27369672e+121 -1.27369672e+121 -1.27369672e+121 -1.27369672e+121]]\n",
            "\n",
            " [[-1.47627674e+121 -1.47627674e+121 -1.47627674e+121 -1.47627674e+121\n",
            "   -1.47627674e+121 -1.47627674e+121 -1.47627674e+121 -1.47627674e+121\n",
            "   -1.47627674e+121 -1.47627674e+121 -1.47627674e+121 -1.47627674e+121\n",
            "   -1.47627674e+121 -1.47627674e+121 -1.47627674e+121 -1.47627674e+121\n",
            "   -1.47627674e+121 -1.47627674e+121 -1.47627674e+121 -1.47627674e+121]]\n",
            "\n",
            " [[-1.50463710e+121 -1.50463710e+121 -1.50463710e+121 -1.50463710e+121\n",
            "   -1.50463710e+121 -1.50463710e+121 -1.50463710e+121 -1.50463710e+121\n",
            "   -1.50463710e+121 -1.50463710e+121 -1.50463710e+121 -1.50463710e+121\n",
            "   -1.50463710e+121 -1.50463710e+121 -1.50463710e+121 -1.50463710e+121\n",
            "   -1.50463710e+121 -1.50463710e+121 -1.50463710e+121 -1.50463710e+121]]\n",
            "\n",
            " [[-1.33062478e+121 -1.33062478e+121 -1.33062478e+121 -1.33062478e+121\n",
            "   -1.33062478e+121 -1.33062478e+121 -1.33062478e+121 -1.33062478e+121\n",
            "   -1.33062478e+121 -1.33062478e+121 -1.33062478e+121 -1.33062478e+121\n",
            "   -1.33062478e+121 -1.33062478e+121 -1.33062478e+121 -1.33062478e+121\n",
            "   -1.33062478e+121 -1.33062478e+121 -1.33062478e+121 -1.33062478e+121]]\n",
            "\n",
            " [[-1.66532541e+121 -1.66532541e+121 -1.66532541e+121 -1.66532541e+121\n",
            "   -1.66532541e+121 -1.66532541e+121 -1.66532541e+121 -1.66532541e+121\n",
            "   -1.66532541e+121 -1.66532541e+121 -1.66532541e+121 -1.66532541e+121\n",
            "   -1.66532541e+121 -1.66532541e+121 -1.66532541e+121 -1.66532541e+121\n",
            "   -1.66532541e+121 -1.66532541e+121 -1.66532541e+121 -1.66532541e+121]]\n",
            "\n",
            " [[-1.36675614e+121 -1.36675614e+121 -1.36675614e+121 -1.36675614e+121\n",
            "   -1.36675614e+121 -1.36675614e+121 -1.36675614e+121 -1.36675614e+121\n",
            "   -1.36675614e+121 -1.36675614e+121 -1.36675614e+121 -1.36675614e+121\n",
            "   -1.36675614e+121 -1.36675614e+121 -1.36675614e+121 -1.36675614e+121\n",
            "   -1.36675614e+121 -1.36675614e+121 -1.36675614e+121 -1.36675614e+121]]\n",
            "\n",
            " [[-1.06788647e+121 -1.06788647e+121 -1.06788647e+121 -1.06788647e+121\n",
            "   -1.06788647e+121 -1.06788647e+121 -1.06788647e+121 -1.06788647e+121\n",
            "   -1.06788647e+121 -1.06788647e+121 -1.06788647e+121 -1.06788647e+121\n",
            "   -1.06788647e+121 -1.06788647e+121 -1.06788647e+121 -1.06788647e+121\n",
            "   -1.06788647e+121 -1.06788647e+121 -1.06788647e+121 -1.06788647e+121]]\n",
            "\n",
            " [[-2.75646374e+121 -2.75646374e+121 -2.75646374e+121 -2.75646374e+121\n",
            "   -2.75646374e+121 -2.75646374e+121 -2.75646374e+121 -2.75646374e+121\n",
            "   -2.75646374e+121 -2.75646374e+121 -2.75646374e+121 -2.75646374e+121\n",
            "   -2.75646374e+121 -2.75646374e+121 -2.75646374e+121 -2.75646374e+121\n",
            "   -2.75646374e+121 -2.75646374e+121 -2.75646374e+121 -2.75646374e+121]]\n",
            "\n",
            " [[-2.41116038e+121 -2.41116038e+121 -2.41116038e+121 -2.41116038e+121\n",
            "   -2.41116038e+121 -2.41116038e+121 -2.41116038e+121 -2.41116038e+121\n",
            "   -2.41116038e+121 -2.41116038e+121 -2.41116038e+121 -2.41116038e+121\n",
            "   -2.41116038e+121 -2.41116038e+121 -2.41116038e+121 -2.41116038e+121\n",
            "   -2.41116038e+121 -2.41116038e+121 -2.41116038e+121 -2.41116038e+121]]\n",
            "\n",
            " [[-1.28053415e+121 -1.28053415e+121 -1.28053415e+121 -1.28053415e+121\n",
            "   -1.28053415e+121 -1.28053415e+121 -1.28053415e+121 -1.28053415e+121\n",
            "   -1.28053415e+121 -1.28053415e+121 -1.28053415e+121 -1.28053415e+121\n",
            "   -1.28053415e+121 -1.28053415e+121 -1.28053415e+121 -1.28053415e+121\n",
            "   -1.28053415e+121 -1.28053415e+121 -1.28053415e+121 -1.28053415e+121]]\n",
            "\n",
            " [[-6.76152959e+120 -6.76152959e+120 -6.76152959e+120 -6.76152959e+120\n",
            "   -6.76152959e+120 -6.76152959e+120 -6.76152959e+120 -6.76152959e+120\n",
            "   -6.76152959e+120 -6.76152959e+120 -6.76152959e+120 -6.76152959e+120\n",
            "   -6.76152959e+120 -6.76152959e+120 -6.76152959e+120 -6.76152959e+120\n",
            "   -6.76152959e+120 -6.76152959e+120 -6.76152959e+120 -6.76152959e+120]]\n",
            "\n",
            " [[-7.75407182e+120 -7.75407182e+120 -7.75407182e+120 -7.75407182e+120\n",
            "   -7.75407182e+120 -7.75407182e+120 -7.75407182e+120 -7.75407182e+120\n",
            "   -7.75407182e+120 -7.75407182e+120 -7.75407182e+120 -7.75407182e+120\n",
            "   -7.75407182e+120 -7.75407182e+120 -7.75407182e+120 -7.75407182e+120\n",
            "   -7.75407182e+120 -7.75407182e+120 -7.75407182e+120 -7.75407182e+120]]\n",
            "\n",
            " [[-1.65012081e+121 -1.65012081e+121 -1.65012081e+121 -1.65012081e+121\n",
            "   -1.65012081e+121 -1.65012081e+121 -1.65012081e+121 -1.65012081e+121\n",
            "   -1.65012081e+121 -1.65012081e+121 -1.65012081e+121 -1.65012081e+121\n",
            "   -1.65012081e+121 -1.65012081e+121 -1.65012081e+121 -1.65012081e+121\n",
            "   -1.65012081e+121 -1.65012081e+121 -1.65012081e+121 -1.65012081e+121]]\n",
            "\n",
            " [[-1.02453050e+121 -1.02453050e+121 -1.02453050e+121 -1.02453050e+121\n",
            "   -1.02453050e+121 -1.02453050e+121 -1.02453050e+121 -1.02453050e+121\n",
            "   -1.02453050e+121 -1.02453050e+121 -1.02453050e+121 -1.02453050e+121\n",
            "   -1.02453050e+121 -1.02453050e+121 -1.02453050e+121 -1.02453050e+121\n",
            "   -1.02453050e+121 -1.02453050e+121 -1.02453050e+121 -1.02453050e+121]]\n",
            "\n",
            " [[-1.08516028e+121 -1.08516028e+121 -1.08516028e+121 -1.08516028e+121\n",
            "   -1.08516028e+121 -1.08516028e+121 -1.08516028e+121 -1.08516028e+121\n",
            "   -1.08516028e+121 -1.08516028e+121 -1.08516028e+121 -1.08516028e+121\n",
            "   -1.08516028e+121 -1.08516028e+121 -1.08516028e+121 -1.08516028e+121\n",
            "   -1.08516028e+121 -1.08516028e+121 -1.08516028e+121 -1.08516028e+121]]\n",
            "\n",
            " [[-1.93393808e+121 -1.93393808e+121 -1.93393808e+121 -1.93393808e+121\n",
            "   -1.93393808e+121 -1.93393808e+121 -1.93393808e+121 -1.93393808e+121\n",
            "   -1.93393808e+121 -1.93393808e+121 -1.93393808e+121 -1.93393808e+121\n",
            "   -1.93393808e+121 -1.93393808e+121 -1.93393808e+121 -1.93393808e+121\n",
            "   -1.93393808e+121 -1.93393808e+121 -1.93393808e+121 -1.93393808e+121]]]\n",
            "---------------------------------\n",
            "True value : [8 3 6 7 7 3 1 5 6 2 7 2 6 7 7 5 1 4 4 4 5 9 4 3 8 6 9 1 7 9 6 7 4]\n",
            "Prediction : [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n",
            "----------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-ec5a65b4e939>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Get the output for the current batch using the network's forward method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# Label y will be 1D , change it into 2D then calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-113-21f3982f53bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-36d388154bc0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mf_w\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0;31m# Putting element in output array after convolution (X * w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_w\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_w\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpad_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mout_w\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mout_h\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_new(inputs:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    to calculate the probablity we pass the output of the last layer to the softmax\n",
        "\n",
        "    Args:\n",
        "         output numpy array of the last layer\n",
        "\n",
        "    Return:\n",
        "           numpy array after transformation\n",
        "    \"\"\"\n",
        "    # substracting maximum value from each value to avoid exploding exponential value\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1))\n",
        "    output = exp_values / np.sum(exp_values)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "e7ZPLpcOfp39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.array([[[1,4,3,45,6]], [[3,545,6,2,4]], [[232, 4,56,6,4]]])\n",
        "pred = np.array([[ 6.94952980e+01, -6.82221706e+01, -1.45857190e+02 , 2.58064020e+01,-1.10672038e+01 , 2.55788607e+01,  2.08673372e+01,  6.71047443e+01,-1.85183491e+01 , 7.21633511e+01]])\n",
        "true = np.array([[0,1,0,0,0,0,0,0,0,0]])\n",
        "print(np.max(pred, axis=1))\n",
        "exp_values = np.exp(pred - np.max(pred, axis=1))\n",
        "print(exp_values)\n",
        "print(exp_values.shape)\n",
        "softmax_y_pred =softmax(pred)\n",
        "print(f\"softmax: {softmax_y_pred}\")\n",
        "\n",
        "y_pred_clipped = np.clip(softmax_y_pred, 1e-9, 1-1e-9)\n",
        "print(f\"y_pred_clipped: {y_pred_clipped}\")\n",
        "correct_confidences = np.sum(y_pred_clipped*true, axis=1)\n",
        "print(f\"correct_confidences: {correct_confidences}\")\n",
        "negative_log_likelihoods = -np.log(correct_confidences)\n",
        "print(negative_log_likelihoods)"
      ],
      "metadata": {
        "id": "CfAfJ6y_iilg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-68.2221706-72.1633511"
      ],
      "metadata": {
        "id": "-yyc0FCCWuVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UBdYrYRrztS7",
        "SNKwUSHw0HBC",
        "pJJa9ZGG0NtS",
        "sugDgTck0TyQ",
        "GzAgGcDa0bGJ",
        "1lrHJ7NY0ieB",
        "335Znvuj0oQS",
        "psDAOHt229YN",
        "fwutzqml0tQP",
        "MTyLe84sxGWr",
        "b_civ4YDtIV1",
        "IqWAb_A7T-Ml",
        "Wag871i3sIty",
        "Q5OTjBCwsNHP",
        "IlexZu4af9Do"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}